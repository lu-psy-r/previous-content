{
  "hash": "92213a13c6174cfecc603424372ecb03",
  "result": {
    "markdown": "---\ntitle: 4. Testing nominal data\nsubtitle: Written by Padraic Monaghan\norder: 5\n\n---\n\n\n# Preparation\n\n- Watch Lecture week 4 [part 1](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2001409), [slides here](ppt/wk4/PSYC401 Lecture week4 part1 handout.pptx)\n\n- [Part 2](), [slides here](ppt/wk4/PSYC401 Lecture week4 part2.pptx).\n\n- [Part 3](), [slides here](ppt/wk4/PSYC401 Lecture week4 part3.pptx).\n\n- Take the [quiz](https://modules.lancaster.ac.uk/mod/quiz/view.php?id=1921440) (not assessed) on the lecture materials.\n\n- See the [guides to reporting numbers and statistical tests](https://apastyle.apa.org/instructional-aids/numbers-statistics-guide.pdf) in American Psychological Association format (the format that we use in Psychology for all reports).\n\n- Work through the materials for the Practical week 4 below.\n\n- Come to the practical.\n\n- Pop into the drop in (optional).\n\n- Complete [Assignment 2](https://modules.lancaster.ac.uk/mod/quiz/view.php?id=1921443) by Friday 4th November 8pm (this requires the [two Assignment 2 data files](data/Wk4 assignment.zip) as well).\n\n- Watch the [clip of the Titanic film if you like](https://modules.lancaster.ac.uk/mod/url/view.php?id=1921445) (not assessed!).\n\n- Read [what is a p-value](https://modules.lancaster.ac.uk/mod/url/view.php?id=1921446)\n\n\nThe materials in this workbook share some material with [Glasgow University Psychology Department Teaching in R website](https://psyteachr.github.io/quant-fun-v2)\n\n# Part 1: Revision for last week\n\n## Task 1: Your data from the paper in Psychological Science\n\n1. Your take-home task was to produce some graphs of the data set downloaded from a paper in Psychological Science. Show your graphs and R script to the rest of your group.\n\n# Part 2: Load in the Vocabulary Scores Data and Produce Graphs\n\n## Task 2: Load in the Data\n\n2.  Remember to clear out R first:\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(list=ls())\n```\n:::\n\n\nThe data set on the Shipley and Gent vocabulary scores is now updated with the data from your group, so it now contains five years of PSYC401 students' data. I've omitted Age as this might impact anonymity of the data. Download the data from the week 4 moodle folder: \"PSYC401-shipley-scores-anonymous-17_22.csv\" and read the data into an object in R studio called `vdat` (for vocabulary data).\n\n 3. As a reminder, when we want to look at a particular variable (a column) in an object in R studio, we refer to it using the `$` notation. So, for the object vdat and the variable academic_year you would refer to it as `vdat$academic_year`. For this data set, we need to change academic year to be a nominal (factor) variable. Why does academic year have to be nominal and not interval/ratio?:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvdat$academic_year <- as.factor(vdat$academic_year)\n\nview(vdat) #view the data\n```\n:::\n\n\n4. Make sure the tidyverse library is loaded. Select all the variables apart from Dyslexia_diagnosis and Age and save as a new object called \"summaryvdat\". We will omit these variables because they are not complete for the dataset.\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Hint\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nsummaryvdat <- select(vdat, -c(Dyslexia_diagnosis, Age))\n```\n:::\n\n\n`-c(Dyslexia_diagnosis, Age)` is a quicker way to \"unselect\" variables. The negative sign means select all columns except for these listed in `c()`. The alternative is to type every column you want to keep.\n\n:::\n\n\n5. Arrange the data according to Gent_2_score, from highest to lowest. Save this as a new object called \"summaryvdat_sort\"\n\n## Task 3: Draw Graphs of the Vocabulary Data\n\n6. Draw graphs of the following relations:\n* English status and academic year\n* Gender and academic year\n* Vocabulary score and academic year\n\n\n7. Save your script file.\n\n# Part 3: Grouping data in R studio\n\n## Task 4: Loading and joining data in R studio \n\n8. Now, let's clear out R-studio before we get started again using `rm(list=ls())`.\n\n9. Go to the data files from week 2 and load them into Rstudio again (\"ahicesd.csv\", and \"participantinfo.csv\"). You can redownload them [here](data/PSYC401_wk2.zip).\n\n- Remember these data come from this study: Woodworth, R.J., O'Brien-Malone, A., Diamond, M.R. and Schuz, B. (2018). Data from, \"Web-based Positive Psychology Interventions: A Reexamination of Effectiveness\". Journal of Open Psychology Data, 6(1).\n- Remind yourself of the aim of the study and the variables that are in the data set (see end of this script file for repeat description on the study).\n\n 10. Next, load and join the ahicesd.csv and participantinfo.csv data in R studio. Call the joined data set \"all_dat\" (see week 2 workbook for reminders about this)\n\n## Task 5: Selecting and manipulating data\n\n11. We're not interested in the individual questionnaire items. So, let's select all the variables we want to keep (omitting the individual questionnaire items), and save this to an object called summary_all_dat (again see week 2 workbook for reminder)\n\n12. Next, we will add another variable to the data. We use the function `mutate()` for this. Let's scale the ahiTotal and cesdTotal values and add them to the summary_all_dat set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary_all_dat_scale <- mutate(.data = summary_all_dat, ahiTotalscale = scale(ahiTotal), cesdTotalscale = scale(cesdTotal))\n```\n:::\n\n\n- What are the minimum and maximum values of the new variable ahiTotalscale?\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Hint\n\nhint: use the `arrange()` function, or the `min()` and `max()` functions.\n\n:::\n\n- What do these scale values mean? (reminder: they are Z scores).\n\n 13. The next way we will work with the data is to organise the observations into different groups. First of all, here is the function summarise(). This works by summarising the results of a data set according to a particular measure. So, instead of `mean(summary_all_dat_scale$ahiTotal)` you can use this, which turns out to be a much more powerful way of looking at the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarise(.data = summary_all_dat_scale, mean(ahiTotal))\n```\n:::\n\n\n- They should give the same results - check that they do. This function `summarise()` is more powerful because you can look at several values at the same time, e.g.:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummarise(.data = summary_all_dat_scale, mean(ahiTotal), sd(ahiTotal), mean(cesdTotal), sd(cesdTotal))\n```\n:::\n\n\n- What is the result of this command?\n\n14. But now let's think about what kind of patterns we'd like to investigate in the data. There are four interventions conducted in this study. Let's look at each of these interventions and their effect of ahiTotal and cesdTotal. We can look at subgroups of data either by using the `filter()` function, or by using the function `group_by()`. The advantage of `group_by()` is that we can look at several groups at the same time, rather than dividing up the data file into pieces. Let's organise by the different interventions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary_all_dat_scale_intervention <- group_by(.data = summary_all_dat_scale, intervention)\n```\n:::\n\n\n- This command takes the data summary_all_dat_scale, and then groups it according to the four interventions in the data. We can't yet see any difference in summary_all_dat_scale_intervention but it's in there, lurking, just waiting. Now, we can look at the means for each intervention using the summary function again. Run the `summary` function on summarydata_scale_intervention. What happens?\n\n15. You can also group by several factors at the same time. We can group by intervention and get means and standard deviations, but that is not going to give us a huge amount of insight into how the interventions affect the happiness measure because we are combining the mean of ahiTotal across all occasions of testing, including testing before the intervention has been applied.\n\nSo, let's group by intervention and occasion of testing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary_all_dat_intocc <- group_by(.data = summary_all_dat_scale, intervention, occasion)\n```\n:::\n\n\n- Now produce the means and standard deviations of the happiness score (ahiTotal) for each intervention at each testing occasion. \n\n16. This doesn't print all the lines out, so you can make a new object (e.g., called sum_output) and view that, or you can filter out some of the lines so we only look at the first and second occasion of testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_output <- summarise(.data = summary_all_dat_intocc, mean(ahiTotal), sd(ahiTotal))\nView(sum_output)\n```\n:::\n\n\n\n# Part 4: Graphing groups\n\n## Task 6: Graph some groups\n\n17. Draw a scatter plot of ahiTotal and cesdTotal values for the whole data set.\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Hint\n\nUse the `ggplot()` function with `geom_point() `\n\n::: \n\n- Make it a bit more beautiful using the labs() addition.\n\n18. Now redraw the plot, but colour the points according to whether they are first, second, third, etc occasion of testing. Add in `col = \"occasion\"` into the `aes()` part of the geom_point function, so that this part looks like this:  `aes(x = ahiTotal, y = cesdTotal, col = occasion)`\n\n\n# Part 5: Working out whether nominal data is random or structured.\n\nRepeating the analyses from Lecture week4 part3\n\n\n## Task 7: Chi-squared and Cramer's V\n\n19. Let's now have a look at running Chi-squared and Cramer's V tests in R. download this week's data from [here](data/PSYC401_wk4.zip). Read titanic.csv into an object called \"titanic\". View the data. It should correspond to the data in the overhead slides.\n\n20. Make a bar graph to count the numbers of survived and died by class.\n\n21. Now let's see if there is a significant relation between class and survival using Chi-squared: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(x = titanic$class, y = titanic$survival)\n```\n:::\n\n\n- The results give the chi-squared value, the number of degrees of freedom, and the p-value. P = 2.2e-16 means p = .0000000000000022. That's highly significant. That means the observations are divided across the categories in a way that is very unlikely to be due to chance (for this number (P = 2.2e-16), it means there's a 2 in a quadrillion chance that titanic survival was not related to class). In a report, you would write: Chi-squared(2, N= 1309) = 127.86, p < .001.\n\n22. Now, let's compute Cramer's V. First, we need to make sure we have the package lsr.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lsr)\n```\n:::\n\n\n- Then run the test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncramersV(x = titanic$class, y = titanic$survival)\n```\n:::\n\n\n23. Your next task is to run some Chi-squared and Cramer's V tests on some of the other nominal data. Open the data \"PSYC401-shipley-scores-anonymous-17_22.csv\" again. Investigate the association between gender and year (are there different distributions of males and females in each of our masters' year cohorts) using Chi-squared and Cramer's V. Is it significant? \n\n24. What about the association between english_status and Gender? \n\n25. What about the association between english_status and academic year?\n\n# Part 6: More practice using Chi-squared and Cramer's V test\n\n## Task 8: More Chi-squared and Cramer's V tests\n\n26. Look at the \"ahicesd.csv\" and \"participantinfo.csv\" data sets from week 2 again. Which nominal measures could you look at an association between? Report the Chi-squared test and Cramer's V results for these associations. Are these associations significant? How do you interpret the significant associations?\n\n27. Have a further browse of Psychological Science for data sets that you can download and begin to explore. Practise applying the data manipulation and graphing functions to these data sets.\n\n\nDescription of Woodworth, R.J., O'Brien-Malone, A., Diamond, M.R. and Schuz, B. (2018). Data from, \"Web-based Positive Psychology Interventions: A Reexamination of Effectiveness\". Journal of Open Psychology Data, 6(1).\n\n- In our study we attempted a partial replication of the study of Seligman, Steen, Park, and Peterson (2005) which had suggested that the web-based delivery of positive psychology exercises could, as the consequence of containing specific, powerful therapeutic ingredients, effect greater increases in happiness and greater reductions in depression than could a placebo control. Participants (n=295) were randomly allocated to one of four intervention groups referred to, in accordance with the terminology in Seligman et al. (2005) as 1: Using Signature Strengths; 2: Three Good Things; 3: Gratitude Visit; 4: Early Memories (placebo control). At the commencement of the study, participants provided basic demongraphic information (age, sex, education, income) in addition to completing a pretest on the Authentic Happiness Inventory (AHI) and the Center for Epidemiologic Studies-Depression (CES-D) scale. Participants were asked to complete intervention-related activities during the week following the pretest. Futher measurements were then made on the AHI and CESD immediately after the intervention period ('posttest') and then 1 month after the posttest (day 38), 3 months after the posttest (day 98), and 6 months after the posttest (day 189). Participants were not able to to complete a follow-up questionnaire prior to the time that it was due but might have completed at either at the time that it was due, or later. We recorded the date and time at which follow-up questionnaires were completed.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}