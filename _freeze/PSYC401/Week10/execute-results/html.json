{
  "hash": "252bab972d0bb1722e9b0ee611c10a81",
  "result": {
    "markdown": "---\ntitle: 10. Developing the linear model\nsubtitle: Written by Rob Davies\norder: 11\n---\n\n::: {.cell}\n\n:::\n\n\n# Preparation\n\n## Lectures\n\n- [Part 1 - aims, assumptions, coding linear models](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2026078)\n- [Part 2 - coding, thinking about, and reporting linear models with multiple predictors](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2026079)\n- [Part 3 - critically evaluating the results of analyses involving linear models](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2026084)\n- [Part 4 - linear model is very flexible, powerful and general](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2026081)\n\n- All slides are [here (html)](https://modules.lancaster.ac.uk/mod/resource/view.php?id=2026086), and [here (.docx)](data/PSYC401_wk10/401-linear-model-develop-printable-edit.docx)\n\n- Each part is about 15 minutes in length. I have labelled lectures so you know what is in each.\n\n- Work through the materials for the Practical below\n\n- The motivation for providing you with a how-to and a workbook section is that you can learn and progress to do the practical work with more support (in the how-to) and then room to try things (in the workbook).\n\n- Attend the practical.\n\n- Test yourself using the [quiz (not assessed)](https://modules.lancaster.ac.uk/mod/quiz/view.php?id=2029455).\n\n- Optionally, if you can [give us your (anonymised) feedback](https://modules.lancaster.ac.uk/mod/feedback/view.php?id=2026089) on how the course is going from your perspective, that would be very welcome.\n\n# Part One: How To\n\n In Week 10, we aim to *further* develop skills in working with the linear model, and in visualizing and testing the associations between variables in psychological data\n\nWe do this to learn how to answer research questions like:\n-- What person attributes predict success in understanding?\n-- Can people accurately evaluate whether they correctly understand written health information?\n\n These kinds of research questions can often be answered through analyses using linear models. We will use linear models to estimate the association between predictors and outcomes\n\n When we do these analyses, we will need to think about how we report the results:  we usually need to report information about the kind of model we specify;and we will need to report the nature of the associations estimated in our model; we usually need to decide, is the association between the outcome and any one predictor significant? Does that association reflect a positive or negative relationship between the outcome and that predictor?are the associations we see in sample data relatively strong or weak?\n\n\n We will consolidate and extend learning on data visualization:\n-- Use scatterplots to examine the relationships we may observe or predict\n-- Generate predictions given model estimates of slope coefficients\n\n## Task 1: Set-up \n\n\n1. Run this code to empty the R environment: `rm(list=ls())`\n\n\n2. Run this code to load relevant libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"ggeffects\")\nlibrary(\"patchwork\")\nlibrary(\"psych\")\nlibrary(\"tidyverse\")\n```\n:::\n\n\n\nIn this how-to guide, we use a collection of data, drawing together data from a series of studies, completed by BSc and MSc students, as replications of the clearly-understood health comprehension project investigations: `2022-12-08_all-studies-subject-scores.csv`\n\n## Task 2: Load data \n\n3. Read in the data file we will be using: \n\n::: {.cell}\n\n```{.r .cell-code}\nall.studies.subjects <- read_csv(\"2022-12-08_all-studies-subject-scores.csv\", \n                                 col_types = cols(\n                                   ResponseId = col_factor(),\n                                   study = col_factor(),\n                                   EDUCATION = col_factor(),\n                                   GENDER = col_factor(),\n                                   ETHNICITY = col_factor(),\n                                   NATIVE.LANGUAGE = col_factor()\n                                 )\n                                )\n```\n:::\n\n\n Notice what we are doing here:\n-- `all.studies.subjects <- read_csv(\"2022-12-08_all-studies-subject-scores.csv\" ..)` reads in the dataset we name, the .csv\n--  col_types = cols(\n   ResponseId = col_factor(),\n   ... tells R how to handle some of the variables in the dataset: the variables that are listed\n   \n-- notice: not all of the variables are categorical or nominal variables that we want R to recognize as factors.\n\n-- We embed one function `cols()` *inside* another function `read_csv()` and that means at the end we need to *close* the brackets\n\n See [here for further information](https://robayedavies.github.io/PSYC401-book/visualization.htmlsec-ricketts-process-data)\n\n4. Inspect the data file. In previous how-to guides I have advised you to use the `summary()` or `head()` functions to look at the data. Here, we use the `describe()` function from the 'psych' library \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndescribe(all.studies.subjects)\n```\n:::\n\n\nIn psychological research reports, we sometimes see table summaries of the  predictor or outcome variables in the dataset being analysed. The `describe()` function in 'psych' is a convenient way to get the descriptive statistics that psychologists commonly report: the mean and standard deviation (SD),  minimum (min) and maximum (max)\n\n\n5. Get descriptive statistics for only the variables you want and get only the descriptive statistics you need. We are going to do this in two steps: \n-- we select the variables we care about\n-- we get just the descriptive statistics we want for those variables\n\n6. First, I am going to do the steps using %>% pipes: \nnot everyone likes coding this way so later I'll do it in the old style\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.studies.subjects %>%\n  select(mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3) %>%\n  describe(skew = FALSE)\n```\n:::\n\n\n These are the steps:\n-- `all.studies.subjects %>%` - you tell R to use the 'all.studies.subjects' dataset, \nthen use `%>%` to ask R to take those data to the next step (ie to `%>%` pipe it)\n-- `select(mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3) %>%` - you tell R to select just those variables in 'all.studies.subjects' you name and pipe %>% them to the next step\n-- `describe(...)` - you tell R to give you descriptive statistics for the variables you have selected\n--`describe(skew = FALSE)` - critically, you add the argument \"skew = FALSE\" to turn off the option in `describe()` to report skew, kurtosis because we do not typically see these statistics reported in psychology\n\n\nNot everyone likes coding this way so now I'll do the same thing in the old style\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.studies.subjects.select <- select(all.studies.subjects, \n                           mean.acc, mean.self, AGE, SHIPLEY, HLVA, FACTOR3)\ndescribe(all.studies.subjects.select, skew = FALSE)\n```\n:::\n\n\nNotice: we can do exactly the same thing in two different but related ways. Use the way that (a) works and (b) you prefer\n\nWhich should you prefer? You may reflect on how easy the code is to write, read, understand and use\n\nHere is an [explanation](https://r4ds.had.co.nz/pipes.html) for how to use pipes %>% when you code and why it may be helpful to do so:\n \nNote we are not going to require the use of pipes in PSYC401, but they can help to make your code more readable and editable when it gets complez.\n\nNotice: we modify here how the function `describe()` works by adding an argument `describe(skew = FALSE)`\n\nWe have been doing this kind of move, already, by adding arguments to e.g. specify point colour in `ggplot()` code\n\nAs your skills advance, so your preferences on how you want R to work for you will become more specific. Thus, for example, you can modify the outputs from functions so that you get exactly what you want\n\nThe information on the options available to you for any argument can be found in different kinds of places\n\n\nYou can get a guide to the 'psych' library [here](http://personality-project.org/r/psych/vignettes/intro.pdf)\nEvery \"official\" R library has a technical manual on the central R resource CRAN, \n and the manual for 'psych' can be found [here](https://cran.r-project.org/web/packages/psych/psych.pdf) where you can see information on the functions the library provides, and how you can use each function\n\nThis is the information you see if you ask for help in R for a function e.g.\n`?describe`\nor\n`help(describe)`\n\n'psych' is written by William Revelle who provides a lot of useful resources [here](http://personality-project.org/r/psych/)\n\n## Task 3: Use a linear model to to answer the research questionsone predictor \n\nWe start by revising how to use `lm()` with one predictor --\n\n\nOne of our research questions is: What person attributes predict success in understanding?\n\n6. Examine the relation between outcome mean accuracy (mean.acc) and health literacy (HLVA). We use lm()\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mean.acc ~ HLVA, data = all.studies.subjects)\nsummary(model)\n```\n:::\n\n  \nIf you look at the model summary you can answer the following questions  \n\nQ1. What is the estimate for the coefficient of the effect of the predictor, HLVA?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n0.041188 \n:::\n\nQ2. Is the effect significant?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nIt is significant, p < .05\n:::\n\nQ3. What are the values for t and p for the significance test for the coefficient?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n*t* = 12.94, *p* = <2e-16\n:::\n\nQ4. What do you conclude is the answer to the research question, given the \n linear model results?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nThe model slope estimate suggests that as HLVA scores increase so also do mean.acc scores\n:::\n\nQ5. What is the F-statistic for the regression? Report F, DF and the p-value.\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nF-statistic: 167.5 on 1 and 559 DF,  p-value: < 2.2e-16\n:::\n\nQ6. Is the regression significant?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nYes: the regression is significant.\n:::\nQ7. What is the Adjusted R-squared?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nAdjusted R-squared:  0.2292\n:::\n\nQ8. Explain in words what this R-squared value indicates?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nThe R-squared suggests that 23% of outcome variance can be explained  by the model\n:::\n\nrevision: use a linear model to generate predictions --\n\n\n7. We can use the model we have just fitted to plot the model predictions. We are going to draw a scatterplot and add a line showing the predictions, given the model intercept and effect coefficient estimates\n\nQ9. What is the coefficient estimate for the intercept?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n0.414250\n:::\n\nQ10. What is the coefficient estimate for the slope of HLVA?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n0.041188\n:::\n\nUse the `geom_abline()` function to draw the prediction line:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = all.studies.subjects, aes(x = HLVA, y = mean.acc)) +\n  geom_point(alpha = 0.5, size = 2,)   +\n  geom_abline(intercept = 0.414250, slope = 0.041188, colour = \"red\", size = 1.5) +\n  theme_bw() +\n  labs(x = \"Health literacy (HLVA)\", y = \"mean accuracy\") +\n  xlim(0, 16) + ylim(0, 1)\n```\n:::\n\n  \nYou can see that all we do is: add the `geom_abline(...)` function and in that, add information about the intercept and the slope. See reference information [here](https://ggplot2.tidyverse.org/reference/geom_abline.html)    \n\n## Task 4: Use a linear model to to answer the research questions multiple predictors \n\nOne of our research questions is: What person attributes predict success in understanding?\n\n8. Examine the relation between outcome mean accuracy (mean.acc) and multiple predictors including:\n\n-- health literacy (HLVA); \n-- vocabulary (SHIPLEY); \n-- AGE; \n-- reading strategy (FACTOR3)\n\nWe use `lm()`, as before, but now specify each variable listed here by variable name\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE, data = all.studies.subjects)\nsummary(model)\n```\n:::\n\n\nNotice that we do the linear model in the steps:\n\n- `model <- lm(...)` - fit the model using `lm(...)`, give the model a name here, we call it \"model\"\n- `...lm(mean.acc ~ HLVA...)` - tell R you want a model of the outcome 'mean.acc' predicted (~) by the predictors: 'HLVA', 'SHIPLEY', 'FACTOR3', 'AGE'. Note that we use the variable names as they appear in the dataset, and that each predictor variable is separated from the next by a plus sign\n- `...data = all.studies.subjects)` - tell R that the variables you name in the formula live in the 'all.studies.subjects' dataset\n- `summary(model)` - ask R for a summary of the model you called \"model\"\n\nNotice: R has a general formula syntax: outcome ~ predictor *or* y ~ x and uses the same format across a number of different functions each time, the left of the tilde symbol ~ is some output or outcome and the right of the tilde ~ is some input or predictor or set of predictors\n\nIf you look at the model summary you can answer the following questions  \n\nQ11. What is the estimate for the coefficient of the effect of the predictor, 'HLVA;, in *this* model?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n0.0274954 \n:::\n\nQ12. Is the effect significant?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nIt is significant, p < .05\n:::\n\nQ13. What are the values for t and p for the significance test for the coefficient?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nt = 8.470, p = < 2e-16\n:::\n\nQ14. What do you conclude is the answer to the research question, given the linear model results?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nThe model slope estimate 0.0274954 suggests that as HLVA scores increase so also do mean.acc scores\n:::\n\nQ15. How is the coefficient estimate for the HLVA slope similar or different, comparing this model with multiple predictors to the previous model with one predictor?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nIt can be seen that the HLVA estimate in the two models is different in that it is a bit smaller in the model with multiple predictors compared to the model with one predictor. The HLVA estimate is similar in that it remains positive, it is about the same size\n\nNotice that:\n- The estimate of the coefficient of any one predictor can be expected to vary depending on the presence of other predictors.\n- This is one reason why we need to be transparent about why we choose to use the predictors we include in our model.\n- The lecture for week 9 discusses this concern in relation to the motivation for good open science practices.\n:::\n  \nQ16. Can you report the estimated effect of SHIPLEY (the measure of vocabulary)?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nThe effect of vocabulary knowledge (SHIPLEY) on mean accuracy of \n understanding is significant (estimate = 0.01, t = 7.30, p < .001)\n indicating that increasing skill is associated with increasing accuracy\n:::\n\nQ17.Can you report the model and the model fit statistics?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nWe fitted a linear model with mean comprehension accuracy as the outcome and health literacy (HLVA),  reading strategy (FACTOR3), vocabulary (SHIPLEY) and AGE (years) as predictors.The model is significant overall, with F(4, 556) = 85.17, p< .001, and explains 38% of variance (adjusted R2 = 0.38).\n:::\n\n## Task 5: Plot predictions from linear models with multiple predictors \n\n9. Plot linear model predictions for one of the predictors. Previously, we used `geom_abline()`, specifying intercept and slope estimates, to draw model predictions. Here, we use functions that are very helpful when we need to plot model predictions for a predictor, for models where we have multiple predictors, and we have to take into account the influence on outcomes of the other predictors\n  \nWe do this in three steps:\n- We first fit a linear model of the outcome, given our predictors. We save information about the model\n- We use the ggpredict() function from the 'ggeffects' library to take the information about the model and create a set of predictions we can use for plotting\n- We plot the model predictions (marginal effects plots)\n\n\nThese steps proceed as follows:-\n\n\n- We first fit a linear model of the outcome, given our predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE, data = all.studies.subjects)\n```\n:::\n\n\n:::{.callout-note}\n## Code explanation\n\n- `model <- lm(...)` - we fit the model using `lm(...)`, give the model a name here, we call it \"model\"\n- `...lm(mean.acc ~ HLVA...)` tell R you want a model of the outcome 'mean.acc' predicted (`~`) by the predictors 'HLVA', 'SHIPLEY', 'FACTOR3', 'AGE'\n- Notice: when we use lm() to fit the model, R creates a set of information about the model, including estimates. We give that set of information a name, and we use that name, next, to access the model information\n:::\n\n- We use the `ggpredict()` function from the 'ggeffects' library to take  \n the information about the model and create a set of predictions we can use for \n plotting  \n \n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- ggpredict(model, \"HLVA\")\n```\n:::\n\n\n:::{.callout-note}\n## Code explanation\n\n- `dat <- ggpredict(...)` - We ask R to create a set of predictions, and we give that set of predictions a name 'dat'\n- `... ggpredict(model, \"HLVA\")` - We tell R what model information it should use (from 'model'), and which predictor variable we need predictions for 'HLVA'\n:::\n\n- We plot the model predictions (marginal effects plots)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(dat)\n```\n:::\n\n\n\n10. Edit the appearance of the marginal effect (prediction) plot as you can with any ggplot object\n\n\n::: {.cell}\n\n```{.r .cell-code}\np.model <- plot(dat)\n\np.model +\n  geom_point(data = all.studies.subjects, \n             aes(x = HLVA, y = mean.acc), size = 1.5, alpha = .75, colour = \"lightgreen\") +\n  geom_line(size = 1.5) +\n  ylim(0, 1.1) + xlim(0, 16) +\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.25)),\n    plot.title = element_text(size = rel(1.4))\n  ) +\n  xlab(\"Health literacy (HLVA)\") + ylab(\"Mean accuracy\") +\n  ggtitle(\"Effect of health literacy on mean comprehension accuracy\")\n```\n:::\n\n\n:::{.callout-note}\n## Code explanation\n\nNotice that we do this in stages, as we have done for other kinds of plots:\n\n- `p.model <- plot(dat)` - we create a plot object, which we call 'p.model'\n- `p.model + ` - we then set up the first line of a series of lines, starting with the name of the plot, 'p.model' and a `+` to show we are going to add some edits\n- `geom_point(data = all.studies.subjects, aes(x = HLVA, y = mean.acc) ...)` - we first add the raw data points showing the observed HLVA and mean.acc for each person in our sample\n- `geom_point(... size = 1.5, alpha = .75, colour = \"lightgreen\") +` - we modify the appearance of the points, then\n- `geom_line(size = 1.5) +` - we add the prediction line, using the predictions created earlier, then\n- `ylim(0, 1.1) + xlim(0, 16) +` - we set axis limits to show the full potential range of variation in each variable, then\n- `theme_bw() +` - we set the theme to black and white, then\n- `theme(...) +` - we modify the relative size of x-axis, y-axis and plot title label font, then\n- `xlab(\"Health literacy (HLVA)\") + ylab(\"Mean accuracy\") +` - we edit labels to make them easier to understand, then\n- `ggtitle(\"Effect of health literacy on mean comprehension accuracy\")` - we give the plot a title\n:::\n\n11. Now produce plots that show the predictions for all the predictor variables in the model. You can create a set of plots then put them together in a grid for presentation\n\n-- The code may get pretty lengthy\n-- Adjust axis labels so for each plot we see the correct predictor as the x-axis label\n-- In each plot, first plot the original sample observations as points *then* the prediction line\n-- Give the plot titles as letters a-e so that, if you put this plot in a report, you can refer to each plot by letter in comments\n\n## Task 6: Estimate the effects of factors as well as numeric variables \n\nWe have not yet included any categorical or nominal variables as predictors but we can, and should: `lm()` can cope with any kind of variable as a predictor. There are different ways to do this, here we look at two:\n\n11. Fit a linear model including both numeric variables and categorical variables as predictors. We can inspect the data to check what variables are categorical or nominal variables factors using `summary()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(all.studies.subjects)\n```\n:::\n\n\nR shows factors with a count of the number of observations for each level. Include the factor NATIVE.LANGUAGE as a predictor\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE + NATIVE.LANGUAGE, \n            data = all.studies.subjects)\n```\n:::\n\n\n\nQ18.Can you report the estimated effect of NATIVE.LANGUAGE (the \n coding of participant language status: English versus other)? You will need to get a summary of the model using `summary(model)`.\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nThe effect of language status (NATIVE.LANGUAGE) on mean accuracy of understanding is significant (estimate = -0.09, t = -6.37, p < .001) indicating that not being a native speaker of English ('Other') is associated with lower accuracy\n:::\n\nQ19.Can you report the model and the model fit statistics?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nWe fitted a linear model with mean comprehension accuracy as the outcome and health literacy (HLVA),  reading strategy (FACTOR3), vocabulary (SHIPLEY) and AGE (years), as well as language status as predictors The model is significant overall, with F(5, 555) = 81.09, p< .001, and explains 38% of variance (adjusted R2 = 0.42).\n:::\n\nQ16.What changes, when you compare the models with versus without NATIVE.LANGUAGE?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nIf you compare the summaries, for the last two models, you can see that the proportion of variance explained, R-sq, increases to 42% (0.4221), suggesting that knowing about participant language background helps to account for their response accuracy in tests of comprehension of health advice.\n:::\n  \nR handles factors, by default, by picking one level ('English') as the reference level (or baseline) and comparing outcomes to that baseline, for each other factor level (here, 'Other'). Thus, in this model, the effect of 'NATIVE.LANGUAGE' is estimated as the difference in 'mean.acc' outcome for 'English' compared to 'Other' participants. This is why the effect is listed as: `NATIVE.LANGUAGEOther` in the model summary\n\nThere are different ways to code factors for analysis. If you are doing an analysis where your data come from e.g. a factorial design (e.g. a 2 x 2 study design) then you will want to use a different coding scheme: e.g. sum or effect coding. It is easy to do this, in two steps:\n\nWe first change the coding scheme:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(memisc) #get the memisc library\n\ncontrasts(all.studies.subjects$NATIVE.LANGUAGE) #check the coding\n\ncontrasts(all.studies.subjects$NATIVE.LANGUAGE) <- contr.sum(2, base = 1) #change the coding\n\ncontrasts(all.studies.subjects$NATIVE.LANGUAGE) #check the coding to show you got what you want\n```\n:::\n\n\n:::{.callout-warning}\nLoading the memisc library can cause problems when using dplyr (tidyverse) functions like select: the problem is expressed as e.g. select() not being able to see variables listed for selection. You can fix this problem by restarting R or by requiring R to use `dplyr::select()`. I would normally avoid problems like this but the memisc `contr.sum()` function is too useful to ignore.\n:::\n\n-- We use the memisc library because it provides some functions that are convenient to use\n-- `contrasts(all.studies.subjects$NATIVE.LANGUAGE)` shows us the R default called dummy coding:\n\n| Other   |     |\n|---------|-----|\n| English |   0 |\n| Other   |   1 |  \n \n-- `contrasts(all.studies.subjects$NATIVE.LANGUAGE) <- contr.sum(2, base = 1)` - we use the `contr.sum()` function to tell R how many levels there are for the factor (2: 'NATIVE.LANGUAGE' is coded as 'Other' or 'English'), and we use `contr.sum()` function to tell R which level we want to be the reference level\n\nWe then fit a linear model of the outcome, given our predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE + NATIVE.LANGUAGE, \n            data = all.studies.subjects) #tell R you want a model of the outcome 'mean.acc' predicted (~) by the predictors 'HLVA', 'SHIPLEY', 'FACTOR3', 'AGE', 'NATIVE.LANGUAGE'\n\nsummary(model) #get a model summary\n```\n:::\n\n\n\n\nQ17.What changes, when you compare the models with versus without sum coding of NATIVE.LANGUAGE?\n\n:::{.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\nIf you compare the summaries, for the last two models, you can see that the estimate for the coefficient of 'NATIVE.LANGUAGE' changes in two ways: the name changes, from NATIVE.LANGUAGEOther to NATIVE.LANGUAGE2, reflecting the change in coding; the slope estimate changes, from -0.0900035 to -0.0450018\n\nNote: the change in the estimate happens because we go from estimating the average difference in level between 0 ('English') versus 1 ('Other'), a change of one unit to estimating the average difference in level between -1 ('English') versus 1 ('Other'), a change of 2 units\n:::\n\nBeing able to change the coding of nominal or categorical variables is very useful and enables you to do ANOVA style analyses given factorial study designs e.g.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- aov(lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE + NATIVE.LANGUAGE, \n       data = all.studies.subjects))\nsummary(model)\n```\n:::\n\n\nNotice: we use `aov()` to get an ANOVA summary.\n\n\n12. Fit a linear model including both numeric variables and categorical variables as predictors: and then plot the predicted effect of the factor. We first fit the model, including NATIVE.LANGUAGE then use the `ggpredict()` function to get the predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- ggpredict(model, \"NATIVE.LANGUAGE\")\nplot(dat)\n```\n:::\n\n\nYou can read more about factor coding schemes [here](https://talklab.psy.gla.ac.uk/tvw/catpred/) and [here](https://phillipalday.com/stats/coding.html).\n\n\n## Optional Task: Work out how to recode factor levels \n\nFactors categorical or nominal variables are a special kind of variable in R and there is a library of functions, forcats, you can familiarise yourself with, if you are going to be working with factors\n\nHere, we look at a common task, recoding the levels of a factor\n\nYou might want to do this e.g. if you are worried about imbalances in factor level numbers\n\n- Change the ways that the levels of a factor are coded\n\n- Consider the factor EDUCATION in the all.studies.subjects dataset. Can you find out how many observations there are, of participants at each level of (self-reported) education? You can use `summary()` for this\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(all.studies.subjects$EDUCATION)\n```\n:::\n\n\n\n|||\n|--|-|\n|Further| Secondary |   Higher             |\n|229 |       66 |      320   |              |\n\n\nWhat if we think that it does not make much sense to distinguish between Further and Secondary?\n\n- Change Further and Secondary to School in the way that EDUCATION responses are coded\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.studies.subjects <- all.studies.subjects %>%\n  mutate(education = fct_recode(EDUCATION,\n                                'School' = 'Further',\n                                'School' = 'Secondary'\n                                ))\n```\n:::\n\n\nNotice we work through the steps:\n- `all.studies.subjects <- all.studies.subjects %>%` - create a new dataset from the old dataset\n- `mutate(education = fct_recode(EDUCATION,` - create a new variable, using mutate(). I call the new variable 'education' and create the new variable by recoding the old variable 'EDUCATION', then use the function fct_recode() to do the recoding\n- *inside* fct_recode() you specify how you want the recoding to work:\n 'School' = 'Further',\n 'School' = 'Secondary' \n'School' = 'Further' means: new level name = old level name\n\nInspect what we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(all.studies.subjects$EDUCATION)  \nsummary(all.studies.subjects$education)  \n```\n:::\n\n\nNotice in the new 'education' both 'Further' and 'Secondary' are now classed as 'School'\n\n\nHow do you recode ETHNICITY? Maybe we think we need to recode ETHNICITY so that we use the simplified scheme: White versus BAME like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.studies.subjects <- all.studies.subjects %>%\n  mutate(ethnicity = fct_recode(ETHNICITY,\n                                'BAME' = 'Asian',\n                                'BAME' = 'Black',\n                                'BAME' = 'Mixed',\n                                'BAME' = 'Other'\n                                ))\n```\n:::\n\n\nInspect what we get:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(all.studies.subjects$ETHNICITY)  \nsummary(all.studies.subjects$ethnicity)\n```\n:::\n\n\n## Optional task: To examine associations comparing data from different samples \n\nThe lecture for developing the linear model includes a discussion of the ways in which the observed associations between variables or the estimated effects of  predictor variables on some outcome may differ between different studies, different samples of data\n\nTo draw the plots, I used `facet_wrap`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.studies.subjects %>%\n  ggplot(aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(size = 2, alpha = .5, colour = \"darkgrey\") +\n  geom_smooth(size = 1.5, colour = \"red\", method = \"lm\", se = FALSE) +\n  xlim(0, 40) +\n  ylim(0, 1.1)+\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.5))\n  ) +\n  xlab(\"Vocabulary (Shipley)\") + ylab(\"Mean accuracy\") +\n  facet_wrap(~ study)\n```\n:::\n\n\nWhat is new here is this bit: `facet_wrap(~ study)`\n\n:::{.callout-note}\n## Code explanation\nThis is how it works:\n- `facet_wrap()` - the function asks R to take the dataset and split it into subsets\n`facet_wrap(~ study)` - tells the function to split the dataset according to the different levels of a named factor: 'study'\n\nSo: you need identify a factor variable to do this\n:::\n\n## Optional Task\n\nChange the factor in facet_wrap() to show how the vocabulary effect may vary between English monolinguals versus non-native speakers of English\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nall.studies.subjects %>%\n  ggplot(aes(x = SHIPLEY, y = mean.acc)) +\n  geom_point(size = 2, alpha = .5, colour = \"darkgrey\") +\n  geom_smooth(size = 1.5, colour = \"red\", method = \"lm\", se = FALSE) +\n  xlim(0, 40) +\n  ylim(0, 1.1)+\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.5))\n  ) +\n  xlab(\"Vocabulary (Shipley)\") + ylab(\"Mean accuracy\") +\n  facet_wrap(~ NATIVE.LANGUAGE)\n```\n:::\n\n\n\nYou can read more about faceting [here](https://ggplot2.tidyverse.org/reference/facet_wrap.html)\n\n# Part Two: The workbook\n\n In Week 10, we aim to *further* develop skills in working with the linear model, and in visualizing and testing the associations between variables in psychological data\n\nWe do this to learn how to answer research questions like:\n-- What person attributes predict success in understanding?\n-- Can people accurately evaluate whether they correctly understand written health information?\n\n These kinds of research questions can often be answered through analyses using linear models. We will use linear models to estimate the association between predictors and outcomes\n\n\n When we do these analyses, we will need to think about how we report the results:  we usually need to report information about the kind of model we specify;and we will need to report the nature of the associations estimated in our model; we usually need to decide, is the association between the outcome and any one predictor significant? Does that association reflect a positive or negative relationship between the outcome and that predictor?are the associations we see in sample data relatively strong or weak?\n\n We will consolidate and extend learning on data visualization: Use scatterplots to examine the relationships we may observe or predict Generate predictions given model estimates of slope coefficients\n\n\n## Task 1: Set-up \n\n13. Run this code to empty the R environment `rm(list=ls())`                          \n\n\n14. Run this code to load relevant libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"ggeffects\")\nlibrary(\"patchwork\")\nlibrary(\"psych\")\nlibrary(\"tidyverse\")\n```\n:::\n\n\n\nIn this how-to guide, we use a collection of data, drawing together data from a series of studies, completed by BSc and MSc students, as replications of the clearly-understood health comprehension project investigations using \"2022-12-08_all-studies-subject-scores.csv\"\n\n## Task 2: Load data \n\n\n15. Read in the data file we will be using: 2022-12-08_all-studies-subject-scores.csv using `read_csv` and col_types = cols() function to tell R how to identify nominal variables as factors within the dataset\n\n\n16. Get summary statistics for *only* the numeric variables: AGE, HLVA. Here, we use the describe() function from the 'psych' library. Here, we can do this in two steps: \n-- we select the variables we care about\n-- we get just the descriptive statistics we want for those variables\n\n\nQ1. What is the mean health literacy (HLVA) score in this sample?\n\nQ2. What are the minimum and maximum ages in this sample?\n\nQ3. Do you see any reason to be concerned about the data in this sample? It is always a good idea to use your visualization skills to \n examine the distribution of variable values in a dataset\n\n## Task 3: Use a linear model to to answer the research questions one predictor \n\nOne of our research questions is: Can people accurately evaluate whether they correctly understand written \n health information?\n\n\nWe can address this question by examining whether someone's rated evaluation of their own understanding matches their performance on a test of that understanding, and by investigating what variables predict variation in  mean self-rated accuracy\n\nNote that ratings of accuracy are ordinal data but that, here, we may choose to examine the average of participants' ratings of their own understanding of health information to keep things fairly simple\n\nFor these data, participants were asked to respond to questions about health  information to get 'mean.acc' scores and were asked to rate their own understanding  of the same information\n\nIf you *can* evaluate your own understanding then ratings of understanding *should* be associated with performance on tests of understanding\n\n\n17. Estimate the relation between outcome mean self-rated accuracy ('mean.self') and tested accuracy of understanding ('mean.acc'). For these data, participants were asked to respond to questions about health information to get 'mean.acc' scores and were asked to rate their own understanding of the same information. We can use `lm()` to estimate whether the ratings of accuracy actually predict the outcome tested accuracy levels\n\n\nIf you look at the model summary you can answer the following questions  \n\nQ4. What is the estimate for the coefficient of the effect of the predictor 'mean.self' on the outcome 'mean.acc' in this modelo?\n\nQ5. Is the effect significant?\n\nQ6. What are the values for t and p for the significance test for the coefficient?\n\nQ7. What do you conclude is the answer to the research question, given the linear model results?\n\nQ8. What is the F-statistic for the regression? Report F, DF and the p-value.\n\nQ9. Is the regression significant?\n\nQ10. What is the Adjusted R-squared?\n\nQ11.Explain in words what this R-squared value indicates?\n\n## Task 4: Use a linear model to to answer the research questionsmultiple predictors \n\nOne of our research questions is: Can people accurately evaluate whether they correctly understand written health information?\n\nWe have already looked at this question by asking whether ratings of understanding predict performance on tests of understanding\nBut there is a problem with that analysisit leaves open the question:  what actually predicts ratings of understanding?\n\nWe can look at that follow-up question, next\n\n\n18. Examine the relation between outcome mean self-rated accuracy ('mean.self') and multiple predictors including: health literacy ('HLVA'); vocabulary ('SHIPLEY'); 'AGE'; reading strategy ('FACTOR3'); as well as 'mean.acc'. We use lm(), as before, but now specify each variable listed  here by variable name\n\nIf you look at the model summary you can answer the following questions  \n\nQ12. What predictors are significant in this model?\n\nQ13. What is the estimate for the coefficient of the effect of the predictor,  'mean.acc', in this model?\n\nQ14. Is the effect significant?\n\nQ15. What are the values for t and p for the significance test for the coefficient?\n\nQ16. What do you conclude is the answer to the follow-up question, \n what actually predicts ratings of understanding?\n\n## Task 5: Understanding linear model predictions by comparing one outcome-predictor relation \n\nNext, we focus in on whether 'mean.self' predicts 'mean.acc' or, in reverse, whether 'mean.acc' predicts 'mean.self'?\n\nNote that the comparison between these models teaches us something about *what* linear models predict\n  \n\nQ17. Why do you think it appears that the slope coefficient estimate is different if you compare:\n-- the model, mean.acc ~ mean.self, versus \n-- the model, mean.self ~ mean.acc?\n\nYou want to fit two simple models here, using the verbal description in the Q14 wording. You may benefit by reflecting on the lm-intro lecture and practical materials, especially where they concern predictions\n\nQ18. Can you plot the predictions from each model? First fit the modelsgive the model objects distinct names. Then get the predictions. Then make the plots\n\nQ19. Look at the two plots side-by-side: what do you see? Look at changes in height of the prediction line, given \n changes in x-axis position of the line\n\n## Task 6: Estimate the effects of factors as well as numeric variables \n\n\nWe have not yet included any categorical or nominal variables as predictors but we can, and should: lm() can cope with any kind of variable as a predictor\n\nThere are different ways to do this, here we ask you to use the R default method:\n\n\n19. Fit a linear model to examine what variables predict outcome mean self-rated accuracy of 'mean.self' include in the model both numeric variables and categorical variables as predictors: health literacy ('HLVA'); vocabulary ('SHIPLEY'); 'AGE'; reading strategy ('FACTOR3'); as well as 'mean.acc' and 'NATIVE.LANGUAGE'\n\n\n20. Can you report the estimated effect of 'NATIVE.LANGUAGE' (the coding of participant language status: 'English' versus 'other')? You will need to get a summary of the model\n\n21. Can you report the overall model and model fit statistics?\n\nQ20. Can you plot the predicted effect of 'NATIVE.LANGUAGE' given your model? We first fit the model, including 'NATIVE.LANGUAGE' then use the `ggpredict()` function to get the predictions\n\nQ21. The plot should give you dot-and-whisker representations of the estimated 'mean.self' for 'English' versus 'Other' participants in the dataset. What is the difference in the estimated 'mean.self' between these groups? The effect or prediction plot will show you dot-and-whisker representations of predicted outcome 'mean.self'. In these plots, the dots represent the estimated 'mean.self' while the lines (whiskers) represent confidence intervals\n\nQ22.Compare the difference in the estimated 'mean.self' between these groups, given the plot, with the coefficient estimate from the model summary: what do you see?\n\n## Optional task: Examine associations comparing data from different samples \n\nThe lecture for developing the linear model includes a discussion of the ways in which the observed associations between variables or the estimated effects of  predictor variables on some outcome may differ between different studies, different samples of data\n\n\nChange the factor in `facet_wrap()` to show how the association between 'mean.self' and 'mean.acc' can vary between the different studies in the dataset\n\nYou can read more about faceting [here](https://ggplot2.tidyverse.org/reference/facet_wrap.html)\n\nYou may need to edit the x-axis labeling to make it readable. Can you work out how to do that, given `ggplot()` help information? Check out the continuous scale information [here](https://ggplot2.tidyverse.org/reference/scale_continuous.html)\n\n## Optional Task: Save or export plots so that you can insert them in reports \n\nThis is something you have not seen before but it is worth exploring because\n it may help you later with reports\n\n\nSave or export a plot that you produce, so that you can insert it in a report or presentation. There are different ways to do this, we can look at one simple example\n\nWe can save the last plot we produce using the tidyverse function ggsave(): `ggsave(\"facet-plots.png\")`. Notice, here, that:\n-- `ggsave(\"facet-plots...\")` - we need to give the plot a name\n-- `ggsave(\"...png\")` - and we need to tell R what format we require\n\n\nThe plot is saved as a file with the name you specify, in the working directory you are using\n\nR will save the plot in the format you specify: here, I choose .png because  .png image files can be imported into Microsoft Word documents easily Notice that ggsave() will use pretty good defaults but that you can over-ride  the defaults, by adding arguments to specify the plot width, height and resolution  (in dpi)\n\n\nCheck out reference information [here](https://ggplot2.tidyverse.org/reference/ggsave.html) and [here](http://www.cookbook-r.com/Graphs/Output_to_a_file/)\n\n\nNow try it for yourself: make a plot, and save it, using what you have learnt so far\n \nFit a model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(mean.acc ~ HLVA + SHIPLEY + FACTOR3 + AGE, \n            data = all.studies.subjects)\n```\n:::\n\n\nCreate a set of predictions we can use for plotting  \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- ggpredict(model, \"FACTOR3\")\n```\n:::\n\n\nmake a plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\np.model <- plot(dat)\np.model +\n  geom_point(data = all.studies.subjects, \n             aes(x = FACTOR3, y = mean.acc), size = 1.5, alpha = .75, colour = \"darkgrey\") +\n  geom_line(size = 1.5) +\n  theme_bw() +\n  theme(\n    axis.text = element_text(size = rel(1.15)),\n    axis.title = element_text(size = rel(1.25)),\n    plot.title = element_text(size = rel(1.4))\n  ) +\n  xlab(\"Reading strategy (FACTOR3)\") + ylab(\"Mean accuracy\") +\n  ggtitle(\"Effect of reading strategy on \\n mean comprehension accuracy\")\n```\n:::\n\n\nSave it\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggsave(\"reading-strategy-prediction.png\", width = 10, height = 10, units = \"cm\")\n```\n:::\n\n\nSome advice: It is often helpful to present plots that are almost square ie height about = width. This helps to present the best fit line in scatterplots in a way that is easier to perceive. We may experiment with width-height (aspect ratio) until we get something that \"looks\" right. Notice also that different presentation venues will require different levels if image  resolution e.g. journals may require .tiff file plots at dpi = 180 or greater\n\n# Assignment\n\nThe link can be accessed [here](https://modules.lancaster.ac.uk/mod/quiz/view.php?id=1921474)\n\nThe data needed is downloaded [here](data/Wk10 assignment.zip)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}