{
  "hash": "de631dfcf02ce7024c9e1d0eb2ae53fa",
  "result": {
    "markdown": "---\ntitle: 3. Assumptions of ANOVA and Follow-Up Procedures\nsubtitle: \"Richard Philpot, Mark Hurlstone\"\norder: 4\n---\n\n\n# Lecture\n\nWatch Part 1 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2002049)\n\nWatch Part 2 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2002050)\n\nWatch Part 3 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2002051)\n\nWatch Part 4 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2002055)\n\nDownload the lecture slides [here](data/Wk3/Psyc214 - Lecture 3 Slides [small].pdf), and [here](data/Wk3/Psyc214 - Lecture 3 Slides [large].pdf) for a larger version.\n\n# Lab\n\nWelcome back to Lab 3 of Year 2 Stats! \\\n\\\nIt's that wonderful time of the week again - a time to tango with R studio. As with preceding weeks, we will be working from an activity sheet, which outlines a number of decent tasks to complete in R Studio. The objectives of today's lab are three fold:\n\n1. Running tests of the assumptions of ANOVA.\n2. Run and report planned comparisons and post-hoc tests - You choose which ;)\n3. Debug code which isn't doing the business.\n\nAs always, myself (Richard Philpot, weeks 1-4), Mark Hurlstone (weeks 5-9) and an apt team of GTAs will be circulating through the room and answering any queries you may have. In Psyc214, we also encourage peer engagement and joined problem solving - so please do not hesitate to ask for help from another on your table or to work together in small groups. Right, that enough for now, so let's get started!ðŸ’ª\n  \n******\n### 1 - General Introduction to Lab 3\n\n| *\"It does not matter how slowly you go, as long as you do not stop.\"* | Confucius.\n\n![Confucius](https://www.weekinchina.com/app/uploads/2019/12/Confucius.jpg)\n  \n******\n### 1.1 Access to R Studio\n\n---\ntitle: \"Statistics for Psychologists\"\npage-layout: article\n---\n\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS' [instructions here](https://portal.lancaster.ac.uk/ask/digital/services/university-it-network/vpn/) or connecting to Eduroam [here](https://portal.lancaster.ac.uk/ask/digital/services/university-it-network/wi-fi/).\n\nWhen you are connected, navigate to [https://psy-rstudio.lancaster.ac.uk](https://psy-rstudio.lancaster.ac.uk), where you will be shown a login screen that looks like the below. Click the option that says \"Sign in with SAML\".\n\n![](/Includes/1LoginSAML.png) <!-- says no image but it's all about the include setup -->\n\nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n![](/Includes/2LoginUser.png)\n\n![](/Includes/3LoginPass.png)\n\n::: {.callout-note}\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n:::\n\n\n\n\n******\n#### 1.2 Loading the packages (dependencies) and loading the data set\n\nLovely stuff. Here we are, up in the server clouds of R Studio - a plain of statistical delight. \n\nSimilar to last week, we first need to access the dataset. \nTo access today's dataset, we first just need to download it from Moodle.\n\n1. Step 1. Please ensure that you download today's data from the Psyc214 week 3 'lab' folder in Moodle. Please download the *week3_robo_lab.csv* file from [here](data/Wk3/week3_robo_lab.csv).\n\n2. Now that you have the data saved on your machine, we next need to make a space on our own individual R Studio servers in which to house the data. Like last week, to do this, we first need to create a folder. On the server, please navigate to the bottom right panel (see figure below). Here, under the 'files' you will see the option to add 'New Folder'. Click on this and name the new folder *psyc214_lab_3* Note. This needs to be spelled correctly and we need to make sure we that we specify this is lab_3! We are so over lab_1 and lab_2 that it's not even funny. \n\n\n3. Now that you have a folder in waiting, it's time to add week 3's data file. To upload today's *week3_robo_lab.csv* file into this folder, please open your new *psyc214_lab_3* folder. When in the new folder, select the 'Upload' tab (see figure below). This will present a box that will ask where the data is that you want to upload. Click on \"Browse...\", find where you have the *week3_robo_lab.csv* data file on your computer and click 'OK'\n\nPerfect! The data is now sat in its slippers and dressing gown on your own R Studio server, waiting to be called upon.\n\n**STOPPPPPPPPP!!!!! We'll want to be able to save this session on our server: to do so please:**\n**click 'File' on the top ribbon --> New project.**\n**Next, select existing directory and name the working directory ~/psyc214_lab_3 --> hit 'create project'**\n\n**STOPPPPPP!!!!! Please ensure you have created a script and you are working in this script - top left of the pane of the R Studio Server. This will make it easier for you to return to your code at a later date. To do this, please click 'File' --> 'New file' --> 'Create new R script'**\n\nThis will now create a project and script file in this week's R Studio Server lab 3 folder where it will save your progress! You can then return to this at a later date - likely close to the exam *wink wink*\n\nAs always, we should start our new session by loading the packages (i.e., the dependencies) we'll use for this session.\n\nPlease copy or type the following commands into R Studio to get these packages activated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # allows pipes, arrang(), summary(), aov(), etc\nlibrary(rstatix) # allows group_by()\nlibrary(effectsize) # a neat little package that provides the effect size\nlibrary(ggpubr) # allows us to examine QQ Plots\n```\n:::\n\n\nMore on these packages later.\n\nOk - Time to load the data and roll on today's work.\n\n  4. First we need to set the working directory to 'psyc214_lab_3', i.e., tell R Studio the location in which today's data sits, waiting and lurking. To recap, the working directory is the default location or folder on your computer or server by which R will read/save any files. \n  \n  The working directory can be set with the following R code:\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n  setwd(\"~/psyc214_lab_3\")\n```\n:::\n\n\n\n  5. Now the have the directory set up, let's type the command to read the housed data.\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n lab3_data <- read_csv(\"week3_robo_lab.csv\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n  \n  where 'lab3_data' is the name we've assigned that R will recognise when it calls up our data, 'read_csv' is the R tidyverse command to pull up the data and \"week3_robo_lab.csv\" is the name of the data file stored on the server. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set condition to a factor with 3 levels: A, B, O\nlab3_data$Group = factor(lab3_data$Group, levels = c(\"A\",\"B\",\"O\"))\n```\n:::\n\n\n------ \n### 2 - Today's lab activities \n\nWith the working directory set, the data loaded and the packages called, it is now time to have a play.\n\n\n#### 2.1 Some background information about the dataset\n\n![Introducing the robo-lab study](https://images.pexels.com/photos/8386434/pexels-photo-8386434.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260)\n\nWe are now all well versed in the mad goings on at Lancaster University Psychology Department. The crazy cats there wanted to examine whether Psyc214 students would respond well to an ambitious initiative which would introduce synthetic robots as stats lab assistants. This began with the creation and assessment of two robots (Robot A[lpha] and B[eta]). Applying a between-participants design, the researchers found that those students assigned Robot B(eta) rated their robot as significantly more likable than those assigned Robot A(lpha). Furthermore, Robot B(eta) students did significantly better overall in their Psyc214 exam. \n\nA grumpy reviewer, however, suggested the research was flawed, as it did not include a third, more realistic robot. Citing the 'uncanny valley' phenomenon, this reviewer expected that a more realistic looking robot would be less likable and that its assigned students would do significantly poorer in their Psyc214 assessments. The researchers set out to satisfy this reviewer and developed a third robot. Unlike the reviewer, however, the researchers expected that the most realistic robot would receive both the highest likeability scores and Psych214 assessment scores. The research team expected the following:\n\n- Likeability attitudes towards a participant's robot would be significantly higher for individuals assigned Robot O(mega) than those assigned Robot A(lpha) or B(eta) **(Hypothesis 1)**\n\n- Psyc214 assessment scores would be significantly higher amongst individuals assigned Robot O(mega) than those assigned Robot A(lpha) or B(eta) **(Hypothesis 2)**\\\n\n\n\n![**Robot A(lpha)**](https://images.unsplash.com/photo-1580835239846-5bb9ce03c8c3?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=976&q=80)\n![**Robot B(eta)**](https://images.pexels.com/photos/2599244/pexels-photo-2599244.jpeg?auto=compress&cs=tinysrgb&h=750&w=1260)\n![**Robot O(mega)**](https://images.unsplash.com/photo-1593376893114-1aed528d80cf?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=958&q=80)\n\n\n**Last week, in our lab, we ran a one-factor between-participants ANOVA and found that our three groups statistically differed: both in their perceptions of how likeable their assigned robot was, but also in their achievement scores for the Psyc214 assessment. What we've learned in our lectures, however, is that while an ANOVA can detect likely statistical differences between groups, it cannot pinpoint specifically which groups varied from another. Is it Alpha and Beta? Or Alpha and Omega? Or Beta and Omega? Questions remain...**\n\n\n******\n#### 2.5 Refamiliarizing with our data\nJust for a recap, let's open our data and look at the top 50 data points:\n\n**Full disclaimer, from this point on, we will start to include some bugs in code which you will need to resolve. Wicked of us, I know, but it is important that you are able to engage with the code and correct it when needs be. Myself (Richard) and Mark H have more experience in R than yourselves, but still frequently and inadvertently write funky code that requires us to go back and work through our (il)logic - it's a fact of coding life that never goes away.**\n\n![The frustration of debugging!]https://images.pexels.com/photos/2868257/pexels-photo-2868257.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260)\n\n  \nPlease call up the data using the `head()` function, and ask for the top 50 data points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  head(lab3 data, n = x)\n```\n:::\n\n\nWhat a piece of work these lecturers are. For some reason this code is buggy. Try to figure out what went wrong and how you can correct it. *hint, it's something with the data name and number of data points being called.*\n\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\n  head(lab3_data, n = 50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 Ã— 4\n      ID Group Likeability Score\n   <dbl> <fct>       <dbl> <dbl>\n 1     1 A               2    71\n 2     2 A               2    53\n 3     3 A               3    62\n 4     4 A               3    65\n 5     5 A               4    64\n 6     6 A               1    61\n 7     7 A               3    63\n 8     8 A               3    50\n 9     9 A               2    59\n10    10 A               4    69\n# â€¦ with 40 more rows\n```\n:::\n:::\n\n\nIf all has gone well and you've forgiven us, you should get the following:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 Ã— 4\n      ID Group Likeability Score\n   <dbl> <fct>       <dbl> <dbl>\n 1     1 A               2    71\n 2     2 A               2    53\n 3     3 A               3    62\n 4     4 A               3    65\n 5     5 A               4    64\n 6     6 A               1    61\n 7     7 A               3    63\n 8     8 A               3    50\n 9     9 A               2    59\n10    10 A               4    69\n# â€¦ with 40 more rows\n```\n:::\n:::\n\n:::\n\nBecause of the way we ordered our data in the original .csv spreadsheet, we are not able to see beyond the first 50 cases in Group A. Therefore, let's use the arrange() function to order our data instead by Score, highest first - we can combine this with the head() through the use of a pipe %>%. *Please recall, a pipe works as a specialized chain, letting you pass an intermediate result onto the next function.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab3_data %>% arrange(Score) %>% head(n=50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 Ã— 4\n      ID Group Likeability Score\n   <dbl> <fct>       <dbl> <dbl>\n 1   112 B               4    40\n 2    34 A               3    44\n 3   159 B               4    44\n 4    50 A               2    46\n 5    68 A               2    46\n 6   104 B               2    46\n 7   160 B               5    46\n 8    20 A               3    47\n 9   106 B               3    47\n10   186 O               1    47\n# â€¦ with 40 more rows\n```\n:::\n:::\n\n\nThis is great. Well, kind of. The order is the wrong way round!? i.e., it's ordered from lowest to highest - this will never do. Let's have another try using the (desc(VARIABLE). If you struggle, please ask for help.\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\n  lab3_data %>% arrange(desc(Score)) %>% head(n=50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 Ã— 4\n      ID Group Likeability Score\n   <dbl> <fct>       <dbl> <dbl>\n 1   207 O               2    79\n 2   199 O               2    77\n 3   222 O               1    77\n 4   175 O               3    75\n 5   176 O               1    75\n 6   210 O               2    75\n 7   219 O               1    75\n 8   121 B               3    74\n 9   142 B               5    74\n10   161 O               2    74\n# â€¦ with 40 more rows\n```\n:::\n:::\n\n\nOK - even I admit that was cruel, but hopefully you have the following: \n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 Ã— 4\n      ID Group Likeability Score\n   <dbl> <fct>       <dbl> <dbl>\n 1   207 O               2    79\n 2   199 O               2    77\n 3   222 O               1    77\n 4   175 O               3    75\n 5   176 O               1    75\n 6   210 O               2    75\n 7   219 O               1    75\n 8   121 B               3    74\n 9   142 B               5    74\n10   161 O               2    74\n# â€¦ with 40 more rows\n```\n:::\n:::\n\n:::\n\nOK - now let's do the same and check the 50 highest Likeability values.\nTo do this, please use the same successful code as above, but change 'Score' to 'Likeability'\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\n  lab3_data %>% arrange(desc(Likeability)) %>% head(n=50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 Ã— 4\n      ID Group Likeability Score\n   <dbl> <fct>       <dbl> <dbl>\n 1   238 O              33    63\n 2   126 B               7    50\n 3    83 B               6    53\n 4    94 B               6    72\n 5    99 B               6    64\n 6   108 B               6    70\n 7   115 B               6    64\n 8   116 B               6    60\n 9   123 B               6    63\n10   129 B               6    60\n# â€¦ with 40 more rows\n```\n:::\n:::\n\n\nIf done correctly you will receive the following output\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 Ã— 4\n      ID Group Likeability Score\n   <dbl> <fct>       <dbl> <dbl>\n 1   238 O              33    63\n 2   126 B               7    50\n 3    83 B               6    53\n 4    94 B               6    72\n 5    99 B               6    64\n 6   108 B               6    70\n 7   115 B               6    64\n 8   116 B               6    60\n 9   123 B               6    63\n10   129 B               6    60\n# â€¦ with 40 more rows\n```\n:::\n:::\n\n\n![Hold up!](https://images.unsplash.com/photo-1542707309-4f9de5fd1d9c?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=987&q=80)\n\nNow wait! We almost forgot! Our data is a tad on the dodgy side. We can see that the highest likeability score was 33. This can't be right? Can it?! Nope, definitely not, the scale should range from 1 - 7, meaning this is an error.\n\nLet's straighten this out, post haste.\nTo do this, please type or copy/paste the following command\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab3data$likeability[lab3_data$Likeability > 7] <- NA # sets any Likeability value over 7 to missing\n```\n:::\n\n\nYou've got to be kidding me! Yet another bug in the code? Well, I guess it's up to us to roll up our sleeves and sort out this mess.\n\nPlease try to correct the code and get this sorted. *hint, there seems to be something funky going on with the name of the data variable and the DV.*\n\n\n![Don't lose your heads, guys!](https://images.unsplash.com/photo-1507162728832-5b8fdb5f99fa?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=2070&q=80\")\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n\nIf now sorted, you should have written the following\n\n::: {.cell}\n\n```{.r .cell-code}\nlab3_data$Likeability[lab3_data$Likeability > 7] <- NA # sets any Likeability value over 7 to missing\n```\n:::\n\n:::\n\nThis is asking R Studio to scan the likeability column of our dataset and to set any value above 7 as missing.\n\nNow let's check out the top 3 likeability scores again, just to be sure the 33 has gone.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  lab3_data %>% arrange(desc(Likeability)) %>% head(n=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 4\n     ID Group Likeability Score\n  <dbl> <fct>       <dbl> <dbl>\n1   126 B               7    50\n2    83 B               6    53\n3    94 B               6    72\n```\n:::\n:::\n\n\nNo nasty tricks this time. Everything looks good.\n\nLet's now use the `rstatix()` package to get some neat and tidy summary statistics. \n\nWe first specify the dataset we will use - which is lab3_data. After this we will use a pipe to ask R Studio to pass this data set on to the next function. Here, we use the `group_by()` function to specify that we want to distinguish between our three different groups - a variable conveniently named 'Group'. We then use another pipe to pass the intermediate result onto the next function - the `get_summary_stats()`. In the parentheses we include our two DVs 'Likeability' and 'Score'. Finally we are asked to specify what 'type' of summary statistic we would like. Let's go for mean, standard deviation, min and max values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nDescriptives = lab3_data %>%\n  group_by(Group) %>%\n  get_summary_stats(Likeability, Score, show = c(\"mean\", \"sd\", \"min\", \"max\"))\noptions(digits = 3)\nprint.data.frame(Descriptives)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Group    variable  n  mean    sd min max\n1     A Likeability 80  2.50 0.928   1   4\n2     A       Score 80 58.10 6.449  44  72\n3     B Likeability 80  4.50 1.006   2   7\n4     B       Score 80 60.36 7.266  40  74\n5     O Likeability 79  2.11 0.847   1   4\n6     O       Score 80 63.60 7.217  47  79\n```\n:::\n:::\n\n\nOk, perfect. This reminds us of a lot of vital information. For example, we can see that the mean Likeability scores for Groups A, B, O are 2.5, 4.5, 2.11, respectively.\n\nSomething we don't have, however, is the median, which is denoted as \"median\". Repeat the code above, but now also include in the c() the \"median\". *Hint, don't forget the comma.*\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\nDescriptives = lab3_data %>%\n  group_by(Group) %>%\n  get_summary_stats(Likeability, Score, show = c(\"mean\", \"sd\", \"min\", \"max\", \"median\"))\noptions(digits = 3)\nprint.data.frame(Descriptives)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Group    variable  n  mean    sd min max median\n1     A Likeability 80  2.50 0.928   1   4    2.0\n2     A       Score 80 58.10 6.449  44  72   58.0\n3     B Likeability 80  4.50 1.006   2   7    4.0\n4     B       Score 80 60.36 7.266  40  74   60.5\n5     O Likeability 79  2.11 0.847   1   4    2.0\n6     O       Score 80 63.60 7.217  47  79   63.0\n```\n:::\n:::\n\n\nWhen added, you should get the following\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n  Group    variable  n  mean    sd min max median\n1     A Likeability 80  2.50 0.928   1   4    2.0\n2     A       Score 80 58.10 6.449  44  72   58.0\n3     B Likeability 80  4.50 1.006   2   7    4.0\n4     B       Score 80 60.36 7.266  40  74   60.5\n5     O Likeability 79  2.11 0.847   1   4    2.0\n6     O       Score 80 63.60 7.217  47  79   63.0\n```\n:::\n:::\n\n:::\n\n******\n#### 3.1 One-factor between group ANOVA - Assumptions\n\nOk. Enough dilly dallying. It is time to re-run our one-factor between group ANOVA and to check that we were not daydreaming last week - there were actually statistical differences between our robot groups on both the Likeability scores and the Psyc214 assessment scores. \n\nBefore we embark on that adventure, however, it is good practice to test the assumptions of ANOVA, to ensure that we are not violating expectations of data and that a parametric test is, indeed, suitable.\n\nAs you will recall from Lecture 3, there are 3 key assumptions of ANOVA.\\\n1) The assumption of independence (i.e., are our data points unique? - let's say 'yes!')\\\n2) The assumption of normality (i.e., are our groups showing normal distributions with their data points?)\\\n3) The assumption of homogeneity of variance #mouthful (i.e., is the variance in data points similar for all groups?)\\\n\nThe first one - assumption of independence - we need to search our souls and ask if this is the case. Let's say \"yes, yes it was\".\n\nThe other two - assumption of normality and homogeneity variance - we can test by analytical means.\n\nLet's give it a go! (To be fair, we should have done this last time before running our ANOVA in the first place!)\n\nWe should also check if are there any extreme values in our data - remember Lecture 3. These extreme values can come from data entry errors (like with the 33), measurement errors or be unusual values. Extreme values can seriously impact the conclusions we can make about our data.\n\n### 3.1.2 Extreme values\n\nWe can eye ball boxplots to look for suspicious data points, or we can test for extreme values very easily using the rstatix() package. It has this wonderful function called identify_outliers(), which will tell us if we have any outliers or extreme values. As a rule, outliers are typically fine to work with, extremes however are when things become a bit hairy.\n\nPlease copy the following code:\n\n::: {.cell}\n\n```{.r .cell-code}\nlab3_data %>% \n  group_by(Group) %>%\n  identify_outliers(Likeability)# the function to identify outliers and extreme values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 6\n  Group    ID Likeability Score is.outlier is.extreme\n  <fct> <dbl>       <dbl> <dbl> <lgl>      <lgl>     \n1 B       104           2    46 TRUE       FALSE     \n2 B       126           7    50 TRUE       FALSE     \n```\n:::\n:::\n\n\nGreat. This has told us that we have a couple of 'TRUE' outlier values (participants 104 and 126) for Group B(eta), but no extremes (denoted by 'FALSE'), so we can relax.\n\nNow please reproduce the code above, but ask R Studio whether there are any outliers for our 'Score' variable:\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\nlab3_data %>% \n  group_by(Group) %>%\n  identify_outliers(Score)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 6\n  Group    ID Likeability Score is.outlier is.extreme\n  <fct> <dbl>       <dbl> <dbl> <lgl>      <lgl>     \n1 B       112           4    40 TRUE       FALSE     \n```\n:::\n:::\n\n\nWhen implemented correctly, you should get the following:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 6\n  Group    ID Likeability Score is.outlier is.extreme\n  <fct> <dbl>       <dbl> <dbl> <lgl>      <lgl>     \n1 B       112           4    40 TRUE       FALSE     \n```\n:::\n:::\n\n:::\n\nAgain we have a value from the B(eta) group which is an outlier away from the others (participant 112) - but no extremes. We can relax again!\n\n\n### 3.1.3 Assumption of Normality\n\nWe can test whether our data are normal using QQ plots and a Shapiro-Wilk test of normality. The QQ plot maps out the correlation between our data and a normal distribution. If many data points fall away from our reference line and outside the band of the 95% confidence interval we can assume that data are non-normal.\\ \n\nThe Shapiro-Wilk test statistically calculates whether our data are 'normal'. If the p-value for our Shapiro-Wilk statistic is equal to or less than *p* = .05, then this indicates we have failed the test and have non-normal data - not good! The Shapiro-Wilk test, however, becomes less useful as we have a larger N. The larger the sample, the more likely it is that you will get a statistically significant Shapiro-Wilk test result and our data will be assumed to be non-normal.\\\n\nAs we have a large data set (N = 239), we will ignore the Shapiro-Wilk statistic for now and only focus on the QQ plot.\n\nWe will use the ggpubr package to generate our QQ plot.\nLet's start by looking at the 'Likeability' values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggqqplot(lab3_data, \"Likeability\", facet.by = \"Group\") # where we use \"\" around our variable names\n```\n\n::: {.cell-output-display}\n![](Week3_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nThe plot generated shows how our data correlate to the normal distribution. As we have many data points away from the reference line, we should be concerned that these data are not normal.\n\nLet's run a Shapiro-Wilk test too and see what it tells us:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab3_data %>%\n  group_by(Group) %>%\n  shapiro_test(Likeability)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 4\n  Group variable    statistic           p\n  <fct> <chr>           <dbl>       <dbl>\n1 A     Likeability     0.869 0.000000744\n2 B     Likeability     0.913 0.0000433  \n3 O     Likeability     0.859 0.000000386\n```\n:::\n:::\n\n\nOh dear. All of our groups have highly significant Shapiro-Wilk test statistics. This is further evidence our Likeability data are not normal!\n\n**We have options. We can either transform the data (see lecture 3) and analyze the transformed data using the ANOVA. Or, we can accept the data are non-normal and run a non-parametric alternative - i.e., the Kruskall-Wallace one-way Analysis of Variance by Ranks. In the interest of brevity, we will leave this for another day. From this point on today, let's park the 'Likeability' variable and focus soley on the 'Score' variable**\n\n\nNow please run both the QQ plot code and Shapiro-Wilk test code for our 'Score' variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\nggqqplot(lab3_data, \"Score\", facet.by = \"Group\")\n```\n\n::: {.cell-output-display}\n![](Week3_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\nlab3_data %>%\n  group_by(Group) %>%\n  shapiro_test(Score)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 4\n  Group variable statistic     p\n  <fct> <chr>        <dbl> <dbl>\n1 A     Score        0.987 0.589\n2 B     Score        0.984 0.418\n3 O     Score        0.984 0.430\n```\n:::\n:::\n\n\nIf run correctly, you should get the following.\n\n::: {.cell}\n::: {.cell-output-display}\n![](Week3_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 4\n  Group variable statistic     p\n  <fct> <chr>        <dbl> <dbl>\n1 A     Score        0.987 0.589\n2 B     Score        0.984 0.418\n3 O     Score        0.984 0.430\n```\n:::\n:::\n\n\nNow these don't look too shabby at all.\nFor the QQ plot, our data points lie close to the reference line - this is good news.\nFurther, the Shapiro-Wilk statistic is non-significant - another good sign.\nWe can be pretty confident that our 'Score' data are normal.\n\n**The Score variable moves on and we still may have a chance to analyze it with an ANOVA**\n\n### 3.1.4 Assumption of Homogeniety of Variance.\n\nBefore we carry out our ANOVA, we need to make sure that our 'Score' variable also meets the assumption of homogeneity of variance. We can test whether our three samples have equal variances using the Levene Test for Equality of Variances. If we find the p-value is > 0.05, aka non-significant, we can be relatively confident that there are no significant difference in variances across groups. \n\nLet's give it a whirl with the 'Score' variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab3_data %>% levene_test(Score ~ Group)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     2   237     0.799 0.451\n```\n:::\n:::\n\n\nWe have a non-significant p-value. This is a good sign! This means that the variance for our groups are roughly equal.\n\n******\n#### 4.1 Running a one-factor between group ANOVA\n\nPhew. Now that's all over and we're legit - well for the 'Score' variable at least, it's time to run our one-factor between participants ANOVA. Like last week, we will use tidyverse's 'aov()' function. Let's set 'Score' as our dependent variable of examination. 'Group' is then our predictive factor (independent variable). Note, in the code below, the DV and the IV are separated by the '~' character. Please carry out the following command. This also includes requesting a summary and effectsize for our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_1 - aov(data = lab3_data, score ~ group) #We name our overarching model Model_1\nsummary(Model_1)#We ask for a summary of this model. This provides F statistic and P value\neffectsize(Model_1) # we ask for an eta2 effect size for our model\n```\n:::\n\n\nOh blast. What an evil web of deceit we weave. The code again appears to have bugs in it - something which incidentally and thematically fits well with 'webs'.\n\nThere is something not working in this code. Let's take it slow, working from top to the bottom. Essentially, if Model_1 is not well specified, all other commands involving this Mode1_1 construction will fail. Please try to correct the code.\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\nModel_1 <- aov(data = lab3_data, Score ~ Group) #We name our overarching model Model_1\nsummary(Model_1)#We ask for a summary of this model. This provides F statistic and P value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value  Pr(>F)    \nGroup         2   1223     611    12.5 6.8e-06 ***\nResiduals   237  11571      49                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\n#ANSWER CODE\neffectsize(Model_1) # we ask for an eta2 effect size for our model\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nGroup     | 0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n```\n:::\n:::\n\n\nIf everything is tickety boo, you should get the following:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value  Pr(>F)    \nGroup         2   1223     611    12.5 6.8e-06 ***\nResiduals   237  11571      49                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nGroup     | 0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n```\n:::\n:::\n\n:::\n\nThis has provided us with a whole bunch of useful information! \nIt shows us the between group variability value (do you remember, the numerator value in our F ratio equation from lecture 2?), which R has named the Group Mean Square (a value here of 611.30). \n\nIt also provides the within group variability value - aka the error term (do you remember, the denominator in our F ratio equation from lecture 2), which R has named the Residuals mean square (a value here of 48.80)\n\nIt provides the crucial F-ratio statistic, here a value of 12.52. \n\nThe output also shows an Eta-squared (Î·2) effect size of 0.10. As a rule of thumb, Î·2 = 0.01 indicates a small effect; Î·2 = 0.06 indicates a medium effect; Î·2 = 0.14 indicates a large effect. Our eta-squared (Î·2) represents a medium effect - i.e., our experimental manipulation had a moderate effect on student Psyc214 test scores. \n\n******\n#### 3.2 Reporting the results of the one-factor between-participants ANOVA in APA format\n\nAll results should be written up in accordance with the American Psychological Association's (APA) guidance. This ensures that your results are easy to interpret and that all the relevant information is present for those wishing to review/replicate your work.\n\nThe current results can be reported as following:\n\"A one-factor between-participants ANOVA revealed that Psyc214 scores were significantly different between our robot groups (Robot A *M* = 58.10, *SD* = 6.45; Robot B *M* = 60.40, *SD* = 7.27; Robot O *M* = 63.60, *SD* = 7.22), *F*(2,237) = 12.52, *p* < .001, Î·2 = 0.10, 95% CI[0.04, 1.00].\n\n  ![Hold up!](https://images.unsplash.com/photo-1542707309-4f9de5fd1d9c?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=987&q=80)\n\n*Hold up!!! While the ANOVA tells us that there are differences between groups, it doesn't tell us specifically which groups differ from one another. For example do Group A and B statistically differ? A and O? B and O? The only way to know this definitively is to carry out posthoc/planned contrast tests**\n\n******\n#### 3.3 Decisions, decisions, decisions...\n\nJust to recall, from lecture 3, following an ANOVA, we need to further scrutinize the differences between our separate groups.\n\nThis can be done in one of three ways. \n1) T-tests with no adjustments (let's ignore this one, because it is just bonkers - please refer to lecture 3 if unsure why)\n2) Planned comparisons of specific relationships with adjusted p-values (e.g., Bonferroni corrections)\n3) Post-hoc tests (e.g., Tukey HSD) which are interested in differences between *all* possible combinations of groups and also have conservative *p* values.\n\nTo recap, as a rule of thumb, planned comparisons are always preferable. But these entail that you have a pre-specified idea of which groups should differ.\n\n**We're not going to make this decision for you. Please choose whether you would like to run a:**\\\ni) planned comparison test in which we compare one or two pairs of specific groups of interest (number of levels minus 1 - i.e., we can run up to two comparison tests).\\\nii) post hoc tests where we are interested in statistical differences between all possible combinations of groups.\\\n\n**Take your pick now. If you're an advocate for the 'Planned Comparisons' please skip to section 3.4 and ignore 3.5 for now. If you're an advocate of 'Post hoc tests' please skip to section 3.5 and ignore 3.4 for now**\n\n******\n#### 3.4 Planned comparisons\n\nOh hello, and welcome. I can see you've gone for the planned comparisons  - i.e., you have an idea which specific groups should differ and want to see that through.\n\nLet's run a between-groups pairwise t-test with Bonferroni correction. Note, to get an adjusted *p* value with Bonferroni corrections for multiple comparisons, we need to be sure that we include *p.adjust.method = \"bonferroni*. If we do not add this line of code then we will just get regular p-values without the conservative adjustment.\n\nLet's only compare the scores for Omega versus Alpha and Omega versus Beta:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab3_data %>% \n  # Execute independent-samples t-tests - remember to set pool.sd = FALSE, and include Bonferroni corrections for multiple comparions\n  pairwise_t_test(Score ~ Group, pool.sd = FALSE, var.equal = TRUE, p.adjust.method = \"bonferroni\",\n    # Just generate the two comparisons of interest: e.g., Omega vs. Alpha    # and Omega vs. Beta    \n  comparisons = list(c(\"O\",\"A\"), c(\"O\",\"A\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 10\n  .y.   group1 group2    n1    n2 statistic    df          p      p.adj p.adj.â€¦Â¹\n* <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl>      <dbl>      <dbl> <chr>   \n1 Score A      O         80    80     -5.08   158 0.00000104 0.00000208 ****    \n2 Score A      O         80    80     -5.08   158 0.00000104 0.00000208 ****    \n# â€¦ with abbreviated variable name Â¹â€‹p.adj.signif\n```\n:::\n:::\n\n\nOh blast, there's an error in our code as we seem to have compared Omega versus Alpha twice! Try to adjust your code to ensure you also compare Omega versus Alpha.\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\nlab3_data %>% \n  # Execute independent-samples t-tests - remember to set pool.sd = FALSE, and include Bonferroni corrections for multiple comparions\n  pairwise_t_test(Score ~ Group, pool.sd = FALSE, var.equal = TRUE, p.adjust.method = \"bonferroni\",\n    # Just generate the two comparisons of interest: e.g., Omega vs. Alpha    # and Omega vs. Beta    \n  comparisons = list(c(\"O\",\"A\"), c(\"O\",\"B\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 10\n  .y.   group1 group2    n1    n2 statistic    df          p      p.adj p.adj.â€¦Â¹\n* <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl>      <dbl>      <dbl> <chr>   \n1 Score A      O         80    80     -5.08   158 0.00000104 0.00000208 ****    \n2 Score B      O         80    80     -2.83   158 0.005      0.011      *       \n# â€¦ with abbreviated variable name Â¹â€‹p.adj.signif\n```\n:::\n:::\n\n\nThat's better. If we now scrutinize the output we can see that the Omega group does indeed statistically differ from the other two groups.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 10\n  .y.   group1 group2    n1    n2 statistic    df          p      p.adj p.adj.â€¦Â¹\n* <chr> <chr>  <chr>  <int> <int>     <dbl> <dbl>      <dbl>      <dbl> <chr>   \n1 Score A      O         80    80     -5.08   158 0.00000104 0.00000208 ****    \n2 Score B      O         80    80     -2.83   158 0.005      0.011      *       \n# â€¦ with abbreviated variable name Â¹â€‹p.adj.signif\n```\n:::\n:::\n\n:::\n\nWe would now add this information in our APA write up. *Make sure when you interpret and write up the output that you report the adjusted p-value (i.e., 'p.adj' in the output), as opposed to the non-adjusted and dicey p value (i.e., 'p' in the output).* \n\nWe also want to generate some effect sizes for these pairwise comparisons. This can be done using the rstatix's cohens_d() function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cohen's d effect size for O vs. A\ncohens_d(lab3_data$Score[lab3_data$Group == \"O\"],lab3_data$Score[lab3_data$Group == \"A\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCohen's d |       95% CI\n------------------------\n0.80      | [0.48, 1.12]\n\n- Estimated using pooled SD.\n```\n:::\n:::\n\n\nFantastic. This tells us that our pairwise comparison for the O and A groups has a Cohen's D effect size of 0.80.\n\nNow try your own code out to generate an effect size for groups O and B.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\n# Cohen's d effect size for O vs. B\ncohens_d(lab3_data$Score[lab3_data$Group == \"O\"],lab3_data$Score[lab3_data$Group == \"B\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCohen's d |       95% CI\n------------------------\n0.45      | [0.13, 0.76]\n\n- Estimated using pooled SD.\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nCohen's d |       95% CI\n------------------------\n0.45      | [0.13, 0.76]\n\n- Estimated using pooled SD.\n```\n:::\n:::\n\n\nThe current results can be reported as following:\n\"A one-factor between-participants ANOVA revealed that Psyc214 scores were significantly different between our robot groups (Robot A, *M* = 58.10, *SD* = 6.45; Robot B, *M* = 60.40, *SD* = 7.27; Robot O, *M* = 63.60, *SD* = 7.22), *F*(2,237) = 12.52, *p* < .001, Î·2 = 0.10, 95% CI[0.04, 1.00]. Planned comparisons with Bonferroni adjustments, found that students who were assigned Robot A and B had significantly lower Psyc214 scores than those assigned Robot O, *t*(156) = -5.08, *p* < .001, d = .80 and *t*(158) = -2.83, *p* = 0.011, d = .45, respectively.\"\n\n*Note, we didn't report all combinations. Given our hypotheses, we were interested in Omega. Therefore, we did not need to look at the combination of Alpha and Beta.*\n\n******\n#### 3.5 Post hoc tests\n\nOh hello, and welcome. I can see that you're more of a post hoc kind of individual - i.e., you haven't got any specific idea of which groups should differ or are just plain nosy and would like to examine all possible combinations of group differences.\n\nWe will run a Tukey-Kramer HSD test. This was covered in Lecture 3 - but fortunately, we will not need to get out our Tukey Studentized Range Statistic table and line up the values by hand - R will do this all for us (lovely, generous R)\n\nOk, so let's enter the posthoc command:\n\n::: {.cell}\n\n```{.r .cell-code}\nlab3_data %>% tukey_hsd(Score ~ Group) # where tukey_hsd is our post-hoc test of choice\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 9\n  term  group1 group2 null.value estimate conf.low conf.high      p.adj p.adj.â€¦Â¹\n* <chr> <chr>  <chr>       <dbl>    <dbl>    <dbl>     <dbl>      <dbl> <chr>   \n1 Group A      B               0     2.26   -0.343      4.87 0.103      ns      \n2 Group A      O               0     5.50    2.89       8.11 0.00000369 ****    \n3 Group B      O               0     3.24    0.632      5.84 0.0103     *       \n# â€¦ with abbreviated variable name Â¹â€‹p.adj.signif\n```\n:::\n:::\n\n\nPlease look at the output. It compares groups A and B, A and O, B and O.\nThe p-values are adjusted to be conservative, to try and combat the vulnerability of familywise type I error:\\\n\nA and B, *p* = 0.103\\\nA and O, *p* = 0.00000369\\\nB and O, *p* = 0.0103\\\n\nThe posthoc results show that there is no statistical difference in scores between groups A and B. A and O are highly significantly different from one another, while B and O are statistically significantly different at a of *p* = 0.01.\n\nWe can generate effect sizes for each combination using the cohens_d() function from rstatix.\n\nThe following code produces the first Cohen's d effect size for the comparison of group A and B:\n\n::: {.cell}\n\n```{.r .cell-code}\n# Cohen's d effect size for A vs. B\ncohens_d(lab3_data$Score[lab3_data$Group == \"A\"],lab3_data$Score[lab3_data$Group == \"B\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCohen's d |         95% CI\n--------------------------\n-0.33     | [-0.64, -0.02]\n\n- Estimated using pooled SD.\n```\n:::\n:::\n\n\nNow try your own code to generate a Cohen's d statistic for A and O; B and O:\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\n# Cohen's d effect size for A vs. O\ncohens_d(lab3_data$Score[lab3_data$Group == \"A\"],lab3_data$Score[lab3_data$Group == \"O\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCohen's d |         95% CI\n--------------------------\n-0.80     | [-1.12, -0.48]\n\n- Estimated using pooled SD.\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\n# Cohen's d effect size for B vs. O\ncohens_d(lab3_data$Score[lab3_data$Group == \"B\"],lab3_data$Score[lab3_data$Group == \"O\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCohen's d |         95% CI\n--------------------------\n-0.45     | [-0.76, -0.13]\n\n- Estimated using pooled SD.\n```\n:::\n:::\n\n\nIf done correctly, you should have the following:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nCohen's d |         95% CI\n--------------------------\n-0.80     | [-1.12, -0.48]\n\n- Estimated using pooled SD.\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nCohen's d |         95% CI\n--------------------------\n-0.45     | [-0.76, -0.13]\n\n- Estimated using pooled SD.\n```\n:::\n:::\n\n:::\n\nThe current results can be reported as following:\n\"A one-factor between-participants ANOVA revealed that Psyc214 scores were significantly different between our robot groups (Robot A, *M* = 58.10, *SD* = 6.45; Robot B, *M* = 60.40, *SD* = 7.27; Robot O, *M* = 63.60, *SD* = 7.22), *F*(2,237) = 12.52, *p* < .001, Î·2 = 0.10, 95% CI[0.04, 1.00]. Tukey-Kramer HSD posthoc tests showed that the difference in students' test scores (5.50) between participants assigned Robot A and Robot O was statistically significant (adjusted *p* < 0.001, d = -0.80). The difference in students' test scores (3.24) between participants assigned Robot B and Robot O was statistically significant (adjusted *p* = .01, d = -0.45), but there was no statistical group difference between Robot A and B (2.26, adjusted *p* = 0.10, d = -0.33).\"\n\nNow that you've finished this. Please go on to '4 Further tasks'.\n  \n******\n#### 4 Further tasks\n\n1. Well done for completing what has been the most difficult lab week thus far. Regardless of your allegiance - be it planned comparisons or post hoc tests - it is important you are fluent with both. Therefore, please navigate up and run the alternative set of pairwise comparisons to the one you originally chose - in short, if you haven't had the chance to run planned comparisons, please do it now. If you haven't yet run post-hoc tests, give them a whirl.\n\n\n2. We didn't end up running tests of homogeneity of variance for the 'Likeability' variable - remember, we decided not to analyze this data without either a transformation or a change to a non-parametric alternative. Regardless, just for practice, why not run a Levene's Test - (see 3.1.4 Assumption of Homogeniety of Variance). How is it looking? Talk to your self or a study buddy.\n\n::: {.callout-warning icon=\"false\" collapse=\"true\"}\n## Answer\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\nlab3_data %>% levene_test(Likeability ~ Group)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 4\n    df1   df2 statistic     p\n  <int> <int>     <dbl> <dbl>\n1     2   236      1.75 0.177\n```\n:::\n:::\n\n:::\n\n3. Before you finish, make sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\n\nPlease end your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\n4. Please chill, you must be fatigued. See you next week and top work.\n\n![Top work!](https://images.unsplash.com/photo-1562776875-c8af826b1a29?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=2071&q=80)\n \n \n# Lab Feedback (voluntary)\n\nThis is a voluntary, super fast [survey](https://modules.lancaster.ac.uk/mod/url/view.php?id=1905315), just to gauge how you found the difficulty of the content and any additional feedback.",
    "supporting": [
      "Week3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}