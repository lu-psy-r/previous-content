{
  "hash": "d55da7b21245fc67c8bb5c158a204cb7",
  "result": {
    "markdown": "---\ntitle: '9. Three-Factor ANOVA'\nsubtitle: \"Mark Hurlstone, Richard Philpot\"\norder: 10\n---\n\n\n\n# Lecture\n\nWatch Part 1 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2022872)\n\nWatch Part 2 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2022874)\n\nWatch Part 3 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=2023033)\n\nDownload the lecture slides [here](data/Wk9/Three-Factor ANOVA HANDOUT.pdf), and [here](data/Wk9/Three-Factor ANOVA.pdf) for a larger version.\n\n# Lab\n\n\n\n\n\n<br>\n<br>\n\n> *I judge you unfortunate because you have never lived through misfortune. You have passed through life without an opponent---no one can ever know what you are capable of, not even you. ---Seneca*\n\n![Seneca](images/Seneca.png){width=50%}\n\n\n## Learning Objectives\nIn this week's lecture, we introduced the procedures involved in interpreting a three-factor ANOVA. Specifically, what to do in the event that the three-way interaction is significant. We saw that the simplest strategy in this instance is to re-analyse the data as a series of two-factor ANOVAs. In today's lab session, we will demonstrate how to perform a three-factor fully within-participants and mixed ANOVA in R (using the two hypothetical data sets presented in the lecture), and how to analyze a three-way interaction using the procedures described in the lecture. **In this lab session, I am also going to show you a better way of rounding the values in dataframes than the `options(digits = )` command used in earlier lab sessions.**\n\n**If you get stuck at any point, be proactive and ask for help from one of the GTAs.**\n\n## Getting Started\n\n---\ntitle: \"Statistics for Psychologists\"\npage-layout: article\n---\n\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS' [instructions here](https://portal.lancaster.ac.uk/ask/digital/services/university-it-network/vpn/) or connecting to Eduroam [here](https://portal.lancaster.ac.uk/ask/digital/services/university-it-network/wi-fi/).\n\nWhen you are connected, navigate to [https://psy-rstudio.lancaster.ac.uk](https://psy-rstudio.lancaster.ac.uk), where you will be shown a login screen that looks like the below. Click the option that says \"Sign in with SAML\".\n\n![](/Includes/1LoginSAML.png) <!-- says no image but it's all about the include setup -->\n\nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n![](/Includes/2LoginUser.png)\n\n![](/Includes/3LoginPass.png)\n\n::: {.callout-note}\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n:::\n\n\n\n\nOnce you are logged into the server, create a folder for today's session. Navigate to the bottom right panel and under the `Files` option select the `New Folder` option. Name the new folder `psyc214_lab_9`. **Please ensure that you spell this correctly** otherwise when you set the directory using the command given below it will return an error.   \n\nSe we can save this session on the server, click `File` on the top ribbon and select `New project`. Next, select existing directory and name the working directory `~/psyc214_lab_9` before selecting `create project`.\n\nFinally, open a script for executing today's coding exercises. Navigate to the top left pane of RStudio, select `File` -> `New File` -> `R Script`. Working from a script will make it easier to edit your code and you will be able to save your work for a later date. \n\n![](images/RScript.png){width=75%}\n\nLet's set our working directory:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsetwd(\"~/psyc214_lab_9\")\n```\n:::\n\n\nNow that you have created a folder for today's session, it’s time to add the Week 9 files. Download the files and data needed from [here](data/wk9/psyc214_lab_9.zip) and upload to the folder `psyc214_lab_9`.\n\nBefore moving on, let's load the relevant libraries that we will be using in today's session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\")  # For data storage and manipulation\nlibrary(\"tidyr\")      # For tidy data\nlibrary(\"rstatix\")    # For descriptives statistics, outlier detection, running the ANOVAs etc.\nsource(\"simple.R\")    # Custom function for generating the simple main effects\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n# Today's Lab Activities\n\n## Analysing the hypothetical data for the memory and context study\n\nA memory researcher wants to know if memory is better when material is tested in the same context it was learned in. The researcher also wants to know whether recall and recognition memory are equally context dependent. The researcher manipulates three factors in a 2 $\\times$ 2 $\\times$ 2 fully within-participants design:\n\n* **memory task** (recall *vs.* recognition) \n\n* **learning context** (learn underwater *vs.* learn land) \n\n* **testing context** (test underwater *vs.* test land)\n\nParticipants are given words to remember in a specific learning context (either under water or on land) and are then tested in either the same context (e.g., under water if the words were learned under water) or a different context (e.g., on land if the words were learned under water). Memory is tested using a recall procedure (by asking participants to recall the studied words) or a recognition procedure (by presenting participants with a list of words and asking them to indicate which they had studied previously). The dependent measure is the number of words remembered correctly. \n\nThe data set contains the following variables:\n\n* **Participant:** represents the participant number, which ranges from 1--5. \n\n* **Recall_Under_Under:** the number of words *recalled* correctly when material was learned under water and tested under water.\t\n\n* **Recall_Under_Land:** the number of words *recalled* correctly when material was learned under water and tested on land.\t\t\n\n* **Recall_Land_Under:** the number of words *recalled* correctly when material was learned on land and tested under water.\t\t\n\n* **Recall_Land_Land:**\tthe number of words *recalled* correctly when material was learned on land and tested on land.\t\t\n\n* **Recognition_Under_Under:**\tthe number of words *recognised* correctly when material was learned under water and tested under water.\t\n\n* **Recognition_Under_Land:** the number of words *recognised* correctly when material was learned under water and tested on land.\t\n\n* **Recognition_Land_Under:** the number of words *recognised* correctly when material was learned on land and tested under water.\n\n* **Recognition_Land_Land:** the number of words *recognised* correctly when material was learned on land and tested on land.\t\t\n\n### Import data, set variables as factors, and generate descriptive statistics\n\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as `memoryContext`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# *** ENTER YOUR OWN CODE HERE TO IMPORT THE DATA ***\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n  Participant Recall_U…¹ Recal…² Recal…³ Recal…⁴ Recog…⁵ Recog…⁶ Recog…⁷ Recog…⁸\n        <dbl>      <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1           1          8       5       3       7       5       5       7       6\n2           2          9       6       3       8       7       6       5       8\n3           3          7       5       4       6       6       7       5       6\n4           4          8       4       4       5       7       5       6       5\n5           5          6       3       3       8       5       4       6       4\n# … with abbreviated variable names ¹​Recall_Under_Under, ²​Recall_Under_Land,\n#   ³​Recall_Land_Under, ⁴​Recall_Land_Land, ⁵​Recognition_Under_Under,\n#   ⁶​Recognition_Under_Land, ⁷​Recognition_Land_Under, ⁸​Recognition_Land_Land\n```\n:::\n:::\n\n\nThe next thing we need to do is convert our data from wide format into long format. The first thing we need to do is group the columns `Recall_Under_Under` through to `Recognition_Land_Land` into a new variable called `Group` using the `gather()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gather factors into a single column\nmemoryContextLong = memoryContext %>%\n  gather(Group,Accuracy,Recall_Under_Under:Recognition_Land_Land,factor_key = TRUE)\n(memoryContextLong)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 40 × 3\n   Participant Group              Accuracy\n         <dbl> <fct>                 <dbl>\n 1           1 Recall_Under_Under        8\n 2           2 Recall_Under_Under        9\n 3           3 Recall_Under_Under        7\n 4           4 Recall_Under_Under        8\n 5           5 Recall_Under_Under        6\n 6           1 Recall_Under_Land         5\n 7           2 Recall_Under_Land         6\n 8           3 Recall_Under_Land         5\n 9           4 Recall_Under_Land         4\n10           5 Recall_Under_Land         3\n# … with 30 more rows\n```\n:::\n:::\n\n\nThis function was explained in the previous lab session, so if it is not clear what is going on here, check the Week 8 lab session materials.\n\nLooking at the new data frame we have created, we can see that it is not exactly what we want. Our new variable `Group` actually contains three independent variables. What we want is to separate these independent variables into three separate columns: `MemoryTask`, `LearningContext`, and `TestingContext`. We can do that with the `separate()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now separate the variable \"Group\" into separate columns for each factor\nmemoryContextLongSep = memoryContextLong %>%\n  separate(Group, c(\"MemoryTask\",\"LearningContext\",\"TestingContext\"))\n(memoryContextLongSep)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 40 × 5\n   Participant MemoryTask LearningContext TestingContext Accuracy\n         <dbl> <chr>      <chr>           <chr>             <dbl>\n 1           1 Recall     Under           Under                 8\n 2           2 Recall     Under           Under                 9\n 3           3 Recall     Under           Under                 7\n 4           4 Recall     Under           Under                 8\n 5           5 Recall     Under           Under                 6\n 6           1 Recall     Under           Land                  5\n 7           2 Recall     Under           Land                  6\n 8           3 Recall     Under           Land                  5\n 9           4 Recall     Under           Land                  4\n10           5 Recall     Under           Land                  3\n# … with 30 more rows\n```\n:::\n:::\n\n\nAgain, this function was explained in the previous lab session, so if it is not clear what is going on here, check the Week 8 lab session materials. The latest version of the data set is named `memoryContextLongSep`, so make sure you use this from henceforth.\n\nThe next thing we need to do is convert the variables `Participant`, `MemoryTask`, `LearningContext`, and `TestingContext` into factors and re-order the levels of the latter two variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make sure all necessary variables are coded as factors -- re-order the levels of \"LearningContext\" and \"TestingContext\"\nmemoryContextLongSep$Participant = factor(memoryContextLongSep$Participant)\nmemoryContextLongSep$MemoryTask = factor(memoryContextLongSep$MemoryTask)\nmemoryContextLongSep$LearningContext = factor(memoryContextLongSep$LearningContext,levels = c(\"Under\",\"Land\"))\nmemoryContextLongSep$TestingContext = factor(memoryContextLongSep$TestingContext,levels = c(\"Under\",\"Land\"))\n```\n:::\n\n\nNext, we will generate some descriptive statistics (mean and standard deviation):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get descriptive statistics\ndescriptives = memoryContextLongSep %>%\n  # Organise the output by the \"MemoryTask\", \"LearningContext\", and \"TestingContext\" factors\n  group_by(MemoryTask, LearningContext, TestingContext) %>%\n  # Request means, standard deviations, and confidence intervals\n  get_summary_stats(Accuracy, show = c(\"mean\", \"sd\"))\n  # Round the statistics to two decimal places\n  descriptives$mean = round(descriptives$mean, 2)\n  descriptives$sd = round(descriptives$sd, 2)\n  # Print the results\n  print.data.frame(descriptives)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   MemoryTask LearningContext TestingContext variable n mean   sd\n1      Recall           Under          Under Accuracy 5  7.6 1.14\n2      Recall           Under           Land Accuracy 5  4.6 1.14\n3      Recall            Land          Under Accuracy 5  3.4 0.55\n4      Recall            Land           Land Accuracy 5  6.8 1.30\n5 Recognition           Under          Under Accuracy 5  6.0 1.00\n6 Recognition           Under           Land Accuracy 5  5.4 1.14\n7 Recognition            Land          Under Accuracy 5  5.8 0.84\n8 Recognition            Land           Land Accuracy 5  5.8 1.48\n```\n:::\n:::\n\n\nNotice the code we have added to round the descriptive statistics to two-decimal places. We use the function `round()`, specifying how many decimal places we want to round the values (in this case 2). Inside this function, we place the variable we want to be rounded. Because our data are in a dataframe, we need to specify the name of our dataframe followed by a dollar sign and the name of the variable in the dataframe to be rounded (e.g., descriptives\\$mean tells R to round the values of the variable `mean` in the dataframe `descriptives`). We also need to re-assign the rounded values to the dataframe, so that the variable gets updated (e.g., that's what the descriptives\\$mean = bit does). \n\nNotice also that our standard deviations have been reported to two decimal places, but our means haven't. Why so? This is because the numbers after the first decimal place in this instance are all zeros, so R doesn't report them. Bearing in mind that APA style requires we report descriptive statistics to two-decimal places, we would just add a single zero to each of the means at the second decimal place. For example, the first mean in the table, 7.6, would be reported as 7.60. \n\nAt this stage, we would ordinarily perform various checks including identifying possible outliers and checking that our data satisfy the normality assumption. However, as per last week, time is limited, so we won't perform those checks today (just remember that ordinarily you should not skip this part!). One assumption that is important in within-participants designs is the sphericity assumption, but remember that this only applies to designs with within-participants factors with three or more levels. All of our factors have two levels, so this assumption is not relevant in this instance (it's also not relevant for our second data set that we analyse later, for which the within-participants factors also only comprise two levels).\n\n### Running the ANOVA, follow-up ANOVAs, and simple main effects\n\nTo run our ANOVA, we are going to use the `anova_test` function from the `rstatix` package. This is the same function that we used in the Week 8 lab session to analyse two-factor fully within-participants and mixed designs. The code required to run the ANOVA is given below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the fully within-participants design ANOVA model\nmemoryContextModel = anova_test(data = memoryContextLongSep, dv = Accuracy, wid = Participant, within = c(MemoryTask, LearningContext, TestingContext), detailed = TRUE)\n# Round the p values to three decimal places\nmemoryContextModel$p = round(memoryContextModel$p, 3)\n# Print the model summary\n(memoryContextModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nANOVA Table (type III tests)\n\n                                     Effect DFn DFd      SSn  SSd       F     p\n1                               (Intercept)   1   4 1288.225 10.9 472.743 0.000\n2                                MemoryTask   1   4    0.225  0.9   1.000 0.374\n3                           LearningContext   1   4    2.025  5.1   1.588 0.276\n4                            TestingContext   1   4    0.025  7.1   0.014 0.911\n5                MemoryTask:LearningContext   1   4    3.025  4.1   2.951 0.161\n6                 MemoryTask:TestingContext   1   4    0.625  3.5   0.714 0.446\n7            LearningContext:TestingContext   1   4   30.625  4.5  27.222 0.006\n8 MemoryTask:LearningContext:TestingContext   1   4   21.025  3.1  27.129 0.006\n  p<.05      ges\n1     * 0.970000\n2       0.006000\n3       0.049000\n4       0.000637\n5       0.072000\n6       0.016000\n7     * 0.439000\n8     * 0.349000\n```\n:::\n:::\n\n\nTo create the model, the first argument we supplied to `anova_test` was the name of our data, `memoryContextLongSep`. The second argument we supplied was our dependent variable, `Accuracy`. The third argument we supplied was `Participant`, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our within-participants factors, `MemoryTask`, `LearningContext`, and `TestingContext`. \n\nAs we saw in last week's lab session, the resulting ANOVA table is different in format to those given in the lecture, which follow a more conventional style. In the ANOVA tables given in the lecture, each outcome (each main effect and interaction) is given on a separate row, with the error term used to test it given in the row directly beneath it. However, `anova_test` gives each outcome and its associated error term all in the same row. Specifically, the row corresponding to each outcome contains the between-group degrees of freedom (DFn), the error degrees of freedom (DFd), the between-group sums of squares (SSn), the error sums of squares (SSd), the $F$ ratio (F), the *p* (p) value, and the generalised eta squared (ges) value (a measure of effect size). What `anova_test` does not give us is the between-group mean squares and the error mean squares that are used to calculate the $F$ ratios. However, I showed you how to calculate these in last week's lab session if you should ever have need for these (you probably won't).\n\nYou might be wondering why `anova_test` has not given us Mauchly's test of sphericity and the Greenhouse-Geisser correction. This is because all of our factors have only two levels, so the sphericity assumption does not apply. Remember, the `anova_test` function only generates these tests and corrections when at least one of the within-participants factors has three or more levels.\n\nInspecting the ANOVA table, rows two to four give the main effects of Memory Task, Learning Context, and Testing Context; rows five to seven give the Memory Task $\\times$ Learning Context, Memory Task $\\times$ Testing Context, and Learning Context $\\times$ Testing Context two-way interactions; and row eight gives the Memory Task $\\times$ Learning Context $\\times$ Testing Context three-way interaction. Looking at the *p* values, we can see that there is a significant Learning Context $\\times$ Testing Context two-way interaction, $p$ = .006, and a significant Memory Task $\\times$ Learning Context $\\times$ Testing Context three-way interaction, $p$ = .006.\n\nBecause the three-way interaction is significant, we need to analyse it further. As explained in the lecture, a significant three-way interaction occurs when there are different two-way interactions between two of the factors according to the levels of the third factor. The simplest way to analyse a signiﬁcant three-way interaction is to re-analyse it as a series of two-factor ANOVAs. To do this, we first need to decide on a factor that we are going to split the analyses by. We can pick any factor we want, but there is usually one factor that stands out as being an obvious choice and in our case it is the memory task factor. So, what we need to do is to perform two, two-factor ANOVAs:\n\n1. a 2 (learning context: learn under water *vs.* learn land) $\\times$ 2 (testing context: test under water *vs.* test land) ANOVA for the *recall* memory test condition only.\n\n2. a 2 (learning context: learn under water *vs.* learn land) $\\times$ 2 (testing context: test under water *vs.* test land) ANOVA for the *recognition* memory test condition only.\n\nWe will start by running the two-factor ANOVA for the recall memory test condition (ignoring the recognition memory test condition). To do this, we first need to produce a filtered version of our data set called `recallOnly` that only includes the results for the recall memory test condition. We can create that with the following piece of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the data for the \"Recall\" condition only\nrecallOnly = memoryContextLongSep %>%\n  filter(MemoryTask == \"Recall\") \n```\n:::\n\n\nThe command `filter(MemoryTask == \"Recall\")` tells R that we only want the data for the recall condition of the Memory Task factor. \n\nNext, we can run our two-factor ANOVA on this filtered data set. The steps are the same as above, except that we need to drop the Memory Task factor included previously (remember, we are only analysing the recall condition of the Memory Task factor). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the two-factor ANOVA for the \"Recall\" condition only\nrecallModel = anova_test(data = recallOnly, dv = Accuracy, wid = Participant, within = c(LearningContext, TestingContext), detailed = TRUE)\n# Round the p values to three decimal places\nrecallModel$p = round(recallModel$p, 3)\n# Print the model summary\n(recallModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nANOVA Table (type III tests)\n\n                          Effect DFn DFd   SSn SSd       F     p p<.05   ges\n1                    (Intercept)   1   4 627.2 5.3 473.358 0.000     * 0.971\n2                LearningContext   1   4   5.0 5.5   3.636 0.129       0.214\n3                 TestingContext   1   4   0.2 4.3   0.186 0.688       0.011\n4 LearningContext:TestingContext   1   4  51.2 3.3  62.061 0.001     * 0.736\n```\n:::\n:::\n\n\nThe only thing we are interested in from the ANOVA table is the outcome of the two-way interaction between Learning Context and Testing Context; you can ignore everything else. You can see that the interaction is significant, *p* = .001, so the next step is to perform a simple main effects analysis to identify the nature of the interaction.\n\nThe procedure for performing the simple main effects analysis is the same as I demonstrated to you in our Week 8 lab session. That is, we use the **pooled error terms** approach, which means that the simple main effects of each factor are calculated using the same error term that was used to test the main effect of that factor in the ANOVA that preceded the simple main effects analysis (in this case, our two-factor ANOVA on the recall data only).\n\nBefore we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the recall ANOVA table\nrecallAnovaTable = get_anova_table(recallModel)\n```\n:::\n\n\nNext, we need to calculate the cell totals for each of the four conditions and the number of observations (i.e., scores) in each cell:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get cell totals and counts\nrecallCellTotals = recallOnly %>%\n  # Organise the output by the \"LearningContext\" and \"TestingContext\" factors\n  group_by(LearningContext, TestingContext) %>%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(Accuracy),n = n())\n  # Print the results\n  (recallCellTotals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n# Groups:   LearningContext [2]\n  LearningContext TestingContext   sum     n\n  <fct>           <fct>          <dbl> <int>\n1 Under           Under             38     5\n2 Under           Land              23     5\n3 Land            Under             17     5\n4 Land            Land              34     5\n```\n:::\n:::\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Learning Context at Testing Context. This means, we are going to:\n\n* Test the difference between learning under water and learning on land when tested *under water* only.\n* Test the difference between learning under water and learning on land when tested *on land* only.\n\nTo do this, we need to declare Learning Context as the \"fixed\" factor (we are always comparing learning under water and learning on land) and Testing Context as the \"across\" factor (the comparison between learning under water and learning on land occurs \"across\" the test under water and test on land levels of the Testing Context factor): \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create \"fixed\" and \"across\" factors\nfixed  = \"LearningContext\"\nacross = \"TestingContext\"\n```\n:::\n\n\nWe then generate the simple main effects of Learning Context by passing these variables into `simple()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple main effects of \"Learning Context\" at \"TestingContext\"\nsmeLearningContext = simple(recallCellTotals,recallAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeLearningContext$P = round(smeLearningContext$P, 3)\n(smeLearningContext)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Levels Sum of Squares Degrees of Freedom Mean Square        F     P\n1      Under           44.1                  1      44.100 32.07273 0.005\n2       Land           12.1                  1      12.100  8.80000 0.041\n3 Error term            5.5                  4       1.375  0.00000 0.000\n```\n:::\n:::\n\n\nWe can see that there is a significant simple main effect of Learning Context at test under water, $p$ = .005; when tested under water, recall memory scores are higher when the material was learned under water than when it was learned on land. There is also a significant simple main effect of Learning Context at test on land, $p$ = .041; when tested on land, recall memory scores are higher when the material was learned on land than when it was learned under water. You will need to consult the descriptive statistics to verify this is correct.\n\nNext, we are going to calculate the simple main effects of the factor Testing Context at Learning Context. This means, we are going to:\n\n* Test the difference between testing under water and testing on land when material was learned *under water* only.\n* Test the difference between testing under water and testing on land when material was learned *on land* only.\n\nTo do this, we now need to declare Testing Context as the \"fixed\" factor and Learning Context as the \"across\" factor: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create \"fixed\" and \"across\" factors\nfixed  = \"TestingContext\"\nacross = \"LearningContext\"\n```\n:::\n\n\nWe then generate the simple main effects of Testing Context with the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple main effects of \"Testing Context\" at \"LearningContext\"\nsmeTestingContext = simple(recallCellTotals,recallAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeTestingContext$P = round(smeTestingContext$P, 3)\n(smeTestingContext)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Levels Sum of Squares Degrees of Freedom Mean Square        F     P\n1      Under           22.5                  1      22.500 20.93023 0.010\n2       Land           28.9                  1      28.900 26.88372 0.007\n3 Error term            4.3                  4       1.075  0.00000 0.000\n```\n:::\n:::\n\n\nWe can see that there is a significant simple main effect of Testing Context at learn under water, $p$ = .010; when the material is learned under water, recall memory scores are higher when tested under water than when tested on land. There is also a significant simple main effect of Testing Context at learn land, $p$ = .007; when the material is learned on land, recall memory scores are higher when tested on land than when tested under water.  \n\nIn sum, from the simple main effects analysis what we can see is that recall memory is context sensitive; that is, recall memory performance is better when people are tested in the same context that they learned the information than when they are tested in a different context to that which they learned the information.\n\nWhat about recognition memory?\n\nThat brings us to our second two-factor ANOVA. For this, we now need to produce a filtered version of our data set called `recognitionOnly` that only includes the results for the recognition memory task condition. We can create that with the following piece of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the data for the \"Recognition\" condition only\nrecognitionOnly = memoryContextLongSep %>%\n  filter(MemoryTask == \"Recognition\") \n```\n:::\n\n\nThe command `filter(MemoryTask == \"Recognition\")` tells R that we only want the data for the recognition condition of the Memory Task factor. \n\nNext, we can run our two-factor ANOVA on this filtered data set.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the two-factor ANOVA for the \"Recognition\" condition only\nrecognitionModel = anova_test(data = recognitionOnly, dv = Accuracy, wid = Participant, within = c(LearningContext, TestingContext), detailed = TRUE)\n# Round the p values to three decimal places\nrecognitionModel$p = round(recognitionModel$p, 3)\n# Print the model summary\n(recognitionModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nANOVA Table (type III tests)\n\n                          Effect DFn DFd    SSn SSd       F     p p<.05   ges\n1                    (Intercept)   1   4 661.25 6.5 406.923 0.000     * 0.970\n2                LearningContext   1   4   0.05 3.7   0.054 0.828       0.002\n3                 TestingContext   1   4   0.45 6.3   0.286 0.621       0.021\n4 LearningContext:TestingContext   1   4   0.45 4.3   0.419 0.553       0.021\n```\n:::\n:::\n\n\nThe key result is that this time the critical two-way interaction is nonsignificant, *p* = 0.553. What this indicates is that, unlike recall memory, recognition memory is not context sensitive. This is the reason for the three-way interaction; recall memory is sensitive to the learning and testing context, whereas recognition memory is apparently insensitive to the learning and testing context. \n\n### Writing up the results\n\n![Figure 1. Memory scores as a function of learning context and testing context for the recall memory task (left panel) and the recognition memory task (right panel).](images/MemoryContextData.png){width=75%}\n\nFigure 1 shows memory scores as a function of learning context and testing context for the recall and recognition memory tasks. These data were subjected to a 2 (memory task: recall *vs.* recognition) $\\times$ 2 (learning context: learn under water *vs.* learn land) $\\times$ 2 (testing context: test under water *vs.* test land) within-participants ANOVA. There was no significant main effect of memory task, *F*(1, 4) = 1.00, *p* = .374, no significant main effect of learning context, *F*(1, 4) = 1.59, *p* = .276, and no significant main effect of testing context, *F*(1, 4) = 0.01, *p* = .911. Neither the memory task $\\times$ learning context interaction, *F*(1, 4) = 2.95, *p* = .161, nor the memory task $\\times$ testing context interaction, *F*(1, 4) = 0.71, *p* = .446, were significant. However, the learning context $\\times$ testing context interaction was significant, *F*(1, 4) = 27.22, *p* = .006. Critically, there was also a significant three-way interaction, *F*(1,4) = 27.13, *p* = .006.\n\nTo explore the three-way interaction, two 2 (learning context) $\\times$ 2 (testing context) ANOVAs were conducted; one using the data for the recall memory task only, and the second using the data for the recognition memory task only. For the first ANOVA on the recall memory task data, there was a significant interaction, *F*(1, 4) = 62.06, *p* = .001. A simple main effects analysis revealed that when tested under water, recall memory was better when the material was learned under water than when it was learned on land, *F*(1, 4) = 44.10, *p* = .005, and when tested on land, recall memory was better when the material was learned on land than when it was learned under water, *F*(1, 4) = 12.10, *p* = .041. Mirroring these results, when the material was learned under water, recall memory was better when tested under water than when tested on land, *F*(1, 4) = 22.50, *p* = .010, and when the material was learned on land, recall memory was better when tested on land than when tested under water, *F*(1, 4) = 28.90, *p* = .007. \n\nFor the second ANOVA on the recognition memory task data, there was no significant interaction, *F*(1, 4) = 0.42, *p* = .553.\n\nIn brief, the three-way interaction reflects the fact that recall memory is sensitive to the learning and testing context, whereas recognition memory is apparently not sensitive to such contextual factors. \n\n## Analysing the hypothetical data for the word pronunciation study\n\nA researcher wants to investigate the development in children’s ability to pronounce regular and irregular words. The researcher adopts a 2 $\\times$ 2 $\\times$ 2 mixed design: \n\n* **age** (7 years old *vs.* 9 years old) is a between-participants factor\n\n* **word frequency** (low *vs.* high) is a within-participants factor\n\n* **word type** (regular *vs.* irregular) is also a within-participants factor\n\nParticipants are given 10 words to pronounce in each category (40 words in total) and the dependent measure of interest is the number of pronunciation errors.\n\nThe data set contains the following variables:\n\n* **Participant:** represents the participant number, which ranges from 1--10.\n\n* **Age:** whether the participant is 7-years-old or 9-years-old.\n\n* **High_Regular:** pronunciation errors for high frequency regular words.\n\n* **High_Irregular:** pronunciation errors for high frequency irregular words.\n\n* **Low_Regular:** pronunciation errors for low frequency regular words.\n\n* **Low_Irregular:** pronunciation errors for low frequency irregular words.\n\n### Import data, set variables as factors, and generate descriptive statistics\n\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as `wordPron`. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# *** ENTER YOUR OWN CODE HERE TO IMPORT THE DATA ***\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 6\n   Participant Age         High_Regular High_Irregular Low_Regular Low_Irregular\n         <dbl> <chr>              <dbl>          <dbl>       <dbl>         <dbl>\n 1           1 7 Year Olds            6              7           5             6\n 2           2 7 Year Olds            7              5           6             7\n 3           3 7 Year Olds            5              6           7             6\n 4           4 7 Year Olds            6              7           5             7\n 5           5 7 Year Olds            6              6           5             7\n 6           6 9 Year Olds            4              4           3             6\n 7           7 9 Year Olds            3              4           4             7\n 8           8 9 Year Olds            4              3           5             9\n 9           9 9 Year Olds            5              5           3             8\n10          10 9 Year Olds            3              4           3             7\n```\n:::\n:::\n\n\nThe next thing we need to do is convert our data from wide format into long format. The first thing we need to do is group the columns `High_Regular` through to `Low_Irregular` into a new variable called `Group` using the `gather()` function. We showed you how to do this in the earlier data set, so give this a go for yourself. Make sure that you name the dependent measure `Errors` and that you call your new data set `wordPronLng`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# *** ENTER YOUR OWN CODE HERE TO GATHER THE WITHIN-PARTICIPANTS FACTORS INTO A COMMON GROUP ***\n```\n:::\n\n\nIf you have executed your code correct, you should see the following output:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 40 × 4\n   Participant Age         Group        Errors\n         <dbl> <chr>       <fct>         <dbl>\n 1           1 7 Year Olds High_Regular      6\n 2           2 7 Year Olds High_Regular      7\n 3           3 7 Year Olds High_Regular      5\n 4           4 7 Year Olds High_Regular      6\n 5           5 7 Year Olds High_Regular      6\n 6           6 9 Year Olds High_Regular      4\n 7           7 9 Year Olds High_Regular      3\n 8           8 9 Year Olds High_Regular      4\n 9           9 9 Year Olds High_Regular      5\n10          10 9 Year Olds High_Regular      3\n# … with 30 more rows\n```\n:::\n:::\n\n\nWe have a new variable `Group` that contains our two independent variables, Frequency and Word Type, and a new variable `Errors` that contains our dependent measure. The next step is to use the `separate()` function to divide the variable `Group` into two new variables, one called `Frequency` and one called `WordType`. Again, we gave you an example of this earlier, so try your own code out for this bit. Just make sure you call your new data set `wordPronLngSep`.\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# *** ENTER YOUR OWN CODE HERE TO SEPARATE \"GROUP\" INTO SEPARATE VARIABLES FOR \"FREQUENCY\" AND \"WORDTYPE\" ***\n```\n:::\n\n\nAssuming you have executed your code correctly, you should see the following output:\n  \n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 40 × 5\n   Participant Age         Frequency WordType Errors\n         <dbl> <chr>       <chr>     <chr>     <dbl>\n 1           1 7 Year Olds High      Regular       6\n 2           2 7 Year Olds High      Regular       7\n 3           3 7 Year Olds High      Regular       5\n 4           4 7 Year Olds High      Regular       6\n 5           5 7 Year Olds High      Regular       6\n 6           6 9 Year Olds High      Regular       4\n 7           7 9 Year Olds High      Regular       3\n 8           8 9 Year Olds High      Regular       4\n 9           9 9 Year Olds High      Regular       5\n10          10 9 Year Olds High      Regular       3\n# … with 30 more rows\n```\n:::\n:::\n\n\nOur variable `Group` has now disappeared and in its place we have two new variables: `Frequency` and `WordType`. This is the version of the data set we will be using for the rest of the analysis.\n\nThe next thing we need to do is convert the columns `Participant`, `Age`, `Frequency`, and `WordType` into factors and re-order the levels of the latter two variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make sure all necessary variables are coded as factors -- re-order the levels of \"Frequency\" and \"WordType\"\nwordPronLngSep$Participant = factor(wordPronLngSep$Participant)\nwordPronLngSep$Age = factor(wordPronLngSep$Age)\nwordPronLngSep$Frequency = factor(wordPronLngSep$Frequency,levels = c(\"Low\",\"High\"))\nwordPronLngSep$WordType = factor(wordPronLngSep$WordType,levels = c(\"Regular\",\"Irregular\"))\n```\n:::\n\n\nNext, we will generate some descriptive statistics (mean and standard deviation). You can generate the code for this yourself. Make sure that you round the statistics to two-decimal places using the procedure I showed you in the earlier example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# *** ENTER YOUR OWN CODE HERE TO GENERATE DESCRIPTIVE STATISTICS ***\n```\n:::\n\n\nIf you have executed the code correctly, then you should see the following output:\n  \n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n          Age Frequency  WordType variable n mean   sd\n1 7 Year Olds       Low   Regular   Errors 5  5.6 0.89\n2 7 Year Olds       Low Irregular   Errors 5  6.6 0.55\n3 7 Year Olds      High   Regular   Errors 5  6.0 0.71\n4 7 Year Olds      High Irregular   Errors 5  6.2 0.84\n5 9 Year Olds       Low   Regular   Errors 5  3.6 0.89\n6 9 Year Olds       Low Irregular   Errors 5  7.4 1.14\n7 9 Year Olds      High   Regular   Errors 5  3.8 0.84\n8 9 Year Olds      High Irregular   Errors 5  4.0 0.71\n```\n:::\n:::\n\n\n### Running the ANOVA, follow-up ANOVAs, and simple main effects\n\nThe code required to run the ANOVA is given below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the mixed design ANOVA model\nwordPronModel = anova_test(data = wordPronLngSep, dv = Errors, wid = Participant, between = Age, within = c(Frequency, WordType), detailed = TRUE)\n# Round the p values to three decimal places\nwordPronModel$p = round(wordPronModel$p, 3)\n# Print the model summary\n(wordPronModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nANOVA Table (type II tests)\n\n                  Effect DFn DFd    SSn SSd        F     p p<.05   ges\n1            (Intercept)   1   8 1166.4 4.5 2073.600 0.000     * 0.981\n2                    Age   1   8   19.6 4.5   34.844 0.000     * 0.467\n3              Frequency   1   8    6.4 8.7    5.885 0.041     * 0.222\n4               WordType   1   8   16.9 3.7   36.541 0.000     * 0.430\n5          Age:Frequency   1   8    6.4 8.7    5.885 0.041     * 0.222\n6           Age:WordType   1   8    4.9 3.7   10.595 0.012     * 0.179\n7     Frequency:WordType   1   8   12.1 5.5   17.600 0.003     * 0.351\n8 Age:Frequency:WordType   1   8    4.9 5.5    7.127 0.028     * 0.179\n```\n:::\n:::\n\n\nTo create the model, the first argument we supplied to `anova_test` was the name of our data, `wordPronLngSep`. The second argument we supplied was our dependent variable, `Errors`. The third argument we supplied was `Participant`, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our between-participants factor, `Age`. The fifth argument we supplied as our within-participants factors, `Frequency` and `WordType`. \n\nAs in our first example, our factors have only two levels, so `anova_test` does not give us Mauchly's test of sphericity or the Greenhouse-Geisser correction for the within-participants factors. \n\nInspecting the ANOVA table, rows two to four give the main effects of Age, Frequency, and Word Type; rows five to seven give the Age $\\times$ Frequency, Age $\\times$ Word Type, and Frequency $\\times$ Word Type two-way interactions; and row eight gives the Age $\\times$ Frequency $\\times$ Word Type three-way interaction. Looking at the $p$ values, we can see that all the main effects, two-way interactions, and the three-way interaction are significant. \n\nBecause the three-way interaction is significant, we need to analyse it further. As before, to do this we need to re-analyse the data as a series of two-factor ANOVAs. First, we must decide which factor to split the analysis by and the obvious contender is the between-participants factor of age. Accordingly, what we need to do is to perform two, two-factor ANOVAs:\n\n1. a 2 (frequency: low *vs.* high) $\\times$ 2 (word type: regular *vs.* irregular) ANOVA for the *7-year-olds* only.\n\n1. a 2 (frequency: low *vs.* high) $\\times$ 2 (word type: regular *vs.* irregular) ANOVA for the *9-year-olds* only.\n\nWe will start by running the two-factor ANOVA for the 7-year-olds (ignoring the data for the 9-year-olds). To do this, we first need to produce a filtered version of our data set called `sevenYearsOnly` that only includes the results for the 7-year-old children. We can create that with the following piece of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the data for the \"7 Year Olds\" only\nsevenYearsOnly = wordPronLngSep %>%\n  filter(Age == \"7 Year Olds\") \n```\n:::\n\n\nNext, we can run our two-factor ANOVA on this filtered data set. The steps are the same as above, except that we need to drop the Age factor included previously (remember, we are only analysing the data for the 7-year-olds). \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the two-factor ANOVA for the \"7 Year Olds\" only\nsevenYearsModel = anova_test(data = sevenYearsOnly, dv = Errors, wid = Participant, within = c(Frequency, WordType), detailed = TRUE)\n# Round the p values to three decimal places\nsevenYearsModel$p = round(sevenYearsModel$p, 3)\n# Print the model summary\n(sevenYearsModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nANOVA Table (type III tests)\n\n              Effect DFn DFd   SSn SSd        F     p p<.05   ges\n1        (Intercept)   1   4 744.2 0.3 9922.667 0.000     * 0.988\n2          Frequency   1   4   0.0 2.5    0.000 1.000       0.000\n3           WordType   1   4   1.8 2.7    2.667 0.178       0.164\n4 Frequency:WordType   1   4   0.8 3.7    0.865 0.405       0.080\n```\n:::\n:::\n\n\nThe only thing we are interested in from the ANOVA table is the outcome of the two-way interaction between Frequency and Word Type; you can ignore everything else. You can see that the interaction is nonsignificant in this instance, $p$ = .405. Thus, for 7-year-old children Frequency and Word Type do not combine with one another to influence pronunciation errors.\n\nWhat about 9-year-old children?\n\nThat brings us to our second two-factor ANOVA. For this, we now need to produce a filtered version of our data set called `nineYearsOnly` that only includes the results for the 9-year-old children. We can create that with the following piece of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnineYearsOnly = wordPronLngSep %>%\n  filter(Age == \"9 Year Olds\") \n```\n:::\n\n\nThe command `filter(Age == \"9 Year Olds\")` tells R that we only want the data for the 9-year-old children. \n\nNext, we can run our two-factor ANOVA on this filtered data set.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the two-factor ANOVA for the \"9 Year Olds\" only\nnineYearsModel = anova_test(data = nineYearsOnly, dv = Errors, wid = Participant, within = c(Frequency, WordType), detailed = TRUE)\n# Round the p values to three decimal places\nnineYearsModel$p = round(nineYearsModel$p, 3)\n# Print the model summary\n(nineYearsModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nANOVA Table (type III tests)\n\n              Effect DFn DFd   SSn SSd       F     p p<.05   ges\n1        (Intercept)   1   4 441.8 4.2 420.762 0.000     * 0.971\n2          Frequency   1   4  12.8 6.2   8.258 0.045     * 0.492\n3           WordType   1   4  20.0 1.0  80.000 0.001     * 0.602\n4 Frequency:WordType   1   4  16.2 1.8  36.000 0.004     * 0.551\n```\n:::\n:::\n\n\nThis time the interaction between Frequency and Word Type is significant, *p* = .004, so we now need to perform a simple main effects analysis to determine why.\n\nBefore we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the 9 year olds ANOVA table\nnineYearsAnovaTable = get_anova_table(nineYearsModel)\n```\n:::\n\n\nNext, we need to calculate the cell totals for each of the four conditions and the number of observations (i.e., scores) in each cell:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get cell totals and counts\nnineYearsCellTotals = nineYearsOnly %>%\n  # Organise the output by the \"Frequency\" and \"WordType\" factors\n  group_by(Frequency, WordType) %>%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(Errors),n = n())\n  # Print the results\n  (recallCellTotals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 4\n# Groups:   LearningContext [2]\n  LearningContext TestingContext   sum     n\n  <fct>           <fct>          <dbl> <int>\n1 Under           Under             38     5\n2 Under           Land              23     5\n3 Land            Under             17     5\n4 Land            Land              34     5\n```\n:::\n:::\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Frequency at Word Type. This means, we are going to:\n\n* Test the difference between low and high frequency *regular* words only.\n* Test the difference between low and high frequency *irregular* words only.\n\nTo do this, we need to declare Frequency as the \"fixed\" factor (we are always comparing low and high frequency words) and Word Type as the \"across\" factor (the comparison between low and high frequency words occurs \"across\" the regular and irregular levels of the Word Type factor): \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create \"fixed\" and \"across\" factors\nfixed  = \"Frequency\"\nacross = \"WordType\"\n```\n:::\n\n\nWe then generate the simple main effects of Frequency at Word Type with the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple main effects of \"Frequency\" at \"WordType\"\nsmeFrequency = simple(nineYearsCellTotals,nineYearsAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeFrequency$P = round(smeFrequency$P, 3)\n(smeFrequency)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Levels Sum of Squares Degrees of Freedom Mean Square           F     P\n1    Regular            0.1                  1        0.10  0.06451613 0.812\n2  Irregular           28.9                  1       28.90 18.64516129 0.012\n3 Error term            6.2                  4        1.55  0.00000000 0.000\n```\n:::\n:::\n\n\nThe simple main effect of Frequency at regular words is nonsignificant, $p$ = .812, indicating that pronunciation errors for regular words do not differ according to whether they are low or high in frequency. However, the simple main effect of Frequency at irregular words is significant, $p$ = .012, indicating that pronunciation errors for irregular words are higher when they are of low frequency than when they are of high frequency (check the descriptive statistics to verify this).\n\nNow, let's calculate the simple main effects of Word Type at Frequency. This means, we are going to:\n\n* Test the difference between regular and irregular *low* frequency words only.\n* Test the difference between regular and irregular *high* frequency words only.\n\nTo do this, we need to declare Word Type as the \"fixed\" factor and Frequency as the \"across\" factor: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create \"fixed\" and \"across\" factors\nfixed  = \"WordType\"\nacross = \"Frequency\"\n```\n:::\n\n\nWe then generate the simple main effects of Word Type at Frequency as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple main effects of \"WordType\" at \"Frequency\"\nsmeWordType = simple(nineYearsCellTotals,nineYearsAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeWordType$P = round(smeWordType$P, 3)\n(smeWordType)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Levels Sum of Squares Degrees of Freedom Mean Square     F     P\n1        Low           36.1                  1       36.10 144.4 0.000\n2       High            0.1                  1        0.10   0.4 0.561\n3 Error term            1.0                  4        0.25   0.0 0.000\n```\n:::\n:::\n\n\nThe simple main effect of Word Type at low frequency is significant, $p$ $<$ .001, indicating that there are more pronunciation errors for low frequency irregular words than for low frequency regular words. Second, the simple main effect of word type at high frequency is nonsignificant, $p$ = .561, indicating that pronunciation errors for high frequency regular and irregular words do not differ.\n\nIn short, the three-way interaction arose because pronunciation errors in 7-year-old children are unaffected by word frequency and word type, whereas pronunciation errors in 9-year-old children are influenced by these factors. Specifically, low frequency irregular words are associated with more pronunciation errors than low frequency regular words, but there is no difference between the frequency of pronunciation errors for high frequency irregular and regular words. This pattern can be seen in Figure 2 below which plots the data for the word pronunciation study.\n\n![Figure 2. Pronunciation errors as a function of word frequency and word type for 7-year-old children (left panel) and 9-year-old children (right panel).](images/WordPronunciationDatax.png){width=75%}\n\n### Writing up the results\n\nThe conventions for writing up the results of a mixed three-factor ANOVA are the same as for a fully within-participants three-factor ANOVA (and indeed a fully between-participants three-factor ANOVA), so see my example write-up for the memory and context study. \n\n<!-- Why don't you have a go at writing up these results? I'll give an example write-up when I post the instructor's copy of the materials online at the end of the week. -->\n\n### Additional tasks\nPhew!! That's probably the most we have covered in any of our lab sessions. Well done for making it through to the end! \n\nHere are some additional tasks you might consider doing:\n\n* Write-up the results of the word pronunciation study.\n* Generate the interaction plots in Figures 1 and 2, but add error bars (confidence intervals) to the data points.\n\nI'll include the write-up/plot code for these additional tasks in the instructors copy of the lab materials at the end of the week.\n\n<br>\n<br>\n<br>\n<br>",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}