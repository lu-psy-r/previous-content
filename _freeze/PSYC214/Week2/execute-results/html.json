{
  "hash": "290241dd59d645db086b48110b344187",
  "result": {
    "markdown": "---\ntitle: 2. One-Factor Between-Participants ANOVA\nsubtitle: \"Richard Philpot, Mark Hurlstone\"\norder: 3\n---\n\n\n# Lecture\n\nWatch Part 1 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=1993847)\n\nWatch Part 2 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=1993848)\n\nWatch Part 3 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=1993851)\n\nWatch Part 4 [here](https://modules.lancaster.ac.uk/mod/panopto/view.php?id=1993852)\n\nDownload the lecture slides [here](data/Wk2/Psyc214 - Lecture 2 [small].pdf), and [here](data/Wk2/Psyc214 - Lecture 2 [large].pdf) for a larger version.\n\n# Lab\n\nWelcome back to Lab 2 of Year 2 Stats!\n\nIt was incredibly pleasant meeting you last week and also getting snug and reacquainted with our old flame - R. Once again, this week, we will be working from an activity sheet, which outlines a bunch of \"fun\" tasks to complete in R Studio. The objectives of today's lab are three fold:\n\n1. Describing and wrangling a new and shiny data set\n2. Running and reporting (to APA standards) a one-factor between participants analysis of variance\n3. Creating a publication quality bar chart (no pressure), which plots our data \n\nAs always, myself (Richard Philpot), Mark Hurlstone (weeks 6-10) and an apt team of GTAs will be on hand to help you through this journey - so please do not panic - you're never alone. In Psyc214, we also encourage peer engagement and joined problem solving - so please do not hesitate to ask for help from another on your table or to work together in small groups. Right, that enough for now, so let's get started!ðŸ’ª\n  \n******\n## 1 - General Introduction to Lab 2\n\n| *\"There is no education like adversity\"* | Benjamin Disraeli.\n\n![Benjamin Disraeli](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/person/image/987/Benjamin-Disraeli.jpg)\n  \n******\n### 1.1 Access to R Studio\n---\ntitle: \"Statistics for Psychologists\"\npage-layout: article\n---\n\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS' [instructions here](https://portal.lancaster.ac.uk/ask/digital/services/university-it-network/vpn/) or connecting to Eduroam [here](https://portal.lancaster.ac.uk/ask/digital/services/university-it-network/wi-fi/).\n\nWhen you are connected, navigate to [https://psy-rstudio.lancaster.ac.uk](https://psy-rstudio.lancaster.ac.uk), where you will be shown a login screen that looks like the below. Click the option that says \"Sign in with SAML\".\n\n![](/Includes/1LoginSAML.png) <!-- says no image but it's all about the include setup -->\n\nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n![](/Includes/2LoginUser.png)\n\n![](/Includes/3LoginPass.png)\n\n::: {.callout-note}\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n:::\n\n\n\n******\n### 1.2 Loading the packages (dependencies) and loading the data set\n\nPerfect. If you are this far you're in R Studio - the hot bed of statistical joys. \n\nSimilar to last week, we first need to access the dataset. \n\n1. Step 1. To access today's dataset, we first just need to download it from [here](data/wk2/week2_robo_lab.csv)\n\n\n2. Now that you have the data saved on your machine, we next need to make a space on our own individual R Studio servers in which to house the data. Like last week, to do this, we first need to create a folder. On the server, please navigate to the bottom right panel (see figure below). Here, under the 'files' you will see the option to add 'New Folder'. Click on this and name the new folder *psyc214_lab_2* Note. This needs to be spelled correctly and we need to make sure we that we specify this is lab_2! We roll with the times, and lab_1 is now an artifact :)\n\n\n3. Great. You have now created a brand, spanking new folder for week 2's content. To upload today's *week2_robo_lab.csv* file into this folder, please open your new *psyc214_lab_2* folder. When in the new folder, select the 'Upload' tab (see figure below). This will present a box that will ask where the data is that you want to upload. Click on \"Browse...\", find where you have the *week2_robo_lab.csv* data file on your computer and click 'OK'\n\nTop work! The data is now chillaxing on your own R Studio server, ready to be called upon.\n\n**STOPPPPPPPPP!!!!! We'll want to be able to save this session on our server: to do so please:**\n**click 'File' on the top ribbon --> New project.**\n**Next, select existing directory and name the working directory ~/psyc214_lab_2 --> hit 'create project**\n\nThis will now create a project in this week's R Studio Server lab 2 folder where it will save your progress! You can then return to this at a later date - likely close to the exam *wink wink*\n\nLike last week, it is important that you work through a script and not through the console. A script is a posh text editor that allows you to write, edit, annotate and save lines of of code. Navigate to the top left pane of RStudio, select File -> New File -> R Script. Working from a script will make it easier to edit your code and you will be able to save your work for a later date.\n\n**Be sure to save the script using the File -> Save As....**\n\n\nLet's start by loading the packages (i.e., the dependencies) we'll use for this session.\n\nPlease copy or type the following commands into R Studio to get these packages activated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # allows pipes, arrang(), summary(), aov(), etc\nlibrary(rstatix) # allows group_by()\nlibrary(effectsize) # a neat little package that provides the effect size\nlibrary(Rmisc) # allows shorthand calculations of standard errors and confidence intervals\n```\n:::\n\n\nMore on these packages later.\n\nData... Assemble! Ok - Let's load the data and crack on with today's work.\n\n  4. First we need to set the working directory to 'psyc214_lab_2', i.e., tell R Studio the location in which today's data sits, waiting to stand to attention . To recap, the working directory is the default location or folder on your computer or server by which R will read/save any files. \n  \n  The working directory can be set with the following R code:\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n  setwd(\"~/psyc214_lab_2\")\n```\n:::\n\n\n  5. Great. Now we're ready to load the dataset which we kindly housed on our server. To do this, please type the following code:\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n lab2_data <- read_csv(\"week2_robo_lab.csv\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n  \n  where 'lab2_data' is the name we've assigned that R will recognise when it calls up our data, `read_csv` is the R command to pull up the data and \"week2_robo_lab.csv\" is the name of the data file stored on the server. \n\n*Note. During the rest of this session, you will not need to refer to the original downloaded .csv data file. R has all the information stored under the 'lab2_data' variable. Further note, you could have called 'lab2_data' by pretty much any name you like... 'my_data', 'robo_2', 'stat_attack', etc. - the name is somewhat arbitrary. For the purpose of this lab session and for ease of read, 'lab2_data', is perhaps more suitable.*\n\n\n------ \n## 2 - Today's lab activities \n\nNow that you have loaded the dataset, let's have a play.\n\n\n### 2.1 Some background information about the dataset\n\n![Introducing the robo-lab study](https://images.pexels.com/photos/8386434/pexels-photo-8386434.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260)\n\nLast week, LU researchers tested whether LU statistics students would respond well to a new, ambitious initiative to deploy a team of synthetic robots as lab assistants. The researchers hypothesized that:\n- Individuals assigned Robot B(eta) would score significantly higher in their Psyc214 tests (Hypothesis 1)\n- Robot B(eta) would be significantly more liked than Robot(A)lpha (Hypothesis 2)\n\nThe researchers expected that this difference would be explained by Robot B(eta)'s closer resemblance to other human beings. Indeed, prior to the initiative, independent raters had reliably agreed that Robot B(eta) resembled a human being more than Robot A(lpha).\n\n![**Robot A(lpha)**](images/Wk1/RobotA.png)\n\n![**Robot B(eta)**](https://images.pexels.com/photos/2599244/pexels-photo-2599244.jpeg?auto=compress&cs=tinysrgb&h=750&w=1260)\n  \n**The authors ran an independent samples t-test and were vindicated in their predictions. They found a significant difference between groups and were able to reject the null hypothesis - i.e., reject the notion that the two samples had come from the same population**\n\nThe researchers rejoiced, wrote a manuscript detailing the research, and sent this manuscript off to the journal *Computers in Human Behavior*.\n\nRoll on a few months and the researchers received a response. Reviewer 2 praised the research, but had reservations about the authors' sweeping interpretation that as robots become more 'human-like' they will always be more approachable and liked. They further suggested that more realistic, unlikeable robots *may* ultimately be associated with lower Psyc214 test scores. \n\nReviewer 2 cited the phenomenon of the 'uncanny valley' - which states that while more 'humanlike' robots are typically more liked, there reaches a point in which strong resemblance between machine and person induces strong revulsion and reduced likeability scores.\n\nFor the manuscript to be accepted for publication, Reviewer 2 asked the research team to repeat the experiment with a new cohort of students, but this time to introduce a third robot, one which is even more human-like! \n\nNever one to disappoint the reviewers, the research team went back to the drawing board and **drumrolllllllll** ...... Introducing Robot O(mega)!!!!!\n\n\n![**Robot O(mega)**](https://images.unsplash.com/photo-1593376893114-1aed528d80cf?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=958&q=80)\n\n\n******\n### 2.2 Study manipulation\n\nIt was now time to test how the students responded to the new robot prototype. In a controlled experiment, the researchers randomly assigned groups of students either Robot A(lpha), Robot B(eta), or Robot O(mega). Note this experiment is one factor (Robot assignment) study made of three levels (Robot A, Robot B, Robot C).\n\nThose individuals assigned Robot A(lpha) were denoted as belonging to 'Group A'.\n\nThose individuals assigned Robot B(eta) were denoted as belonging to 'Group B'.\n\nThose individuals assigned Robot O(mega) were denoted as beloning to 'Group O'.\n\nNote. The groups are mutually exclusively - i.e., a participant was assigned to *either* Group A/Robot A, Group B/Robot B, Group O/Robot O.\n\n\n  \n******\n### 2.3 Outcome (a.k.a dependent) variables\n\nThe dependent variables for this new study remained the same as in the previous study\nRecall, the research team measured student stats competence (something important!) and attitudes towards teaching support (something also very important!).\n\nDV1: Student stats competence was measured by a Psyc214 test score; ranging between 40-100.\n\nDV2: Student attitudes towards the robot lab assistant was assessed with a likert scale response between 1-7; with 1 = 'strongly dislike' and 7 = 'strongly like'.\\\n  \n******\n### 2.4 Predictions of the research team\n\nThe research team respectfully disagreed with Reviewer 2 and decided on a general research hypothesis that the more realistic a robot was, the more liked it would and the higher the students would score on stats. \nSpecifically, the experimental hypotheses were that:\n\n- Psyc214 assessment scores would be significantly higher amongst individuals assigned Robot O(mega) than those assigned Robot A(lpha) or B(eta) (Hypothesis 1)\n-  Attitudes towards a participant's robot would be significantly higher for individuals assigned Robot O(mega) than those assigned Robot A(lpha) or B(eta)(Hypothesis 2)\n\nPrior to the experiment, independent raters reliably agreed that Robot O(mega) more closely resembled a human being than Robot B(eta) [second closest resemblance] and Robot A(lpha) [lowest resemblance].\n  \n******\n### 2.5 Exploring our data\nNow that you are familiar with the research setting, let's explore this week's data.\n\nLet's start with a nose, by looking a the top 20 rows of our dataset - like last week we will use the `head()` command.\n\nTo view these first rows, please apply the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(lab2_data, n = 20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 Ã— 4\n      ID Group Likeability Score\n   <dbl> <chr>       <dbl> <dbl>\n 1     1 A               2    71\n 2     2 A               2    53\n 3     3 A               3    62\n 4     4 A               3    65\n 5     5 A               4    64\n 6     6 A               1    61\n 7     7 A               3    63\n 8     8 A               3    50\n 9     9 A               2    59\n10    10 A               4    69\n11    11 A               4    52\n12    12 A               4    63\n13    13 A               3    59\n14    14 A               2    53\n15    15 A               2    54\n16    16 A               2    59\n17    17 A               3    57\n18    18 A               3    54\n19    19 A               2    72\n20    20 A               3    47\n```\n:::\n:::\n\n\nBecause of the way we ordered our data in the original .csv spreadsheet, we are not able to see beyond the first 20 cases in Group A. Therefore, let's use the arrange() function to order our data instead by Score - we can combine this with the head() through the use of a pipe %>%. *Please recall, a pipe works as a specialized chain, letting you pass an intermediate result onto the next function*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab2_data %>% arrange(desc(Score)) %>% head(n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 Ã— 4\n      ID Group Likeability Score\n   <dbl> <chr>       <dbl> <dbl>\n 1   207 O               2    79\n 2   199 O               2    77\n 3   222 O               1    77\n 4   175 O               3    75\n 5   176 O               1    75\n 6   210 O               2    75\n 7   219 O               1    75\n 8   121 B               3    74\n 9   142 B               5    74\n10   161 O               2    74\n11   211 O               3    74\n12   223 O               3    74\n13    87 B               4    73\n14   183 O               2    73\n15    19 A               2    72\n16    94 B               6    72\n17   148 B               4    72\n18   166 O               3    72\n19   177 O               3    72\n20   179 O               1    72\n```\n:::\n:::\n\n\nThis all looks fine. We can see that the highest score a student received was 79. We also see a tendency in data in which the highest scores are achieved by the Robot Beta and Omega groups (i.e., groups B and O).\n\nNow let's do the same and check the 20 highest Likeability values.\nTo do this, please copy the code above, but change 'Score' to 'Likeability'\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\nlab2_data %>% arrange(desc(Likeability)) %>% head(n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 Ã— 4\n      ID Group Likeability Score\n   <dbl> <chr>       <dbl> <dbl>\n 1   238 O              33    63\n 2   126 B               7    50\n 3    83 B               6    53\n 4    94 B               6    72\n 5    99 B               6    64\n 6   108 B               6    70\n 7   115 B               6    64\n 8   116 B               6    60\n 9   123 B               6    63\n10   129 B               6    60\n11   133 B               6    64\n12   145 B               6    63\n13   147 B               6    71\n14   150 B               6    69\n15   154 B               6    61\n16    82 B               5    52\n17    86 B               5    67\n18    92 B               5    62\n19    95 B               5    48\n20    96 B               5    62\n```\n:::\n:::\n\n\nIf done correctly you will receive the following output\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 Ã— 4\n      ID Group Likeability Score\n   <dbl> <chr>       <dbl> <dbl>\n 1   238 O              33    63\n 2   126 B               7    50\n 3    83 B               6    53\n 4    94 B               6    72\n 5    99 B               6    64\n 6   108 B               6    70\n 7   115 B               6    64\n 8   116 B               6    60\n 9   123 B               6    63\n10   129 B               6    60\n11   133 B               6    64\n12   145 B               6    63\n13   147 B               6    71\n14   150 B               6    69\n15   154 B               6    61\n16    82 B               5    52\n17    86 B               5    67\n18    92 B               5    62\n19    95 B               5    48\n20    96 B               5    62\n```\n:::\n:::\n\n\n![Hold up!](https://images.unsplash.com/photo-1542707309-4f9de5fd1d9c?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=987&q=80)\n  \nNow wait! This data looks proper dodgy. We can see that the highest likeability score was 33. This can't be right? Can it?! Nope, definitely not, the scale should range from 1 - 7, meaning this is an error.\n\nWhat on earth happened? Perhaps the '3' key was hit twice by mistake? Perhaps the researcher dropped their biscuit on the keyboard when working? As we do not know what happened, we need to ensure that this data point is recorded as missing.\nTo do this, please type or copy/paste the following command\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab2_data$Likeability[lab2_data$Likeability > 7] <- NA\n```\n:::\n\n\nThis is asking R Studio to scan the likeability column of our dataset and to set any value above 7 as missing.\n\nNow let's check out the top 20 likeability scores again, just to be sure the 33 has gone.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  lab2_data %>% arrange(desc(Likeability)) %>% head(n=20)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 Ã— 4\n      ID Group Likeability Score\n   <dbl> <chr>       <dbl> <dbl>\n 1   126 B               7    50\n 2    83 B               6    53\n 3    94 B               6    72\n 4    99 B               6    64\n 5   108 B               6    70\n 6   115 B               6    64\n 7   116 B               6    60\n 8   123 B               6    63\n 9   129 B               6    60\n10   133 B               6    64\n11   145 B               6    63\n12   147 B               6    71\n13   150 B               6    69\n14   154 B               6    61\n15    82 B               5    52\n16    86 B               5    67\n17    92 B               5    62\n18    95 B               5    48\n19    96 B               5    62\n20   105 B               5    61\n```\n:::\n:::\n\n\nPhew, that's all sorted now.  \nWow, looking at the data it seems that Robot B(eta) is extremely likeable - cute little fella.\n\nNext, let's use the ggplot  package to create two histograms to inspect the distribution of all data points for both Likeability and Scores variables. First we call up the ggplot function. In the brackets, we first inform the function the name of the dataset - for us, lab2_data. Then, we write aes() *shorthand for for astethics* followed by the variable we would like on the x asis - for us 'Likeability'. Finally, we choose number of bins (e.g., how many columns we'd like for our plot) and choose a fill colour - I have grey, but as this is exploratory and not an official APA style report feel free to try other colours you like. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n  ggplot(data = lab2_data, aes(x = Likeability)) + \n  geom_histogram(bins = 5, fill = \"grey\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing non-finite values (`stat_bin()`).\n```\n:::\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\nThis gives us some general information. Have a chat with a study neighbour or with your inner self about how the number of occurrences tend to be distributed across our Likeability measure.\n\nNow, I would like you to borrow from the example above and try to make your own histogram plot for our other DV, Score. This will require the careful replacing of the x = Likeability to our other dependent variable name. Also, please change the number of bins (aka columns) to 12.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\n  ggplot(data = lab2_data, aes(x = Score)) + \n  geom_histogram(bins = 12, fill = \"grey\")\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nIf done correctly, you should receive the following output:\n\n::: {.cell}\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nOne again, this provides some nice frequency information. Have a chat with a study neighbour or with your inner self about how the number of occurrences tend to be distributed across our Score measure.\n\nAlthough this information is useful, we are restricted to frequency data (i.e., number of occurrences data) and aggregated data (i.e., we're seeing all data points, regardless of the group assigned)\n\nTherefore, let's use the `rstatix()` package again to get some neat and tidy summary statistics. We first specify the dataset we will use - which is lab2_data. After this we will use a pipe to ask R Studio to pass this data set on to the next function. Here, we use the group_by() function to specify that we want to distinguish between our three different groups - a variable conveniently named 'Group'. We then use another pipe to pass the intermediate result onto the next function - the get_summary_stats(). In the parentheses we include our two DVs 'Likeability' and 'Score'. Finally we are asked to specify what 'type' of summary statistic we would like. Let's go for mean, standard deviation, min and max values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n  lab2_data %>%\n  group_by(Group) %>%\n  get_summary_stats(Likeability, Score, show = c(\"mean\", \"sd\", \"min\", \"max\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 7\n  Group variable        n  mean    sd   min   max\n  <chr> <fct>       <dbl> <dbl> <dbl> <dbl> <dbl>\n1 A     Likeability    80  2.5  0.928     1     4\n2 A     Score          80 58.1  6.45     44    72\n3 B     Likeability    80  4.5  1.01      2     7\n4 B     Score          80 60.4  7.27     40    74\n5 O     Likeability    79  2.11 0.847     1     4\n6 O     Score          80 63.6  7.22     47    79\n```\n:::\n:::\n\n\nOk, perfect. This now tells us a lot of information. For example, we can see that the mean Likeability scores for Groups A, B, O are 2.5, 4.5, 2.11, respectively.\n\nOnce again, it seems that we may have different values between groups. However, it is impossible to know whether there are statistically differences based on eye-balling the means. Rather, we need to statistically examine the degree of shared within and between group variances.\n\n******\n\n## 3. And more\n\n### 3.1 One-factor between group ANOVA\n\nTo identify whether the differences between groups are statistically significant, we need to perform an Analysis of Variance. First, let's be sure we have chosen the correct type of ANOVA. We have one factor. This is because we have one Independent Variable (Robot) with three levels (Robot A, B and O). Also, the design is between group, as each participant is assigned to only one robot, and data from only one time point is taken.\n\nTo run our one-factor between participants ANOVA, we will use tidyverse's 'aov()' function. Let's set 'Likeability' as our first dependent variable of examination. 'Group' is then our predictive factor (independent variable). Note, in the code below, the DV and the IV are separated by the '~' character. Please carry out the following command. This also includes requesting a summary and effectsize for our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel_1 <- aov(data = lab2_data, Likeability ~ Group) #We name our overarching model Model_1\nsummary(Model_1)#We ask for a summary of this model. This provides F statistic and P value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value Pr(>F)    \nGroup         2  261.6  130.80   151.3 <2e-16 ***\nResiduals   236  204.0    0.86                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n1 observation deleted due to missingness\n```\n:::\n\n```{.r .cell-code}\neffectsize(Model_1) # we ask for an eta2 effect size for our model\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nGroup     | 0.56 | [0.50, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n```\n:::\n:::\n\n\nThis has provided us with a whole bunch of useful information! \nIt shows us the between group variability value (do you remember, the numerator value in our F ratio equation from lecture 2?), which R has named the Group Mean Square (a value here of 130.80). \n\nIt also provides the within group variability value - aka the error term (do you remember, the denominator in our F ratio equation from lecture 2), which R has named the Residuals mean square (a value here of 0.86)\n\nIt provides the crucial F-ratio statistic, here a value of 151.3. \n**note, like in lecture 2, you could have calculated this value yourselves by dividing the 130.80 numerator by the 0.86 denominator. Try it in the r console by typing: Note, the value will differ slightly because of rounding - i.e., the number of decimals R works with versus us**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n130.80 / 0.86\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 152.093\n```\n:::\n:::\n\n\nThe output also shows the Eta-squared (Î·2) effect size. As a rule of thumb, Î·2 = 0.01 indicates a small effect; Î·2 = 0.06 indicates a medium effect; Î·2 = 0.14 indicates a large effect. Our eta-squared (Î·2) represents a very large effect - i.e., the effect of our experimental manipulation on the Likeability scores was very large indeed. \n\n******\n### 3.2 Reporting the results of the one-factor between-participants ANOVA in APA format\n\nAll results should be written up in accordance with the American Psychological Association's (APA) guidance. This ensures that your results are easy to interpret and that all the relevant information is present for those wishing to review/replicate your work.\n\nThe current results can be reported as following:\n\"A one-factor between-participants ANOVA revealed that likeability scores were significantly different between our robot groups (Robot A *M* = 2.50, *SD* = 0.93; Robot B *M* = 4.50, *SD* = 1.01; Robot O *M* = 2.11, *SD* = 0.85), *F*(2,236) = 151.3, *p* < .001, Î·2 = 0.56, 95% CI[0.50, 1.00].\n\n*Please note. While the ANOVA tells us that there are differences between groups, it doesn't tell us specifically which groups differ from one another. For example do Group A and B statistically differ? A and O? B and O? The only way to know this definitively is to carry out posthoc/planned contrast tests. We won't do this today, as this will be a key facet of Lecture 3 and Lab 3, but please be aware that without these supplementary analyses the above APA reporting is incomplete*\n\n******\n### 3.3 Creating an APA barchart\n\nWhen reporting the results of an ANOVA in an APA report or publication, it is typical to  include a barchart, illustrating the average scores and error bars for each group.\n\nAs this is a formal chart, it is more sterile and less jazzy than the coloured graphs you have produced before. There are also important APA aesthetics which are required, such as grey colour scales, white background, black axes and text, etc. Before you fall asleep, let's make one of these posh plots.\n\nPlease copy and paste the following code. Please pay attention to the #annotations, which provide detail as to what each line of code is doing:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab2_data %>% #our dataset\n    ggplot(aes(x= Group, # specifying our X axis\n               y = Likeability, # specifying our Y axis\n               fill = Group)) + # How we will colour our separate bars\n    geom_col(width = .7, position = position_dodge(.3)) + # width of columns and space between columns\n    scale_fill_manual(values = c(\"#D4D4D4\", \"#737373\", \"#323232\")) + # APA colours for bars - yuck!\n    scale_y_continuous(expand = expansion(0), # trick to remove space below 0 on y axis\n                       limits = c(0, 7), # set limit of Y axis to 7\n                       breaks = seq(0, 7, 1)) + # set the breaks between yaxis points\n    theme(panel.background = element_blank(), # removing the default grey background panel\n          axis.line = element_line(color = \"black\"), # creating black axes lines\n          axis.ticks.x = element_blank(),\n          legend.position = \"none\") # removing the legend, which is redundant\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing missing values (`geom_col()`).\n```\n:::\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nThis looks great. But to make it APA publishable we need to add some error bars to our plot. To add things like error bars and 95% Confidence intervals, we first need work out the values of these summary statistics. Luckily, an amazing package called **Rmisc** will provide these values for us with a simple command.\n\nLet's make a new dataset for plotting our graphs which will include these values - using the summarySE() function of **Rmisc**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLike_plot <- summarySE(lab2_data, measurevar=\"Likeability\", groupvars=c(\"Group\"), na.rm = TRUE) #Like_plot is the name of our plot data set and na.rm asks R Studio to ignore our missing data point when calculating there summaries\n```\n:::\n\n\nNot let's view this new data using head()\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(Like_plot)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Group  N Likeability        sd         se        ci\n1     A 80    2.500000 0.9277713 0.10372798 0.2064654\n2     B 80    4.500000 1.0063092 0.11250879 0.2239431\n3     O 79    2.113924 0.8471270 0.09530923 0.1897461\n```\n:::\n:::\n\n\nYou can see it now gives us a summary of our Likeability data, including the Means, Standard Deviations, Standard Errors and confidence intervals.\n\nNow let's add the Standard Error Bars to our graph. To do this, we repeat the same plotting code from above, but with two key differences.\n1. We need to ensure that we change the dataset for the graph from *lab2_data* to *Like_plot* (our new summary data).\n2. We add the function geom_errorbar() to specify that we would like to include error bars for our graph.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nLike_plot %>% #our dataset\n    ggplot(aes(x= Group, # our X axis\n               y = Likeability, # our Y axis\n               fill = Group)) + # How we will colour our seperate bars\n    geom_col(width = .7, position = position_dodge(.3)) + # width of columns and space between columns\n    scale_fill_manual(values = c(\"#D4D4D4\", \"#737373\", \"#323232\")) + # APA colours for bars\n    scale_y_continuous(expand = expansion(0), # trick to remove space below 0 on y axis\n                       limits = c(0, 7),\n                       breaks = seq(0, 7, 1)) + # set limit of Y axis to 7\n    theme(panel.background = element_blank(), # removing the background panel colour\n          axis.line = element_line(color = \"black\"), # creating black axes lines\n          axis.ticks.x = element_blank(),\n          legend.position = \"none\") + # removing the legend, which is redundant\n    geom_errorbar(aes(ymin=Likeability-se, ymax=Likeability+se, width = 0.2))\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nPerfect! Error bars accomplished.\n\n  \n******\n## 4 Further tasks\n\n1. We have seen that there are signficant differences between at least two groups when it comes to the Likeability scores. Now please eye ball the means and the barchart and try to predict which groups differed. Talk this over with a lab mate or instructor. We will test this with further analyses next week!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#STUDENT COMPLETES\n```\n:::\n\n\n2. We have yet to look for statistical differences between in Psyc214 Scores between our groups. Run another one-factor between-participants ANOVA following the instructions above. This time, be sure to replace our previous DV Likeability with our other DV Scores. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#ANSWER CODE\nModel_2 <- aov(data = lab2_data, Score ~ Group)\nsummary(Model_2)#We ask for a summary of this model. This provides F statistic and P value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nGroup         2   1223   611.3   12.52 6.77e-06 ***\nResiduals   237  11571    48.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\neffectsize(Model_2) # we ask for an eta2 effect size for our model\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nGroup     | 0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n```\n:::\n:::\n\n\nWhen completely correctly, you should get the following output:\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nGroup         2   1223   611.3   12.52 6.77e-06 ***\nResiduals   237  11571    48.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nGroup     | 0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n```\n:::\n:::\n\n\n3. Report this second ANOVA to APA standards following example above (3.2)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#STUDENT COMPLETES\n```\n:::\n\n\n4. Repeat the steps 3.3 and again create an APA standard barchart - this time plotting our three different groups and their Psyc214 Scores. Please finish this off with error bars. *Hint for the error bars - you will need to create a new 'Scores_plot' sub data set using the summarySE() function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nScores_plot <- summarySE(lab2_data, measurevar=\"Score\", groupvars=c(\"Group\"), na.rm = TRUE) #Like_plot is the name of our plot data set and na.rm asks R Studio to ignore our missing data point when calculating there summaries\n\nScores_plot %>% #our dataset\n    ggplot(aes(x= Group, # our X axis\n               y = Score, # our Y axis\n               fill = Group)) + # How we will colour our seperate bars\n    geom_col(width = .7, position = position_dodge(.3)) + # width of columns and space between columns\n    scale_fill_manual(values = c(\"#D4D4D4\", \"#737373\", \"#323232\")) + # APA colours for bars\n    scale_y_continuous(expand = expansion(0), # trick to remove space below 0 on y axis\n                       limits = c(0, 90),\n                       breaks = seq(0, 90, 5)) + # set limit of Y axis to 7\n    theme(panel.background = element_blank(), # removing the background panel colour\n          axis.line = element_line(color = \"black\"), # creating black axes lines\n          axis.ticks.x = element_blank(),\n          legend.position = \"none\") + # removing the legend, which is redundant\n    geom_errorbar(aes(ymin=Score-se, ymax=Score+se, width = 0.2))\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n5. Again, try to think which groups may significantly differ from one another by eyeballing the means and barchart information. We will confirm this all next week.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#STUDENT COMPLETES\n```\n:::\n\n\n6. Before you finish, make sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\n\nPlease end your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\n\n7. ...................... Now breathe! You've rocked it!!!\n\n![Top work!](https://images.pexels.com/photos/92870/pexels-photo-92870.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\" width=\"420\" height=\"250)\n\n# Lab Feedback (voluntary)\n\nThis is a voluntary, super fast [survey](https://modules.lancaster.ac.uk/mod/url/view.php?id=1905313), just to gauge how you found the difficulty of the content and any additional feedback.",
    "supporting": [
      "Week2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}