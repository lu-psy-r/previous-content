{
  "hash": "0c224089ac55d1701251a348e28e8e6d",
  "result": {
    "markdown": "---\ntitle: 2. Multiple regression including categorical predictors\nsubtitle: Emma Mills\norder: 3\n---\n\n\n\n\n\n# Lecture\n\n[Lecture Link](https://modules.lancaster.ac.uk/mod/url/view.php?id=2042339)\n\nThe lecture materials, including and and R files are downloaded from [here](data/Wk2/Week2_lecture_materials.zip) as a zip file. You can upload the zip file directly on to the R server and it will populate a new folder with the files and data files automatically.\n\n## Part 1: Multiple Regression: 2 or more predictors\n\n### Continuous predictors:\n\n-   $Y = Intercept + predictor_{continuous} + predictor_{categorical} + Error$\n-   $Y = b_0 + b_1X_1 + b_2X_2 +ùúÄ$\n\nInterpretation of Intercept term ($b_0$): Average when all continuous predictors are at 0 and categorical predictors are at their reference level\n\nInterpretation for continuous coefficients: + A one unit increase in $X_1$ gives a change in Y by the amount of $b_1$ + A one unit increase in $X_2$ gives a change in Y by the amount of $b_2$\n\n### Continuous OR categorical predictors\n\n$Y = Intercept + predictor_1 + predictor_2 + Error$ $Y = b0 + b_1X_1 + b_2X_2 +ùúÄ$\n\nInterpretation for continuous coefficients: A one unit increase in $X_1$ gives a change in Y by the amount of $b_1$\n\nInterpretation for categorical coefficients: + A change to another category within $X_2$ gives a change in Y by the amount of $b_2$\n\n### Binary categorical variables\n\nThese are predictors with **only** two levels\n\n| Predictor | Level 1 | Level 2   |\n|-----------|---------|-----------|\n| Accuracy  | 0       | 1         |\n| Accuracy  | No      | Yes       |\n| Smoker    | No      | Yes       |\n| Group     | Control | Treatment |\n|           |         |           |\n\n: Examples of binary variables\n\n\nRegression models cope with the categorical nature by assigning numbers and creating 'contrasts'.\n+ Default contrast scheme in R for binary predictors:\n+ 0 to a level 1\n+ 1 to level 2\n+ this is known as 'Dummy coding' or 'treatment coding', and it automatically creates a 'reference level' = 0 which is the level that comes first in the alphabet/numerically. The reference level group is estimated in the Intercept coefficient.\n\nYou can allow R to automatically set your reference, which is fine for a balanced variable (with equal numbers of participants/stimuli in each level of the variable), but setting a *different* reference level may make your hypothesis easier to interpret\n  + This may be useful in the context of factors with more than two levels\n  + You can change the reference level by using the relevel() function to manually reorder the levels of your grouping variable\n  \n\n\n\n\n## Demonstration: Multiple regression with categorical predictors\n\nIn the demonstration sections, you can follow along using the `Multiple_Regression_Categorical_Predictors.Rmd` file.\n\n - dummy / treatment coding\n - sum coding\n   - changing the reference level\n - centring and standardising\n\nMake sure to load in the following libraries to follow along\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(effects)\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(PerformanceAnalytics)\n```\n:::\n\n\n#### Model for multiple regression\n\nKeep the model for multiple regression in mind:\n\nYou have seen the two predictor regression model equation before:\n\n$$\nY_i = b_0 + b_1 * X_1 + b_2*X_2 + e_i\n$$\n  -  $X_1$ and $X_2$ can be categorical predictors here - there is no different notation between continuous and categorical predictors.\n\n### Import the data\n\nThe data set is retrieved from https://www.kaggle.com/mirichoi0218/insurance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- read_csv(\"data/Wk2/_Week2_lecture_materials/insurance.csv\")\n```\n:::\n\n\nLooking at the information on reading the data in,  we have three variables that R has detected as character variables: sex, smoker and region.  Brilliant - three categorical variables.\n\nWe also have three variables that are continuous: age, bmi, children\n\n### Tidy\n\nPurely for cosmetic reasons, I am going to change the region variable to acronyms of the geographic regions.  This is for labeling of plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use nested ifelse() statements\nd$region <- ifelse(d$region == \"northeast\", \"NE\",\n                   ifelse(d$region == \"northwest\", \"NW\",\n                          ifelse(d$region == \"southeast\", \"SE\", \"SW\"))) \n```\n:::\n\n\nVery quickly, from the title of the dataset, it looks like several variables are being used as predictors for insurance `charges`.  You can read a little more at the website above if you like.\n\n### Visualise\n\nLets visualise the structure of the variables of the data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# bar charts for categorical variables\np_sex <- ggplot(d, aes(x = factor(sex), fill = factor(sex))) + geom_bar()\np_smok <- ggplot(d, aes(x = factor(smoker), fill = factor(smoker))) + geom_bar()\np_reg <- ggplot(d, aes(x = factor(region), fill = factor(region))) + geom_bar()\n\n# (so called) continuous variables\np_age <- ggplot(d, aes(x = age)) + geom_density(fill = \"slateblue\")\np_age_hist <- ggplot(d, aes(x = age)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi <- ggplot(d, aes(bmi)) + geom_density(fill = \"red\")\np_child <- ggplot(d, aes(children)) + geom_histogram()\ngrid.arrange(p_sex, p_smok, p_reg, p_age_hist, p_bmi, p_child)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe can also look at the relationships between each of the variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchart.Correlation(d[, c(1, 3, 4)], histogram=TRUE, pch=19)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Model\n\nwe'll just add them all at this point:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m6 <- lm(charges ~ age + bmi + children + sex + smoker + region, d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = charges ~ age + bmi + children + sex + smoker + \n    region, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11938.5      987.8 -12.086  < 2e-16 ***\nage            256.9       11.9  21.587  < 2e-16 ***\nbmi            339.2       28.6  11.860  < 2e-16 ***\nchildren       475.5      137.8   3.451 0.000577 ***\nsexmale       -131.3      332.9  -0.394 0.693348    \nsmokeryes    23848.5      413.1  57.723  < 2e-16 ***\nregionNW      -353.0      476.3  -0.741 0.458769    \nregionSE     -1035.0      478.7  -2.162 0.030782 *  \nregionSW      -960.0      477.9  -2.009 0.044765 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,\tAdjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nLets have a quick look at the output and the coefficient labels column.  All of the categorical variables are toward the bottom of the list because we entered them toward the end of the list in the model.\n\n  - the label for `sex` is `sexmale`, which tells us that the reference level for the sex variable is female - makes sense since the default is for the first group in the alphabet to be taken as the reference level group.\n  - `smoker` becomes `smokeryes` to reflect that being a non-smoker is the reference level group.\n  \nand so on...\n\n## Part 2: Reference levels in the regression model\n\nReference level in dummy coding becomes hidden in the intercept for the categorical predictor. It‚Äôs a good idea to write out what your intercept represents before you start to interpret your model. If dummy coding is used, switching from one level of the categorical predictor to the second level is the same as moving along one unit of a continuous predictor. It follows that for a binary categorical predictor, the slope term is giving you the mean difference between the two groups. If sum coding is used, switching from one level of the categorical predictor to the second level is the same as moving along two units of a continuous predictor\n\nMuch better choice if you are including interactions in your regression model. Instead of 0 and 1, -1 and +1 are used. Intercept is at 0 ‚Äì \n+ Like continuous predictors\n+ Categorical predictor is now 'centred' (explained later)\n+ This type of contrast can help for interpreting interactions that occur between categorical and continuous predictors\n\n\n## Demonstration: Types of categorical contrasts - transformations\n\n#### Dummy or Treatment Coding Scheme\n\nlets check how the model sees the categorical variables:  \n\nat the moment, R sees the variables as character variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nspc_tbl_ [1,338 √ó 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ age     : num [1:1338] 19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr [1:1338] \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num [1:1338] 27.9 33.8 33 22.7 28.9 ...\n $ children: num [1:1338] 0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr [1:1338] \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr [1:1338] \"SW\" \"SE\" \"SE\" \"NW\" ...\n $ charges : num [1:1338] 16885 1726 4449 21984 3867 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   age = col_double(),\n  ..   sex = col_character(),\n  ..   bmi = col_double(),\n  ..   children = col_double(),\n  ..   smoker = col_character(),\n  ..   region = col_character(),\n  ..   charges = col_double()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n```\n:::\n:::\n\n\nTo check the type of contrast coding we need convert the variables to factor class:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% \n  mutate(sex = factor(sex),\n         smoker = factor(smoker),\n         region = factor(region))\n```\n:::\n\n\ncheck that the class has been changed through calling str() once more\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [1,338 √ó 7] (S3: tbl_df/tbl/data.frame)\n $ age     : num [1:1338] 19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 1 1 1 2 1 ...\n $ bmi     : num [1:1338] 27.9 33.8 33 22.7 28.9 ...\n $ children: num [1:1338] 0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 1 1 1 1 1 1 1 ...\n $ region  : Factor w/ 4 levels \"NE\",\"NW\",\"SE\",..: 4 3 3 2 2 3 3 2 1 2 ...\n $ charges : num [1:1338] 16885 1726 4449 21984 3867 ...\n```\n:::\n:::\n\n\nNow we use the `contrasts()` function to see how R views them:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(d$sex) # reference level = female\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       male\nfemale    0\nmale      1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(d$smoker) # reference level = non-smoker\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    yes\nno    0\nyes   1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(d$region) # reference level = NE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   NW SE SW\nNE  0  0  0\nNW  1  0  0\nSE  0  1  0\nSW  0  0  1\n```\n:::\n:::\n\n\nYou can see from the rows that contain the zeroes (female, nonsmoker and northeast) that these reflect the reference levels automatically selected in the first full regression model summary print out above.\n\nAs a quick demonstration, you can control the reference level in the dummy coding command:  Using the `contr.treatment()` function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n(contrasts(d$region) = contr.treatment(4, base = 3)) # change ref level to southeast (row 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  1 2 4\n1 1 0 0\n2 0 1 0\n3 0 0 0\n4 0 0 1\n```\n:::\n:::\n\n\nLook at row three in the output - the row of zeros indicates that this is now the reference level. I'll change it back before moving forward.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(contrasts(d$region) = contr.treatment(4, base = 1)) # change ref level back to northeast (row 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  2 3 4\n1 0 0 0\n2 1 0 0\n3 0 1 0\n4 0 0 1\n```\n:::\n:::\n\n\n\nHas our model explained all the structured variance? Lets look at diagnostic plots before we go any further:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) # display plots in a 2 x 2 panel\nplot(m6)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\nSo,  we know little about these variables, and we haven't even looked at the individual coefficients in the summary outputs but we can see that the percentage of variance explained in the `charges` variable is greater in the 6 predictor model (75%) over the 3 predictor model (12%) but it is clear from the  diagnostic plot above that there is still something not explained in the structure of the residuals.\n\n### Sum Coding Schemes\n\nSo, lets think about interactions - introduced in the context of ANOVA last term and thinking about many of the presentations in the 204 module at the end of last term, they are germain to the structure of experimental design whenever groups are included.\n\nWhat interactions could be plausible here? Just using what we know about medical problems and charges and the variables we have - could there be an interaction between bmi and smoker status?  Do lifestyle choices cluster together to effect medical insurance charges?  Does the number of children covered by insurance vary systematically by region?\n\nPlausible interactions from the perspective of a layman (me) and I am not going to model them here.  But it may inform the choice of contrasting scheme.  If we want to interpret interactions between variables later on - either a mix of continuous and categorical or both categorical predictors, it makes sense to use sum coding.  Even though we have one categorical variable with four levels, we can still do this.\n\nIt's a good idea to make copies of variables so that your original variables remain intact.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% \n  mutate(sexsum = sex,\n         smokersum = smoker,\n         regionsum = region) # copy categorical variables to the same dataset with a suffix to denote sum coding status\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncontrasts(d$sexsum) # dummy coded right now\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       male\nfemale    0\nmale      1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(contrasts(d$sexsum) <- contr.sum(2)) # levels are now 1 and -1 and variable is now centred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [,1]\n1    1\n2   -1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# repeat for the other two variables\n(contrasts(d$smokersum) <- contr.sum(2)) # levels are now 1 and -1 and variable is now centred\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [,1]\n1    1\n2   -1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(contrasts(d$regionsum) <- contr.sum(4)) # levels are now 1 and -1 but variable is not centred (because this isn't a binary variable)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [,1] [,2] [,3]\n1    1    0    0\n2    0    1    0\n3    0    0    1\n4   -1   -1   -1\n```\n:::\n:::\n\n\nLets refit the regression model using this contrast scheme:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m6_sum <- lm(charges ~ age + bmi + children + sexsum + smokersum + regionsum, d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = charges ~ age + bmi + children + sexsum + smokersum + \n    regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -666.94     968.64  -0.689 0.491239    \nage            256.86      11.90  21.587  < 2e-16 ***\nbmi            339.19      28.60  11.860  < 2e-16 ***\nchildren       475.50     137.80   3.451 0.000577 ***\nsexsum1         65.66     166.47   0.394 0.693348    \nsmokersum1  -11924.27     206.58 -57.723  < 2e-16 ***\nregionsum1     587.01     293.11   2.003 0.045411 *  \nregionsum2     234.05     292.92   0.799 0.424427    \nregionsum3    -448.01     291.24  -1.538 0.124219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,\tAdjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nThe intercept is massively reduced because: \n\n - the sum coding scheme has swapped the reference levels of the categorical predictors - For the `sex` and `smoker` coefficients, they now represent level 1s `female` and `non-smokers`.    \n - the binary categorical variables are now centred. You can see that the estimates are halved from the `m6` model that uses dummy / treatment contrast coding. \n - The binary coefficient estimates shows how much a change in Y for level 1 of the coefficient. A minus sign means it is smaller than the intercept term.  No minus sign (a positive value) means it is more than the intercept term.\n - To find how much of a change for level -1 of the coefficient, you simply change the sign of the coefficient.\n \n\n\nLets do the math:\n\n$$\nY_i = b_0 + b_1 * X_1 + b_2*X_2 + e_i\n$$\nWe are just going to do the maths for the `sex` variable for each level:  `female` and `male`.  The `sex` variable is our fourth predictor, so our maths equation looks like this:\n\n$$\nY_i = b_0 + b_4 * X_4\n$$\n\nWe need the:  \n\n - `intercept` term = $b_0$ =  (-666.94)\n - the coefficient value for `sexsum1` (65.66) from the model output; this is the fourth predictor, so if we had written out our equation for all the predictors, it would be $b_4$ (as above)\n - we also need the labels for the two levels of the sex predictor:  female (1) and male (-1) because we are going to put those in our $X_4$ part of the equation.\n \nHere we go:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(InsuranceCharge_Female <- -666.94 + (65.66 * 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -601.28\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(InsuranceCharge_Male <- -666.94 + (65.66 * -1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -732.6\n```\n:::\n:::\n\nLets do the math for the `smoker` variable:\n\n The `smoker` variable is our fifth predictor, so our maths equation looks like this:\n\n$$\nY_i = b_0 + b_5 * X_5\n$$\n\nWe need the:  \n\n - `intercept` term = $b_0$ =  (-666.94)\n - the coefficient value for `smokersum1` (-11924.27) from the model output; this is the fifth predictor, so if we had written out our equation for all the predictors, it would be $b_5$ (as above)\n - we also need the labels for the two levels of the smoker variable:  no (1) and yes (-1) because we are going to put those in our $X_5$ part of the equation.\n \nHere we go:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(InsuranceCharge_nonsmoker <- -666.94 + (-11924.27 * 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -12591.21\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(InsuranceCharge_smoker <- -666.94 + (-11924.27 * -1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11257.33\n```\n:::\n:::\n\n\n \n - You can compare the continuous predictor coefficients and see no difference for their coefficients from model m6.\n\nThe `region` variable is a little harder to interpret.  Remember that the reference level is now the southwest level of the variable and each region coefficient is the difference from the intercept. So if we want to find the value for southwest, we have to take away region 1 2 and 3 from the intercept term.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(InsuranceCharge_SW <- -666.94 + (-587.01 * 1) + (234.05 * 1) + (-448.01 *1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -1467.91\n```\n:::\n:::\n\n\n\n## Mean Centering Continuous Predictors\n\nIf you have unequal numbers of participants in your groups:\n+ It may be better to transform your categorical variable to a numerical variable (sometimes called an indicator variable)¬†\n+ and mean centre the new variable as if it was a continuous variable\n  + Use ifelse() function `ifelse(d$sex == \"female\", 1,0)`\n  + Use x_mean <- x‚Äì mean(x) on the variable to mean centre `d$sex_n - mean(d$sex_n)`\n\nRemember that the raw intercept term represents the average value when all predictors are at zero, so the coefficients are interpreted as the change in Y for some value (larger than 0) the X predictor\n\nAn Intercept like this often doesn't make sense for the verbal model,  i.e. the research questions or our hypotheses. What does an intercept term of 0 years or 0 kgs mean?\n\nIf we mean-centre the continuous predictors, the Intercept now represents the average value when all continuous predictors are at their average value\n\nSubtract the mean value of  a variable from every observation in that variable\n+ The mean of the variable is then = 0\n+ All values are 'centred' around zero\n+ Some will be below zero, some will be above\n+ The intercept term in your model now represents the intercept at the average for the predictor\n+ The predictors are still in their raw units (e.g. years/kgs)\n+ (learn to do these techniques by hard coding¬†‚Äì easy to do and the common function that is used, `scale()`, doesn‚Äôt always play very nicely with some other packages!)\n\n\n## Demonstration: Unbalanced variables - transforming binary categorical to numerical variables and centering them\n\nLook back at the visualisations of the dataset above.  Female and male participants in the sex variable are quite evenly spread. Smoker status, however, is very unbalanced.  Gelman (2007) recommends creating a new numeric variable in this case and then mean centring a binary variable in this case.  If you use this method,  do this across each binary categorical variable for consistency.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# bar charts for categorical variables\np_sex <- ggplot(d, aes(x = factor(sex), fill = factor(sex))) + geom_bar()\np_smok <- ggplot(d, aes(x = factor(smoker), fill = factor(smoker))) + geom_bar()\np_reg <- ggplot(d, aes(x = factor(region), fill = factor(region))) + geom_bar()\n\n# (so called) continuous variables\np_age <- ggplot(d, aes(x = age)) + geom_density(fill = \"slateblue\")\np_age_hist <- ggplot(d, aes(x = age)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi <- ggplot(d, aes(bmi)) + geom_density(fill = \"red\")\np_child <- ggplot(d, aes(children)) + geom_histogram()\ngrid.arrange(p_sex, p_smok, p_reg, p_age_hist, p_bmi, p_child)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd$sex_n <- ifelse(d$sex == \"female\", 1, 0) # if sex = female recode as 1 otherwise as 0 in a new numeric variable\n```\n:::\n\n\nHave a look at what the summary of the new variable shows - should be quite symmetric:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(d$sex_n) # balanced variable - look at the mean value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.4948  1.0000  1.0000 \n```\n:::\n:::\n\n\nso if we transform it to be centred now, by subtracting the mean from each observation rather than leaving it to R:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd$sex_c <- signif(d$sex_n - mean(d$sex_n), 3) # round to 3 significant figures for readability\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(d$sex_c) #\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.4950000 -0.4950000 -0.4950000 -0.0002317  0.5050000  0.5050000 \n```\n:::\n:::\n\n\nhave a look at the first few values of the variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(d$sex_c, 8) # female was 1 so is the positive numbers; male was zero so is the negative numbers here.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  0.505 -0.495 -0.495 -0.495 -0.495  0.505  0.505  0.505\n```\n:::\n:::\n\n\nIf we do the same for the `smoker` variable - we saw in the bar chart that this was a very unbalanced variable, however here are the actual numbers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(factor(d$smoker))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n  no  yes \n1064  274 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd$smoker_n <- ifelse(d$smoker == \"yes\", 1, 0) # if smoker = yes recode as 1 otherwise as 0 in a new variable\n\nsummary(d$smoker_n) # value is not 0.5 which is would be if the variable was balanced in observations\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.2048  0.0000  1.0000 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd$smoker_c <- signif(d$smoker_n - mean(d$smoker_n), 3) # round to 3 significant figures for readability\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(d$smoker_c) # yes was coded as 1 so the positive values here are smokers and the negative values are non-smokers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.2050000 -0.2050000 -0.2050000 -0.0002167 -0.2050000  0.7950000 \n```\n:::\n:::\n\n\nhave a look at the first few values of the variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(d$smoker_c, 8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  0.795 -0.205 -0.205 -0.205 -0.205 -0.205 -0.205 -0.205\n```\n:::\n:::\n\n\nLet's refit the model with these variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m6_cen <- lm(charges ~ age + bmi + children + sex_c + smoker_c + regionsum, d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = charges ~ age + bmi + children + sex_c + smoker_c + \n    regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7702.9      963.6  -7.994 2.82e-15 ***\nage            256.9       11.9  21.587  < 2e-16 ***\nbmi            339.2       28.6  11.860  < 2e-16 ***\nchildren       475.5      137.8   3.451 0.000577 ***\nsex_c          131.3      332.9   0.394 0.693348    \nsmoker_c     23848.5      413.1  57.723  < 2e-16 ***\nregionsum1     587.0      293.1   2.003 0.045411 *  \nregionsum2     234.1      292.9   0.799 0.424427    \nregionsum3    -448.0      291.2  -1.538 0.124219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,\tAdjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nSo - we have changes for the categorical variables - they are now \"centred\" - either automatically by R or we have hand coded, transforming our categorical variables into numeric variables.\n\nFor consistency - it would make sense to centre the continuous variables also. While you are learning, it's a good idea to do it longhand:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create new variables to keep the original variables intact:\n\nd <- d %>% \n  mutate(age_c = age - mean(age), # new variable = old variable minus the mean of the old variable\n         bmi_c = bmi - mean(bmi),\n         child_c = children - mean(children)\n         )\n```\n:::\n\n\nYou can check: redraw the plots to check the axes - Note now that the axes are centred around zero for the continuous predictors. Note also how no information has changed within and between the variables - all relative relationships remain the same.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# bar charts for categorical variables\np_sex_c <- ggplot(d, aes(x = factor(sex_c), fill = factor(sex_c))) + geom_bar()\np_smok_c <- ggplot(d, aes(x = factor(smoker_c), fill = factor(smoker_c))) + geom_bar()\np_regsum <- ggplot(d, aes(x = factor(regionsum), fill = factor(regionsum))) + geom_bar()\n\n# frequency polygons for (so called) continuous variables\np_age_c <- ggplot(d, aes(x = age_c)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi_c <- ggplot(d, aes(bmi_c)) + geom_density(fill = \"red\")\np_child_c <- ggplot(d, aes(child_c)) + geom_histogram()\ngrid.arrange(p_sex_c, p_smok_c, p_regsum, p_age_c, p_bmi_c, p_child_c)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\nLets plug the centred continuous variables into the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m6_cen_all <- lm(charges ~ age_c + bmi_c + child_c + sex_c + smoker_c + regionsum, d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = charges ~ age_c + bmi_c + child_c + sex_c + smoker_c + \n    regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  13289.1      165.9  80.079  < 2e-16 ***\nage_c          256.9       11.9  21.587  < 2e-16 ***\nbmi_c          339.2       28.6  11.860  < 2e-16 ***\nchild_c        475.5      137.8   3.451 0.000577 ***\nsex_c          131.3      332.9   0.394 0.693348    \nsmoker_c     23848.5      413.1  57.723  < 2e-16 ***\nregionsum1     587.0      293.1   2.003 0.045411 *  \nregionsum2     234.1      292.9   0.799 0.424427    \nregionsum3    -448.0      291.2  -1.538 0.124219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,\tAdjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nYou'll notice that the only change is on the intercept value - because it is now at the average for each predictor, rather than when each predictor is 0...and it makes more sense, right - the intercept is no longer a negative unit value. Although it would be nice to think that people could pay negative insurance charges - what does that mean?  That everyone is in debt? That the health treatment costs less than 0?\n\nCreate some more variables to store the standardised variables, which we will create next.\n\nRemember to create a standardised variable, we first centre the variable and then divide it by its standard deviation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create new variables to store the standardised values\n# these are standardised by dividing by 1 x sd:\n\nd <- d %>% \n  mutate(age_z1 = (age - mean(age)) / sd(age), # new variable = centred variable divided by the standard deviation of the centred variable\n         bmi_z1 = (bmi - mean(bmi)) / sd(bmi),\n         child_z1 = (children - mean(children)) / sd(children)\n         )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# bar charts for categorical variables\np_sex_c <- ggplot(d, aes(x = factor(sex_c), fill = factor(sex_c))) + geom_bar()\np_smok_c <- ggplot(d, aes(x = factor(smoker_c), fill = factor(smoker_c))) + geom_bar()\np_regsum <- ggplot(d, aes(x = factor(regionsum), fill = factor(regionsum))) + geom_bar()\n\n# frequency polygons for (so called) continuous variables\np_age_z1 <- ggplot(d, aes(x = age_z1)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi_z1 <- ggplot(d, aes(bmi_z1)) + geom_density(fill = \"red\")\np_child_z1 <- ggplot(d, aes(child_z1)) + geom_histogram()\ngrid.arrange(p_sex_c, p_smok_c, p_regsum, p_age_z1, p_bmi_z1, p_child_z1)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\nOnly the axes have changed once more....the centre is still zero but the units are now in standard deviations.\n\nWhat happens when we use the standardised variables with the centred binary variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m6_z <- lm(charges ~ age_z1 + bmi_z1 + child_z1 + sex_c + smoker_c + regionsum, d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = charges ~ age_z1 + bmi_z1 + child_z1 + sex_c + smoker_c + \n    regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  13289.1      165.9  80.079  < 2e-16 ***\nage_z1        3608.8      167.2  21.587  < 2e-16 ***\nbmi_z1        2068.5      174.4  11.860  < 2e-16 ***\nchild_z1       573.2      166.1   3.451 0.000577 ***\nsex_c          131.3      332.9   0.394 0.693348    \nsmoker_c     23848.5      413.2  57.723  < 2e-16 ***\nregionsum1     587.0      293.1   2.003 0.045411 *  \nregionsum2     234.0      292.9   0.799 0.424427    \nregionsum3    -448.0      291.2  -1.538 0.124219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,\tAdjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nNo change in the original correlations either:\n\nThis line of code charts correlations between columns 18 - 20 of the dataset `d` - i.e. the standardised varaibles we just made.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchart.Correlation(d[, c(18:20)], histogram=TRUE, pch=19)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\n## Transformation via Standardising\n\nTake your centred variable and divide each observation by one standard deviation of the variable. Interpretation is now a one standard deviation decrease / increase in the predictor is a (part of) standard deviation in the outcome variable. Because all predictors are now measured in standard deviation units, instead of their raw units, you can compare the size of their coefficients. You couldn‚Äôt do this before because they were in different units.\n\nDividing by 2 standard deviations allows you to directly compare the coefficients for continuous and binary predictors.\n\n\n## Demonstration: Standardising by Two Standard Deviations\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create new variables to store the standardised values\n# these are standardised by dividing by 1 x sd:\n\nd <- d %>% \n  mutate(age_z2 = (age - mean(age)) / (2*sd(age)), # new variable = centred variable divided by the standard deviation of the centred variable\n         bmi_z2 = (bmi - mean(bmi)) / (2*sd(bmi)),\n         child_z2 = (children - mean(children)) / (2*sd(children))\n         )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# bar charts for categorical variables\np_sex_c <- ggplot(d, aes(x = factor(sex_c), fill = factor(sex_c))) + geom_bar()\np_smok_c <- ggplot(d, aes(x = factor(smoker_c), fill = factor(smoker_c))) + geom_bar()\np_regsum <- ggplot(d, aes(x = factor(regionsum), fill = factor(regionsum))) + geom_bar()\n\n# frequency polygons for (so called) continuous variables\np_age_z2 <- ggplot(d, aes(x = age_z2)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi_z2 <- ggplot(d, aes(bmi_z2)) + geom_density(fill = \"red\")\np_child_z2 <- ggplot(d, aes(child_z2)) + geom_histogram()\ngrid.arrange(p_sex_c, p_smok_c, p_regsum, p_age_z2, p_bmi_z2, p_child_z2)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\nOnly the axes have changed once more....the centre is still zero, the units are now in standard deviations.  The correlations remain the same also.  I leave it to the interested reader to cut and paste the code from above and visualise columns 21 - 23 to check this for themselves.\n\n\n#### Ordered variables\n\nWe're not quite finished on our contrast journey...\n\nConsider the `children` variable - so far I have treated it as a continuous variable but it's definitely ordered: look at the histogram above again.  If we factorise the variable, we will better see the categories\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(d$children) # summary as a continuous predictor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   1.000   1.095   2.000   5.000 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(d$children_f <- factor(d$children)) # wrap the factorising command within a summary command for speed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  0   1   2   3   4   5 \n574 324 240 157  25  18 \n```\n:::\n:::\n\n\nand we can be pretty confident that the step from 0 - 1 and 3 - 4 is the same size unit of an increase in one person so `children` is the kind of variable that can be treated as continuous - it has a true zero, but it also is discrete....groups representing counts of different numbers of children...so what happens if we treat this as an ordered variable and use the `helmert` variant of contrast coding:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd$children_h <- factor(d$children)\n(contrasts(d$children_h) <- contr.helmert(6))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [,1] [,2] [,3] [,4] [,5]\n1   -1   -1   -1   -1   -1\n2    1   -1   -1   -1   -1\n3    0    2   -1   -1   -1\n4    0    0    3   -1   -1\n5    0    0    0    4   -1\n6    0    0    0    0    5\n```\n:::\n\n```{.r .cell-code}\n# levels(d$children_h) # checking the order of levels in the children_h variable\n```\n:::\n\nThis contrast coding tell us that the first regression coefficient (look at column [ ,1]) will be the comparison between the first two categories of the `children` variable - 0 children and one-child families.  The second regression coefficient (look at column [ ,2]) for the variable will show a comparison between families with two children and families with either 0 or one child and so on.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m6_zh <- lm(charges ~ age_z1 + bmi_z1 + children_h + sex_c + smoker_c + regionsum, d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = charges ~ age_z1 + bmi_z1 + children_h + sex_c + \n    smoker_c + regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11689.4  -2902.6   -943.7   1492.2  30042.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13893.35     337.07  41.218  < 2e-16 ***\nage_z1       3613.56     167.40  21.587  < 2e-16 ***\nbmi_z1       2054.53     174.48  11.775  < 2e-16 ***\nchildren_h1   195.49     210.68   0.928  0.35362    \nchildren_h2   480.10     148.35   3.236  0.00124 ** \nchildren_h3    72.19     130.47   0.553  0.58017    \nchildren_h4   439.92     245.53   1.792  0.07341 .  \nchildren_h5   -11.94     243.80  -0.049  0.96094    \nsex_c         128.16     332.83   0.385  0.70025    \nsmoker_c    23836.41     414.14  57.557  < 2e-16 ***\nregionsum1    591.52     293.16   2.018  0.04382 *  \nregionsum2    211.47     293.68   0.720  0.47160    \nregionsum3   -441.62     291.54  -1.515  0.13006    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6059 on 1325 degrees of freedom\nMultiple R-squared:  0.7519,\tAdjusted R-squared:  0.7497 \nF-statistic: 334.7 on 12 and 1325 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\nThis `children` predictor now has an independent coefficient for each level of the categorical variable.  Previously, we had entered the `children` predictor as continuous, and we were getting an even rate of change, which registered as a significant impact on insurance charges.\n\nNow that we have transformed the variable - and respected the structure of the variable data (it was never continuous), out interpretation of the model needs to change somewhat.  The rate of change in insurance chargest across the increase in children is not even (480.10 is not the same as 72.19).  This is not a linear relationship. The only significantly different change in insurance charges is the change from 0 or 1 child to two child families.  Can you think of a reason why a familiy of two children is predicted to pay significantly higher insurance charges than a family of one child? \n\nThere is a little more detail in this model than the previous ones - whether it is useful would depend upon the research question, or a better model than the previous ones, we could deduce using formal model comparison techniques - more of that later.\n\nLets check the residuals once more:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) # display plots in a 2 x 2 panel\nplot(m6_zh)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\nThat structure is still there!\n\n#### Interpretation of the model\n\nLets reprint the summary to save scrolling but also save the summary to an object so that we can call the values while interpreting and reporting the model\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(m6_zh_summary <- summary(m6_zh))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = charges ~ age_z1 + bmi_z1 + children_h + sex_c + \n    smoker_c + regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11689.4  -2902.6   -943.7   1492.2  30042.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13893.35     337.07  41.218  < 2e-16 ***\nage_z1       3613.56     167.40  21.587  < 2e-16 ***\nbmi_z1       2054.53     174.48  11.775  < 2e-16 ***\nchildren_h1   195.49     210.68   0.928  0.35362    \nchildren_h2   480.10     148.35   3.236  0.00124 ** \nchildren_h3    72.19     130.47   0.553  0.58017    \nchildren_h4   439.92     245.53   1.792  0.07341 .  \nchildren_h5   -11.94     243.80  -0.049  0.96094    \nsex_c         128.16     332.83   0.385  0.70025    \nsmoker_c    23836.41     414.14  57.557  < 2e-16 ***\nregionsum1    591.52     293.16   2.018  0.04382 *  \nregionsum2    211.47     293.68   0.720  0.47160    \nregionsum3   -441.62     291.54  -1.515  0.13006    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6059 on 1325 degrees of freedom\nMultiple R-squared:  0.7519,\tAdjusted R-squared:  0.7497 \nF-statistic: 334.7 on 12 and 1325 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n#### A possible interpretation and reporting: \n\nWe have modelled a dataset that has several variables that may be informative in the prediction of insurance charges.  With no guiding research questions or hypotheses, we have looked at the structure of the variables and entered them all into a multiple regression model.  This is a data-driven, exploratory analysis.\n\nContinuous variables have been standardised; gender and smoker status have been coerced to be numerical variables and mean centred.  The six levels of the children variable have been given a helmert type contrast to provide detail of which numbers of children are influential.\n\nWithout engaging in any sensitivity analysis or outlier analysis, we report a model with a full set of predictors.  The model is significant ($F{_(}{_12,_1325}{_)}$ = 334.7, *p* < 0.001). This set of predictors explains 75.19% of the variance in insurance charges.\n\nThe following details the significant predictors in this model.  Each value represents a change in the predictor variable of one standard deviation and its impact upon the outcome variable while holding all other variables constant.  Because we have standardised the continuous variables this means they are held constant at their average level. The intercept therefore represents non smoking male individuals, with no children, that live in the SW region of the US.\n\nAge shows a positive association with insurance charges, with an increase in approximately \\$3613.56 for a one standard deviation unit change on the age scale (age: *t* = 21.59, *p* < .001). A person's body mass index (BMI) also shows a positive relationship and increases insurance charges by approximately \\$2054 with a one standard deviation change in BMI score (*t* = 11.77, *p* < .001). The number of dependents included in insurance cover was only significant on insurance charges at the comparison between 0 and 1 child to the two child level of coverage. Having two children increased insurance charges by approximately \\$480 (*t* = 3.24, *p* < .005), compared to the mean levels for 0 and 1 child. Being a woman is positively associated with an increase in insurance charges, raising insurance costs by approximately \\$128, however this increase is non-significant (*t* = 0.39, *p* = .700). Being a smoker is significantly and positively associated with an increase in insurance charges, raising insurance costs by approximately \\$23,836 (*t* = 57.56, *p* < .001). While living in the northeast region of the country also shows a positive association with insurance charges compared to living in the southwest region (reference level), showing an increase of around \\$591 (*t* = 2.02, *p* = 0.044).\n\nWe should be cautious with these interpretations as the residuals show that there is unexplained variability and there are a number of observations that are indicated as having high leverage or high influence so further terms and sensitivity analyses are warranted for a fuller understanding.\n\n\n##### Plotting predictions\n\nPlotting an individual effect (or lack of it) is very simple if we are not too worried about being pretty.  Plus - because we haven't changed any of the essential information in any of the coding / centring schemes,  you can choose how to plot them, based upon the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(predictorEffect(\"age_z1\", m6_zh))\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(predictorEffect(\"bmi_z1\", m6_zh))\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-53-2.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(predictorEffect(\"children_h\", m6_zh))\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-53-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(predictorEffect(\"sex_c\", m6_zh))\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-53-4.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(predictorEffect(\"smoker_c\", m6_zh))\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-53-5.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(predictorEffect(\"regionsum\", m6_zh))\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-53-6.png){width=672}\n:::\n:::\n\n\n##### Checking that assumptions of linear regression are not violated\n\nWe can collect some diagnostic measures to help with assumption checking using the `augment()` function from the `broom` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(m6zh_metrics <- augment(m6_zh))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,338 √ó 13\n   charges age_z1 bmi_z1 childr‚Ä¶¬π  sex_c smoke‚Ä¶¬≤ regio‚Ä¶¬≥ .fitted  .resid    .hat\n     <dbl>  <dbl>  <dbl> <fct>     <dbl>   <dbl> <fct>     <dbl>   <dbl>   <dbl>\n 1  16885. -1.44  -0.453 0         0.505   0.795 SW       25243. -8358.  0.00967\n 2   1726. -1.51   0.509 1        -0.495  -0.205 SE        3309. -1584.  0.00772\n 3   4449. -0.798  0.383 3        -0.495  -0.205 SE        6195. -1746.  0.0105 \n 4  21984. -0.442 -1.31  0        -0.495  -0.205 NW        3702. 18283.  0.00616\n 5   3867. -0.513 -0.292 0        -0.495  -0.205 NW        5525. -1658.  0.00529\n 6   3757. -0.584 -0.807 0         0.505  -0.205 SE        3685.    71.9 0.00604\n 7   8241.  0.483  0.455 1         0.505  -0.205 SE       10528. -2287.  0.00614\n 8   7282. -0.157 -0.479 3         0.505  -0.205 NW        7519.  -238.  0.00947\n 9   6406. -0.157 -0.137 2        -0.495  -0.205 NE        9147. -2740.  0.00779\n10  28923.  1.48  -0.791 0         0.505  -0.205 NW       11830. 17093.  0.00705\n# ‚Ä¶ with 1,328 more rows, 3 more variables: .sigma <dbl>, .cooksd <dbl>,\n#   .std.resid <dbl>, and abbreviated variable names ¬π‚Äãchildren_h, ¬≤‚Äãsmoker_c,\n#   ¬≥‚Äãregionsum\n```\n:::\n:::\n\n\nColumns 1 - 7 are our variables. `fitted` and `residuals` are the predicted values of charges and the `error` values.  The final four columns we use below with some explanation.\n\n\nEvery regression model is built upon the following assumptions:\n\n  1. The relationship between $X$ and $Y$ is assumed to be linear (additive)\n  2. The residual errors are normally distributed\n  3. The residuals have constant variance (homoscedasticity)\n  4. The residuals are not correlated (assumption of independence)\n  \n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2)) # display plots in a 2 x 2 panel\nplot(m6_zh) # plot diagnostic plots for m6_zh\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n\n\n**Residuals vs Fitted**: \"fitted\" here means the predicted values. We want the pink line to align pretty closely with the horizontal dashed line. Comparing this plot with that from the simple regression, this plot looks better. Take note of observation 13 - that was also labelled in the simple regression plots.\n\n**Normal Q-Q**: If the residual points (open circles) follow the dashed line, you can assume the residuals are normally distributed\n\n**Scale-Location**: This is checking for constant variance in the residuals - not much here.  A good indication would be a horizontal pink line with equally spread points.  Our graph is not good.\n\n**Residuals vs Leverage** - are there any points that are having a large influence on the regression results.  They will be numbered and you can then inspect them in your data file. Observations that show standardised residuals (see the table above) above 3 would be problematic.  As would observations of a hat value above $2(p+1)/n$ where $p$ = is the number of predictors (but see below) and $n$ = is the number of observations\n\nA different way to observe points with high leverage and high influence:\n\nIn the previous scripts, we used a formula for calculating hat values and Cook's Distance values.  Now we are modelling both continuous and categorical predictors that can have reference levels or more than two levels etc etc - each of which have their own model coefficient, it gets a little confusing remembering what 'p' stands for.  So an easier way is to graph your outlier observations:\n\nGraphing Model Hat Values and Observations with High Leverage: This is a larger dataset, so lets get R doing the sorting for us to check if there are any hat values larger than the model threshold:\n\n1) calculating hat value by hand\n2) using the hat value and filter out any observations from the augmented dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- length(coefficients(m6_zh)) # number of parameters estimated by the model\nN <- nrow(d) # number of observations\n\n\n# 1) calulate the hat value\n(m6zh_hat <- (2*(p+1))/N)  # model hat value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02092676\n```\n:::\n\n```{.r .cell-code}\n# 2) filter out observations that are above the hat value in the augmented dataset\nm6zh_hatvalues <- m6zh_metrics %>% \n  filter(.hat > 0.02092676) \n```\n:::\n\n\n3) Plot them: hat.plot function taken from Kabacoff, (2022), R in Action.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhat.plot <- function(fit) {\n              p <- length(coefficients(fit))\n              n <- length(fitted(fit))\n              plot(hatvalues(fit), main=\"Index Plot of Hat Values\")\n              abline(h=c(2,3)*p/n, col=\"red\", lty=2)\n              identify(1:n, hatvalues(fit), names(hatvalues(fit)))\n            }\n\nhat.plot(m6_zh)\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ninteger(0)\n```\n:::\n:::\n\n\n\n43 of the datapoints above have hat values larger than the model hat threshold value.  \n\nChecking for observations that are influential follows a similar pattern:  observations that exceed the *Cook's distance* value = $4/(n-p-1)$ are likely to have high influence and the regression results may change if you exclude them.  In the presence of such observations that exceed Cook's distance, unless you know the observation are errors, you probably need to estimate the model without the observations and report both sets of results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(m6zh_Cooks <- 4/(N-p-1)) # model Cook's distance value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.003021148\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm6zh_Cooksvalues <- m6zh_metrics %>% \n  filter(.cooksd > 0.003021148) \n\n# make sure your dataset is in nrow() function\n# and your model is before $coefficients and in the plot line\n# here it is m6_zh\n# when you are ready to draw the plot\n# select all three lines at once and press control and enter\n# or rerun the chunk by pressing the green arrow in the top right hand corner.\ncutoff <- 4/(nrow(d)-length(m6_zh$coefficients)-1)\nplot(m6_zh, which=4, cook.levels=cutoff)\nabline(h=cutoff, lty=2, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](Week2_files/figure-html/unnamed-chunk-59-1.png){width=672}\n:::\n:::\n\nThere are 77 observations that appear to be influential within the dataset...So we may need to run a reduced dataset and compare the outputs\n\nBut the model building is not complete. It is likely that there are interactions that could be put into the model and age, as a variable, may be nonlinear - how to deal with both of these is for the next lecture.\n\nThere are lots of things to do as we run a multiple regression model - both before, while and after building the model, which will be the focus of the future lectures.  We need to look at:\n\n  - a model with some significant predictors! - done\n  - correlation matrices - done\n  - centering predictors - done\n  - standardising predictors - done\n  - models with categorical predictors - done & ongoing\n    - interpreting models with categorical predictors - done & ongoing\n  - models with interaction terms\n  - the properties of multicollinearity\n  - choosing between different models\n  - reporting models - ongoing\n\n## More than two levels and ordered variables?\n\nWhat about categorical variables with more than two levels?\n+ Dummy / treatment coding  - each level‚Äôs coefficient is the difference between the reference level and that level\n+ Sum coding ‚Äì each level‚Äôs coefficient is the difference of the level from the intercept\n+ In variables with more than one level, To find the value of  -1 , you have to calculate the sum of the intercept and all the other levels of the variable.\n\nWhat about ordered categorical variables?\n+ Ordered outcome variables use ordinal regression model\n+ Ordered predictor variables use ‚Äúhelmert‚Äù coding\n+ Each level of the coefficient is the difference between those level or levels below them\n+ To get the value of the nth level in an ordered variable, you add all the other coefficients that come before that level to the intercept.\n\n# Lab Task\n\nIn this lab, you will be working with the R script and data stored [here](data/Wk2/Week2_lab_materials.zip), and the accompanying codebook is accessed [here](data/Wk2/Data_Codebook.pdf).\n\nWork through the Week12_R_Script.Rmd file and enter in your own code to arrive at the following model summary and figures.\n\n![Model summary](Images/Wk2/Model Summary Output.png)\n\n![Visualise Predictors](Images/Wk2/Visualise_Predictors.png)\n\n![Visualise Correlations](Images/Wk2/Visualise_Bivariate_Correlations.png)\n\n![Predictor Absence](Images/Wk2/Predictor_plot_absences_z1.png)\n![Predictor Health](Images/Wk2/Predictor_Plot_health_z1.png)\n![Predictor School](Images/Wk2/Predictor_Plot_school_c.png)\n![Predictor Walc](Images/Wk2/Predictor_plot_walc_h.png)\n![Predictor Sex](Images/Wk2/PredictorPlot_sex_c.png)\n\n# Submit Scripts\n\nRemember to submit your group scripts if you want to receive and see feedback on your own and other groups' scripts.",
    "supporting": [
      "Week2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}