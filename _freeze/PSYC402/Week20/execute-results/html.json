{
  "hash": "75b2790a445ef01af64d991cd12acf21",
  "result": {
    "markdown": "---\ntitle: 10. Introduction to Ordinal Models\nsubtitle: Written by Rob Davies\norder: 11\nbibliography: references.bib\n---\n\n\n# Introduction to Ordinal Models {#sec-ordinal-intro}\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n## Motivations: working with ordinal outcomes {#sec-ordinal-motivations}\n\nOrdinal data are very common in psychological science. Often, we will encounter ordinal data recorded as responses to Likert-style items in which the participant is asked to indicate a response on an ordered scale ranging between two end points [@burkner2019; @liddell2018]. An example of a Likert question item might be: *How well do you think you have understood this text? (Please check one response)* where the participant must respond by checking an option, given 5 options ranging from 1 (not well at all) to 5 (very well). The critical characteristics of such responses are that:\n\n-   The responses are ordered, as indicated by the number labels;\n-   Response types are categorical or qualitative, not numeric.\n\nWe will be working with study data in which the outcome that is the target for our analyses comprise responses to questions designed to elicit ratings. Ordinal data may, however, also derive from situations in which ordered categorical responses do not derive from ratings items [we will look briefly at sequential responses, @burkner2019].\n\nThe challenge we face is that we will aim to develop skills in using *ordinal models* when, in contrast, most psychological research articles will report analyses of ordinal data using conventional methods like ANOVA or linear regression. We will work to understand why ordinal models are better. We will learn that applying conventional methods to ordinal data will, in principle, involve a poor account of the data and, in practice, will create the risk of producing misleading results. And we will learn how to work with and interpret the results from ordinal models with or without random effects.\n\nIn our work in this chapter, we will rely extensively on the ideas set out by @liddell2018, see @sec-ordinal-recommended-reading.\n\n## The key idea to get us started {#sec-ordinal-ideas}\n\n::: callout-important\nOrdinal responses are labelled with numbers but ordinal data are *not* numeric.\n:::\n\nOrdinal responses are coded with numeric labels. These number labels may indicate order but we do not know that the difference between e.g. response options `1` versus `2` is the same as the difference between `2` versus `3` or `3` versus `4`. Ordinal data contrast with *metric* data [@liddell2018] which are recorded on scales for which we assume *both* order and equal intervals. When researchers apply metric models to ordinal data, they incorrectly assume that the response options e.g. in ratings are separated by equal intervals. Yet, in a review of the 68 recently articles that mentioned the term \"Likert\" in a sample of highly ranked Psychology journals, @liddell2018 found that ordinal data were treated as metric and the articles presented results from metric models.\n\nOne way to think about ordinal data is that often (but not always) ratings may be understood to come from psychological processes in which the participant, in response to the Likert question, divides some latent (unobserved) psychological continuum or scale into categories in order to select a response option. \n\nImagine, for example, that you have been asked the question \"How well do you understand this text? (on a scale from 1-5)\". Presumably, to answer this question, you will have to choose a response based on where you think you are on your unobserved measure of your understanding.\nYou may be able to evaluate the cohesion, or some other internal measure, of your understanding of the text. Simplifying a bit, we might assume that your internal measure of understanding is associated with a normal probability distribution so that it peaks over some value (e.g., `3`) of the strength of understanding though other values are possible. As @fig-latent-normal-splits suggests, a participant in this common situation will have to map the internal measure (the latent scale, e.g., of understanding) to a number from the response options you are given (e.g., rating scale values ranging 1-5). But there is no reason to suppose that your internal measure of your understanding is divided into an ordered metric scale.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![A latent scale on the horizontal axis is divided into intervals or bins divided by thresholds marked by dotted lines. The cumulative normal probability in the intervals is the probability of the ordinal values.](Week20_files/figure-html/fig-latent-normal-splits-1.png){#fig-latent-normal-splits fig-alt='The figure shows a normal curve, split by dotted lines shown at x-axis points labelled 1-5.' width=576}\n:::\n:::\n\n\nIn conducting analyses of ordinal data with ordinal models, we often fit models that describe the cumulative probability that a rating response is located at some value (typically, understood in terms of threshold) on an underlying latent continuum. In ordinal models, we do not assume that the ordinal responses map to equally spaced intervals on the latent scale: the values or thresholds at which the continuum are split are to be estimated.\n\nIn applying metric models to ordinal data, we *do* assume that intervals are equal though this assumption is unlikely to be true or, at least, is unlikely to be verifiable. This faulty assumption has consequences because the mis-application of metric models (e.g. ANOVA, linear models) to ordinal data is both commonplace and *risky*. As @liddell2018 demonstrate, mis-applying metric models to ordinal data can result in false positives (detecting a difference when none is present), false negatives (missing a difference that is present) and inversions (swapping the difference so that it appears to be positive instead of negative or vice versa).\nThese kinds of misrepresentions cannot be avoided and are not fixed by, for example, averaging ratings scales data together.\n\n## Targets {#sec-ordinal-targets}\n\n1.  Understand practically the reasons for using ordinal models when we analyze ordinal outcome variables.\n2.  Practice running ordinal models with varying random effects structures.\n3.  Practice reporting the results of ordinal models, including through the use of prediction plots.\n\n## Study guide {#sec-ordinal-study-guide}\n\nI have provided a collection of materials you can use. Here, I explain what they are and how I suggest you use them.\n\n**1. Chapter: 05-ordinal**\n\n1.1. I have written this chapter to discuss the main ideas and set out the practical steps you can follow to start to develop the skills required to work with ordered categorical outcomes i.e. *ordinal data* using ordinal models.\n\n1.2. The practical elements include data tidying, visualization and analysis steps.\n\n1.3. You can read the chapter and run the code to gain experience and encounter code you can adapt for your own purposes.\n\n-   Read in the example dataset.\n-   Experiment with the .R code used to work with the example data.\n-   Run ordinal models of demonstration data.\n-   Run ordinal models of alternate data sets (see links in @sec-ordinal-reporting-results).\n-   Review the recommended readings (@sec-ordinal-recommended-reading).\n\n**2. Practical materials**\n\n2.1 In the following sections, I describe the practical steps, and associated resources, you can use for your learning.\nI set out the data tidying, analysis and visualization steps you can follow, working with the example dataset, described next.\n\n## The data we will work with: {#sec-ordinal-data}\n\nWe will be working, at first, with a sample of data collected as part of the **Clearly understood: health comprehension** project (Davies, Ratajczak, Gillings, Chadwick & Gold). These data are unpublished.\n\n### Study information {#sec-ordinal-data-study}\n\n#### Introduction: the background for the study {#sec-ordinal-data-background}\n\nOur interest, in conducting the project, lies in identifying what factors make it easy or difficult to understand written health information. In part, we are concerned about the processes that health providers or clinicians apply to assure the effectiveness of the text they produce to guide patients or carers, for example, in taking medication, in making treatment decisions, or in order to follow therapeutic programmes.\n\nIt is common, in the quality assurance process in the production of health information texts, that text producers ask participants in patient review panels to evaluate draft texts. In such reviews, a participant may be asked a question like \"How well do you understand this text?\" This kind of question presents a metacognitive task: we are asking a participant *to think about their thinking*. But it is unclear that people can do this well or, indeed, what factors determine the responses to such questions [@dunlosky2007].\n\nFor these reasons, we conducted studies in which we presented adult participants with sampled health information texts (taken from health service webpages) and, critically, asked them to respond to the question:\n\n-   `How well do you think you have understood this text? (Please check one response)`\n\nFor each text, in response to this question, participants were asked to click on one option from an array of response options ranging from (1) `Not well at all` to (9) `Extremely well`. The data we collected in this element of our studies comprise, clearly, *ordinal responses*. Thus, we may use these data to address the following research question.\n\n::: callout-note\n-   What factors predict self-evaluated *rated* understanding of health information.\n:::\n\n#### Participants {#sec-ordinal-data-participants}\n\nWe will work with a sample of participant data drawn from a series of Lancaster University undergraduate dissertation studies connected to the **Clearly understood** project. In these studies, we collected data from 202 participants on a series of measures (@sec-ordinal-data-materials-procedure) of vocabulary knowledge, health literacy, reading strategy, as well as responses to health information texts. The distributions of participants' scores on each of a range of attribute variables\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Grid of plots showing the distribution of participant attributes. The grid includes histograms of the distributions of: self-rated accuracy; vocabulary (SHIPLEY); health literacy (HLVA); reading strategy (FACTOR3); and age (years). We also see dot plots presenting counts of numbers of participants of different self-reported gender, education, and ethnicity categories.](Week20_files/figure-html/fig-histogram-grid--1.png){#fig-histogram-grid- fig-alt='The figure presents a grid of histograms indicating the distribution of (x-axis) scores on a range of participant attribute variables. The grid includes histograms of the distributions of: self-rated accuracy; vocabulary (SHIPLEY); health literacy (HLVA); reading strategy (FACTOR3); age (years); gender; education, and ethnicity. The plots indicate: (1.) most self-rated accuracy scores are high (over 6); (2.) many participants with vocabulary scores greater than 30, a few present lower scores; (3.) health literacy scores centered on 8 or some, with lower and higher scores; (4.) a skewed distribution of reading strategy scores, with many around 20-40, and a tail of higher scores; (5.) most participants are 20-40 years of age, some older; (6.) many more female than male participants, very few non-binary reported; (7.) many more participants with higher education than further, very few with secondary; and (8.) many White participants (ONS categories), far fewer Asian or Mixed or Black ethnicity participants.' width=1344}\n:::\n:::\n\n\nThe plots indicate:\n\n1.  most self-rated accuracy scores are high (over 6);\n2.  many participants with vocabulary scores greater than 30, a few present lower scores;\n3.  health literacy scores centered on 8 or some, with lower and higher scores;\n4.  a skewed distribution of reading strategy scores, with many around 20-40, and a tail of higher scores;\n5.  most participants are 20-40 years of age, some older;\n6.  many more female than male participants, very few non-binary reported;\n7.  many more participants with higher education than further, very few with secondary;\n8.  and many White participants (*Office of National Statistics* categories), far fewer Asian or Mixed or Black ethnicity participants.\n\n#### Stimulus materials and data collection procedure {#sec-ordinal-data-materials-procedure}\n\nWe collected data through an online survey administered through Qualtrics.\n\nWe used the Shipley vocabulary sub-test [@shipley2009shipley] to estimate vocabulary knowledge.\n\nWe used the Health Literacy Vocabulary Assessment (HLVA, Ratajczak, 2020; adapted for online presentation, Chadwick, 2020) to estimate health literacy.\n\nWe used an instrument drawn from unpublished work by Calloway (2019) to assess the approach participants took to reading and understanding written information.\n\nWe presented participants with a sample of 20 health information texts. In the data collection process for this dataset, participants were recruited in multiple different studies. In each study, any one participant was presented with a randomly selected subset of the total of 20 texts.\n\nWe asked participants to rate their level of understanding of the health-related texts that we presented in the study. We used a nine-point judgment scales because they have been found to outperform alternative scales with fewer categories in terms of criterion validity, internal consistency, test-retest reliability, and discriminating power [@preston2000].\n\nWe recorded participants' demographic characteristics: gender (coded: Male, Female, non-binary, prefer not to say); education (coded: Secondary, Further, Higher); and ethnicity (coded: White, Black, Asian, Mixed, Other).\n\n### Locate and download the data file {#sec-ordinal-data-download}\n\nYou can download the [2021-22_PSYC304-health-comprehension.csv](files/2021-22_PSYC304-health-comprehension.csv) file holding the data we analyse in this chapter by clicking on the link.\n\n### Read-in the data file using read_csv {#sec-ordinal-data-import}\n\nI am going to assume you have downloaded the data file, and that you know where it is. We use `read_csv` to read the data file into R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth <- read_csv(\"2021-22_PSYC304-health-comprehension.csv\", \n                                 na = \"-999\",\n                                 col_types = cols(\n                                   ResponseId = col_factor(),\n                                   rating = col_factor(),\n                                   GENDER = col_factor(),\n                                   EDUCATION = col_factor(),\n                                   ETHNICITY = col_factor(),\n                                   NATIVE.LANGUAGE = col_factor(),\n                                   OTHER.LANGUAGE = col_factor(),\n                                   text.id = col_factor(),\n                                   text.question.id = col_factor(),\n                                   study = col_factor()\n                                 )\n                               )\n```\n:::\n\n\nNotice that we use `col_types = cols(...)` to require `read_csv()` to class some columns as factors.\n\nImportantly, we ask R to treat the `rating` variable as a *factor* with `rating = col_factor()`.\n\n:::callout-tip\nIn the practical work we do, we will be using functions from the `{ordinal}` library to model ordinal data.\n\n- In using these functions, we need ask R to treat the ordinal outcome variable as a *factor*.\n:::\n\n### Inspect the data {#sec-ordinal-data-inspect}\n\nIt is always a good to inspect what you have got when you read a data file in to R.\nHere, what may most concern us is the distribution of observed responses on the rating scale (responses to the \"How well do you understand?\" question).\n@fig-rating-dotplots is a dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth <- health %>% mutate(rating = fct_relevel(rating, sort))\n\nhealth %>%\n  group_by(rating) %>%\n  summarise(count = n()) %>%\n  ggplot(aes(x = rating, y = count, colour = rating)) + \n  geom_point(size = 3) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + theme_bw() +\n  theme(\n    panel.grid.major.y = element_blank()  # No horizontal grid lines\n  ) +\n  coord_flip()\n```\n\n::: {.cell-output-display}\n![Dot plot showing the distribution of ratings responses. The Likert-style questions in the surveys asked participants to rate their level of understanding of the texts they saw on a scale from 1 (not well) to 9 (extremely well). The plot shows the number of responses recorded for each response option, over all participants and all texts.](Week20_files/figure-html/fig-rating-dotplots-1.png){#fig-rating-dotplots fig-alt='The figure presents a dot plot showing the distribution of ratings responses. The plot shows the number of responses recorded for each response option, over all participants and all texts. Points are coloured to distinguish different response options. The plot indicates that most participants rated their understanding very high to most texts, choosing ratings from 5-6 or above.' width=624}\n:::\n:::\n\n\nThe plot indicates that most participants chose response options 5-9, while very few rated their understanding at the lowest levels (options 1-4).\nInterestingly, many ratings responses around 7-8 were recorded: many more than responses at 5-6.\n\nIn analyzing these data, we will seek to estimate what information available to us can be used to predict whether a participant's rating of their understanding is more likely to be, say, `1` or `2`, `2` or `3` ... `7` or `8`, `8` or `9`. \n\n::: calllout-tip\nOne practical way to think about the estimation problem when working with ratings-style ordinal data is this:\n\n- What factors move or how do influential factors move the probability that the ordinal response is a relatively low or relatively high order response option?\n- In doing this, we do not have to assume that rating scale points map to equal sized intervals on the underlying latent scale where the scale may be an unobserved psychological continuum (like understanding).\n:::\n\nHere, we mostly have information on participant attributes and some information on text properties to do our prediction analyses.\nIn other studies, we may be using information about experimental conditions, or selected groups of participants to estimate effects on variation in ratings responses.\n\n## Tidy the data {#sec-ordinal-data-tidy}\n\nThe **Clearly understood** health comprehension project dataset is tidy ([Week 17](Week17.qmd#sec-intro-mixed-data-tidy)):\n\n1.  Each variable has its own column.\n2.  Each observation has its own row.\n3.  Each value has its own cell.\n\nHowever, there are aspects of the data structure or properties of the dataset variables that will cause inefficiencies or problems in later data analysis if we do not fix them first.\n\nYou can see what we have if you look at the results we get from using `summary()` and `str()` to inspect the dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(health)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id\n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86   \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86   \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86   \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86   \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86   \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86   \n (Other)         :2096   (Other)              :3524   \n```\n:::\n:::\n\n\nYou should be used to seeing the `summary()` of a dataset, showing summary statistics of numeric variables and counts of the numbers of observations of data coded at different levels for each categorical or nominal variable classed as a factor.\n\nUsing the `str()` function may be new to you and, as you can see, the output from the function call gives you a bit more information on how R interprets the data in the variable columns. \nYou can see that each variable is listed alongside information about how the data in the column are interpreted (as `Factor` or `num` numeric).\nWhere we have columns holding information on factors there we see information about the levels.\n\nRecall that for a categorical or nominal variable e.g. `ETHNICITY`, provided R interprets the variable as a factor, each data value in the column is coded as corresponding to one `level` i.e. group or class or category (e.g., we have `ETHNICITY` classes `\"Asian\"` etc.)\nRecall, also, that at the data read-in stage, we instructed R how we wanted it to interpret each column using `col_types = cols()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(health)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [4,040 × 17] (S3: tbl_df/tbl/data.frame)\n $ ResponseId         : Factor w/ 202 levels \"R_sKW4OJnOlidPxrH\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE                : num [1:4040] 20 20 20 20 20 20 20 20 20 20 ...\n $ GENDER             : Factor w/ 3 levels \"Female\",\"Male\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ EDUCATION          : Factor w/ 3 levels \"Further\",\"Higher\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ETHNICITY          : Factor w/ 4 levels \"Asian\",\"White\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ NATIVE.LANGUAGE    : Factor w/ 2 levels \"English\",\"Other\": 1 1 1 1 1 1 1 1 1 1 ...\n $ OTHER.LANGUAGE     : Factor w/ 17 levels \"NA\",\"Catonese\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ ENGLISH.PROFICIENCY: chr [1:4040] \"NA\" \"NA\" \"NA\" \"NA\" ...\n $ SHIPLEY            : num [1:4040] 26 26 26 26 26 26 26 26 26 26 ...\n $ HLVA               : num [1:4040] 8 8 8 8 8 8 8 8 8 8 ...\n $ FACTOR3            : num [1:4040] 59 59 59 59 59 59 59 59 59 59 ...\n $ rating             : Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n $ response           : num [1:4040] 1 1 1 1 1 1 1 0 1 1 ...\n $ RDFKGL             : num [1:4040] 10.61 10.61 10.61 10.61 8.12 ...\n $ study              : Factor w/ 4 levels \"cs\",\"jg\",\"ml\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ text.id            : Factor w/ 20 levels \"studyone.TEXT.105\",..: 1 1 1 1 2 2 2 2 3 3 ...\n $ text.question.id   : Factor w/ 80 levels \"studyone.TEXT.105.CQ.1\",..: 1 2 3 4 5 6 7 8 9 10 ...\n```\n:::\n:::\n\n\nOur specific concern, here, is that the `rating` response variable is treated as a factor because the `{ordinal}` library we are going to use to do the modeling *must* find the outcome variable is a factor.\n\nWe can focus `str()` on the `rating` variable.\nWe see that it *is* being treated as a factor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(health$rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Factor w/ 9 levels \"1\",\"2\",\"3\",\"4\",..: 8 8 8 8 7 7 7 7 7 7 ...\n```\n:::\n:::\n\n\nHowever, we also need to make sure that the `rating` outcome variable is being treated as an *ordered* factor.\n\nWe can perform a check as follows.\n(I found how to do this [here](https://bookdown.org/Tazinho/Tidyverse-Cookbook/factors.html))\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.ordered(factor(health$rating))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\nWe can see that the variable is not being treated as an ordered factor.\nWe need to fix that.\n\nThe ordinal model estimates the locations (thresholds) for where to split the latent scale (the continuum underlying the ratings) corresponding to different ratings values.\nIf we do not make sure that the outcome factor variable is split as it should be then there is no guarantee that `{ordinal}` functions will estimate the thresholds in the right order (i.e., `1,2,3 ...` rather than `3,2,1...`).\n\nWe can make sure that the confidence rating factor is ordered precisely as we wish using the `ordered()` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth$rating <- ordered(health$rating,\n                         levels = c(\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"))\n```\n:::\n\n\nWe can then do a check to see that we have got what we want.\nWe *do not* want`rating` to be treated as numeric, we *do* want it to be treated as an ordered factor.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis.numeric(health$rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n\n```{.r .cell-code}\nis.factor(health$rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nstr(health$rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Ord.factor w/ 9 levels \"1\"<\"2\"<\"3\"<\"4\"<..: 8 8 8 8 7 7 7 7 7 7 ...\n```\n:::\n\n```{.r .cell-code}\nis.ordered(health$rating)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nIt is.\n\nNext, before doing any modelling, it will be sensible to standardize potential predictors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth <- health %>% \n  mutate(across(c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL), \n                scale, center = TRUE, scale = TRUE,\n                .names = \"z_{.col}\"))\n```\n:::\n\n\nYou can see that in this chunk of code, we are doing a number of things:\n\n1. `health <- health %>% ` recreates the `health` dataset from the following steps.\n2. `mutate(...)` do an operation which retains the existing variables in the dataset, to change the variables as further detailed.\n3. `across(...)` work with the multple column variables that are named in the `c(AGE, SHIPLEY, HLVA, FACTOR3, RDFKGL)` set.\n4. `...scale, center = TRUE, scale = TRUE...` here is where we do the standardization work.\n\nWhat we are asking for is that R takes the variables we name and standardizes each of them.\n\n5. `.names = \"z_{.col}\")` creates the standardized variables under adapted names, adding `z_` to the original column name so that we can distinguish between the standardized and original raw versions of the data columns.\n\nNote that the `across()` function is a useful function for applying a function across multiple column variables [see information here](https://dplyr.tidyverse.org/reference/across.html)\nThere is a helpful discussion on how we can do this task [here](https://stackoverflow.com/questions/62714796/standardize-variables-using-dplyr-r)\n\nWe can then check that we have produced the standardized variables as required.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(health)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             ResponseId        AGE                     GENDER    \n R_sKW4OJnOlidPxrH:  20   Min.   :18.0   Female           :2900  \n R_27paPJzIutLoqk8:  20   1st Qu.:20.0   Male             :1120  \n R_1nW0lpFdfumlI1p:  20   Median :27.0   Prefer-not-to-say:  20  \n R_31ZqPQpNEEapoW8:  20   Mean   :34.3                           \n R_2whvE2IW90nj2P7:  20   3rd Qu.:50.0                           \n R_3CAxrri9clBT7sl:  20   Max.   :81.0                           \n (Other)          :3920                                          \n     EDUCATION    ETHNICITY    NATIVE.LANGUAGE    OTHER.LANGUAGE\n Further  :1780   Asian: 680   English:2720    NA        :2720  \n Higher   :1800   White:3260   Other  :1320    Polish    : 580  \n Secondary: 460   Other:  40                   Cantonese : 280  \n                  Mixed:  60                   Chinese   : 120  \n                                               Portuguese:  60  \n                                               polish    :  60  \n                                               (Other)   : 220  \n ENGLISH.PROFICIENCY    SHIPLEY           HLVA           FACTOR3     \n Length:4040         Min.   :15.00   Min.   : 3.000   Min.   :17.00  \n Class :character    1st Qu.:30.00   1st Qu.: 7.000   1st Qu.:45.00  \n Mode  :character    Median :34.00   Median : 9.000   Median :49.00  \n                     Mean   :32.97   Mean   : 8.564   Mean   :49.03  \n                     3rd Qu.:37.00   3rd Qu.:10.000   3rd Qu.:55.00  \n                     Max.   :40.00   Max.   :13.000   Max.   :63.00  \n                                                                     \n     rating        response          RDFKGL       study    \n 8      :1044   Min.   :0.0000   Min.   : 4.552   cs: 480  \n 7      : 916   1st Qu.:1.0000   1st Qu.: 6.358   jg:1120  \n 9      : 824   Median :1.0000   Median : 8.116   ml: 720  \n 6      : 500   Mean   :0.8064   Mean   : 7.930   rw:1720  \n 5      : 352   3rd Qu.:1.0000   3rd Qu.: 9.413            \n 4      : 176   Max.   :1.0000   Max.   :13.278            \n (Other): 228                                              \n             text.id                  text.question.id       z_AGE.V1      \n studyone.TEXT.37: 344   studyone.TEXT.37.CQ.1:  86    Min.   :-0.9826247  \n studyone.TEXT.39: 344   studyone.TEXT.37.CQ.2:  86    1st Qu.:-0.8620352  \n studyone.TEXT.72: 344   studyone.TEXT.37.CQ.3:  86    Median :-0.4399723  \n studyone.TEXT.14: 344   studyone.TEXT.37.CQ.4:  86    Mean   : 0.0000000  \n studyone.TEXT.50: 344   studyone.TEXT.39.CQ.1:  86    3rd Qu.: 0.9468060  \n studyone.TEXT.10: 224   studyone.TEXT.39.CQ.2:  86    Max.   : 2.8159420  \n (Other)         :2096   (Other)              :3524                        \n    z_SHIPLEY.V1          z_HLVA.V1          z_FACTOR3.V1    \n Min.   :-3.294105   Min.   :-2.6074887   Min.   :-4.205936  \n 1st Qu.:-0.543723   1st Qu.:-0.7330662   1st Qu.:-0.529155  \n Median : 0.189713   Median : 0.2041450   Median :-0.003900  \n Mean   : 0.000000   Mean   : 0.0000000   Mean   : 0.000000  \n 3rd Qu.: 0.739789   3rd Qu.: 0.6727506   3rd Qu.: 0.783981  \n Max.   : 1.289866   Max.   : 2.0785675   Max.   : 1.834490  \n                                                             \n     z_RDFKGL.V1     \n Min.   :-1.4644660  \n 1st Qu.:-0.6814594  \n Median : 0.0807363  \n Mean   : 0.0000000  \n 3rd Qu.: 0.6430616  \n Max.   : 2.3187650  \n                     \n```\n:::\n:::\n\n\n## Working with Cumulative Link Models in R {#sec-ordinal-working-models-clm}\n\nIn our first analysis, we can begin by assuming no random effects.\nWe keep things simple at this point so that we can focus on the key changes in model coding.\n\nThe model is conducted to examine what shapes the variation in `rating` responses that we see in @fig-rating-dotplots.\n\n::: callout-note\n-   What factors predict self-evaluated *rated* understanding of health information.\n:::\n\nIn our analysis, the outcome variable is the ordinal response variable `rating`.\nThe predictors consist of the variables we standardized earlier.\nWe use the `clm()` function from the `{ordinal}` library to do the analysis.\nI will give information in outline here, the interested reader can see more detailed information in @christensen2022 and @christensen2015.\nYou can also find the manual for the `{ordinal}` library functions [here](https://cran.r-project.org/web/packages/ordinal/index.html)\n\nWe code the model as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth.clm <- clm(rating ~\n                    \n                    z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL,\n                  \n                  Hess = TRUE, link = \"logit\",\n                  data = health)\n\nsummary(health.clm)\n```\n:::\n\n\nThe code works as follows.\n\nFirst, we have a chunk of code mostly similar to what we have done before, but changing the function.\n\n-   `clm()` the function name changes because now we want a *cumulative link* model of the ordinal responses.\n\nThe model specification includes information about the fixed effects, the predictors: `z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL`.\n\nSecond, we have the bit that is specific to *cumulative link* models fitted using the `clm()` function.\n\n-   `Hess = TRUE` is required if we want to get a summary of the model fit; the default is `TRUE` but it is worth being explicit about it.\n- `link = \"logit\"` specifies that we want to model the ordinal responses in terms of the log odds (hence, the probability) that a response is a low or a high rating value (compare to [week 19](Week19.qmd#sec-glmm-practical-understanding)).\n\n### Read the results {#sec-ordinal-results-clm}\n\nIf you run the model code, it may take a few seconds to run.\nThen you will get the results shown in the output.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL\ndata:    health\n\n link  threshold nobs logLik   AIC      niter max.grad cond.H \n logit flexible  4040 -6880.78 13787.55 5(0)  9.26e-07 7.3e+01\n\nCoefficients:\n          Estimate Std. Error z value Pr(>|z|)    \nz_AGE     -0.17719    0.02966  -5.975 2.30e-09 ***\nz_SHIPLEY  0.34384    0.03393  10.135  < 2e-16 ***\nz_HLVA     0.16174    0.03265   4.954 7.28e-07 ***\nz_FACTOR3  0.74535    0.03145  23.699  < 2e-16 ***\nz_RDFKGL  -0.27220    0.02892  -9.412  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2 -4.65066    0.12978 -35.836\n2|3 -4.13902    0.10395 -39.817\n3|4 -3.26845    0.07390 -44.228\n4|5 -2.56826    0.05804 -44.248\n5|6 -1.69333    0.04437 -38.160\n6|7 -0.87651    0.03671 -23.876\n7|8  0.24214    0.03402   7.117\n8|9  1.63049    0.04252  38.346\n```\n:::\n:::\n\n\nThe `summary()` output for the model is similar to the outputs you have seen for other model types.\n\n1. We first get `formula:` information about the model you have specified.\n2. R will tell us what `data:` we are working with.\n3. We then get `Coefficients:` estimates.\n\nThe table summary of coefficients arranges information in ways that will be familiar you:\n\n- For each predictor variable, we see 'Estimate, Std. Error, z value, and Pr(>|z|)` statistics.\n- The `Pr(>|z|)` p-values are based on Wald tests of the null hypothesis that a predictor has null impact.\n- The coefficient estimates can be interpreted based on whether they are positive or negative.\n\nA positive coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of higher rating values.\nA negative coefficient estimate indicates that higher values of the predictor variable are associated with greater probability of lower rating values.\n\n4. We then get `Threshold coefficients:` indicating where the model fitted estimates the threshold locations: where the latent scale is cut, corresponding to different rating values.\n\n::: callout-tip\nIn reporting ordinal (e.g., cumulative link) models, we typically focus on the coefficient estimates for the predictor variables.\n:::\n\n## Working with Cumulative Link Mixed-effects Models in R {#sec-ordinal-working-models-clmm}\n\nIn our analysis, we begn by assuming no random effects.\nHowever, this is unlikely to be appropriate given the data collection process deployed in the **Clearly understood** projects, where:\n\n- a sample of participants were asked to respond to a sample of texts;\n- we have multiple observations of responses for each participant;\n- we have multiple observations of responses for each stimulus text;\n- participants were assigned to groups, and within a group all participants were asked to respond to the same stimulus texts.\n\nThese features ensure that the data have a *multilevel* structure and this structure requires us to fit a *Cumulative Link Mixed-effects Model* (CLMM).\n\nWe keep things simple at this point so that we can focus on the key changes in model coding.\nWe can code a Cumulative Link Mixed-effects Model as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhealth.clmm <- clmm(rating ~\n                      \n                      z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +\n                      \n                      (1|ResponseId),\n                    \n                    Hess = TRUE, link = \"logit\",\n                    data = health)\n\nsummary(health.clmm)\n```\n:::\n\n\nIf you inspect the code chunk, you can see that we have made two changes.\n\nFirst, we have changed the function.\n\n-   `clmm()` the function name changes because now we want a Cumulative Linear *Mixed-effects Model*.\n\nSecondly, the model specification includes information about fixed effects and *now about random effects*.\n\n-   With `(1 | Participant)` we include random effects of participants on on intercepts.\n\n### Read the results {#sec-ordinal-results-clmm}\n\nIf you run the model code, you will see that the model may take several seconds, possibly a minute or two to complete.\nWe will then get the results shown in the output.\n\n\n::: {.cell hash='Week20_cache/html/unnamed-chunk-17_93495571e89b5437eea80af7488fecf1'}\n::: {.cell-output .cell-output-stdout}\n```\nCumulative Link Mixed Model fitted with the Laplace approximation\n\nformula: rating ~ z_AGE + z_SHIPLEY + z_HLVA + z_FACTOR3 + z_RDFKGL +  \n    (1 | ResponseId)\ndata:    health\n\n link  threshold nobs logLik   AIC     niter       max.grad cond.H \n logit flexible  4040 -4978.08 9984.16 1480(19002) 3.93e-03 3.5e+02\n\nRandom effects:\n Groups     Name        Variance Std.Dev.\n ResponseId (Intercept) 9.825    3.134   \nNumber of groups:  ResponseId 202 \n\nCoefficients:\n          Estimate Std. Error z value Pr(>|z|)    \nz_AGE     -0.44313    0.23483  -1.887  0.05916 .  \nz_SHIPLEY  0.77130    0.26514   2.909  0.00363 ** \nz_HLVA     0.20809    0.25617   0.812  0.41663    \nz_FACTOR3  1.68342    0.23836   7.063 1.63e-12 ***\nz_RDFKGL  -0.44345    0.03677 -12.059  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|2  -9.3297     0.3453 -27.019\n2|3  -8.1413     0.2948 -27.618\n3|4  -6.5243     0.2632 -24.786\n4|5  -5.2159     0.2491 -20.938\n5|6  -3.4668     0.2371 -14.623\n6|7  -1.8481     0.2316  -7.981\n7|8   0.2965     0.2296   1.292\n8|9   3.0417     0.2347  12.959\n```\n:::\n:::\n\n\nYou can see that the output summary presents the same structure.\nIf you compare the output you see in @sec-ordinal-results-clm, however, you will notice some similarities and some differences:\n\n- If you focus first on the estimates of the coefficients for the predictor variables, you will see that the estimates have the same sign (positive or negative) as they had before.\n- However, you will see that the estimates have different magnitudes.\n- You will also see that the p-values are different.\n\nStudents often focus on p-values in reading model summaries. This is mistaken for multiple reasons. \nThe p-values correspond to the probabilities associated with the null hypothesis significance test: the test of the hypothesis that the effect of the predictor is *null* (i.e. the predictor has no impact).\nThis null assumption is made whichever model we are looking at. The p-values *do not* indicate whether an effect is more or less probable. \nBut you *do* get such posterior probabilities in Bayesian analyses. \nSo it does not really mean much, though it is common, to talk about effects being *highly* significant.\nThus it should not worry us too much if the p-values are significant in one analysis but not significant in another.\n\nThat said, it *is* interesting, perhaps, that once we include random effects of participants on intercepts in our analysis then the effects of `z_AGE` and `z_HLVA` are no longer significant.\nI would be tempted to ask if the previously significant effects of these variables owed their impact to random differences between participants in their average or overall level of rating response.\n\n### Presenting and visualizing the effects {#sec-ordinal-visualizing}\n\nIt will be helpful for the interpretation of the estimates of the coefficients of these predictor variables if we visualize the predictions we can make, about how `rating` values vary, given differences in predictor variable values, given our model estimates.\nWe can do this using functions from the `{ggeffects}` library.\nYou can read more about the `{ggeffects}` library [here](https://strengejacke.github.io/ggeffects/index.html) where you will see a collection of articles explaining what you can do, and why, as well as technical information including some helpful tutorials.\n\nThe basic model prediction coding looks like this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n```\n:::\n\n\n@fig-rating-clmm-factor3-predictions shows you the marginal effect of variation in the reading strategy attribute, i.e., the effect of differences between individuals in how they score on the `FACTOR3` measure of reading strategy.\nNote that the variable is listed as `z_FACTOR3` because, as you will recall, we standardized numeric predictor variables before entering them in our model.\n\nThese kinds of plots are understood to present what are variously called conditional effects, or adjusted predictions or marginal effects.\nYou can find a discussion of marginal effects in the context of working with the `{ggeffects}` library [here](https://strengejacke.github.io/ggeffects/articles/ggeffects.html) and [here](https://strengejacke.github.io/ggeffects/articles/introduction_marginal_effects.html).\nYou can find an extensive, helpful (with examples) discussion of marginal effects by Andrew Heiss [here](https://www.andrewheiss.com/blog/2022/05/20/marginalia/).\n\nIn short, what we want to do is to take the model coefficient estimates, and generate predictions with these estimates, given different values of the predictor variable, while holding the other predictor variables at some constant or some level (or some series of values).\n\nIf you look at the code chunk, you can see that we first:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\n```\n:::\n\n\n- In this line, we use `ggpredict()` to work with some model information, assuming we previously fitted a model and gave it a name (here, `health.clmm`).\n\nNote that if you fit the model and call it `health.clmm`, as we did in @sec-ordinal-working-models-clmm, then an object of that name is created in the R workspace or environment.\nIf you click on that object name in the environment window in R-Studio, you will see that there is a list of pieces of information about the model, including the coefficient estimates, the model formula etc. associated with that name.\n\n- So when we use `ggpredict()`, we ask R to take that model information and, for the term we specify, here, specify using `terms=\"z_FACTOR3 [all]\"`, we ask R to generate some predictions.\n- `dat <- ggpredict(...)` asks R to put those predictions in an object called `dat`.\n\nIf you click on that object name in the environment window in R-Studio, you will see that it comprises a dataset.\nThe dataset includes the columns:\n\n- `x` giving different values of the predictor variable. `ggpredict()` will choose some 'representative' values for you but you can construct a set of values of the predictor for which you want predictions.\n- `predicted` holds predicted values, given different predictor `x` values.\n\nIf you then run the line `plot(dat)` you can see what this gets us for these kinds of models.\n@fig-rating-clmm-factor3-predictions presents a grid of plots showing the model-predicted probabilities that a rating response will have one value for each of the 1-9 rating response values that are possible given the Likert rating scale used in data collection.\nIn the grid, a different plot is shown for each possible response value, indicating how the probability varies that the rating response will take that value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nplot(dat)\n```\n\n::: {.cell-output-display}\n![A grid of plots showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable.](Week20_files/figure-html/fig-rating-clmm-factor3-predictions-1.png){#fig-rating-clmm-factor3-predictions fig-alt='The figure presents a grid of plots showing marginal or conditional -- adjusted prediction values -- predicted ratings and how they vary given variation in values of the standardized reading strategy (FACTOR3) variable. We can see flat lines for plots corresponding to predictions concerning response options 1-4, suggesting little probability that a rating response will take one of these values. We see norml curves for plots corresponding to predictions concerning response options 5-9, suggesting how the probability that a response will take one of these values may rise and then fall, depending on the FACTOR3 score a person has. The plots suggest that for higher FACTOR3 scores the probability increases that a rating response will have a higher value.' width=480}\n:::\n:::\n\n\nIf you examine @fig-rating-clmm-factor3-predictions, you can recognize that we have one plot for each different value of the response options available for the Likert-scale `rating` items: 1-9.\nYou can also see that in each plot we get a curve.\nIn some cases -- for `rating` response values `1-4` -- the curve is flat or flattens very quickly, for higher levels of the `z_FACTOR3` variable.\nIn some cases -- for `rating` response values `5-9` -- the curve is more obvious, and resembles a normal distribution curve.\n\nIf you think about it, what these plots indicate are the ways in which the probability that a `rating` response is a low value (e.g., a  rating of `1`) or a high value (e.g., a  rating of `9`) rises or falls.\nEach possible `rating` response is associated with a probability distribution.\nFor example, look at the plot labelled `6`: that shows you the probability distribution indicating how the probability varies that a response will take the value `6`.\nWe can see that the distribution is normal in shape, a bell-shaped curve.\nWe can see that the peak of the curve is over the `z_FACTOR3` score (shown on the x-axis) of about 1.5.\nWe can see that the probability represented by the height of the line showing the curve is lower for `z_FACTOR3` scores lower than the score under the peak (e.g. scores less than `z_FACTOR3` $=2$).\nThe probability represented by the height of the line showing the curve is lower for `z_FACTOR3` scores higher than the score under the peak (e.g. scores greater than `z_FACTOR3` $=1$).\n\nWe can see that the peak of the normal curve, in the case of `rating` response values `5-9`, is located at different places on the horizontal axis.\nLook at each of the plots labelled `5-9`.\nNotice how the horizontal location of the curves *shifts* as `z_FACTOR3` scores increase.\nIf you go from left to right, i.e. from low to high values of `z_FACTOR3`, on each plot then you will see that the peak of the curve is located in different places: going from plot `5` to plot `9` the peak of the curve moves *rightwards*.\nThese curves show how the probability that a `rating` response takes a high value (e.g. `9` instead of `8` or `8` instead of `7` etc.) is *higher* for *higher* values of `z_FACTOR3`.\nThis idea might be a bit clearer if we draw the plot in a different way.\n\n@fig-rating-clmm-factor3-predictions-fancy shows the same model predictions but plots the predictions of the way that probability changes, for each `rating` response, by superimposing the plots for each response value, one on top of the other.\nI have drawn each probability curve in a different colour, and these colours match those used to present the counts of different response values shown in @fig-rating-dotplots.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- ggpredict(health.clmm, terms=\"z_FACTOR3 [all]\")\nggplot(dat, aes(x, predicted, \n                colour = response.level)) + \n  geom_line(size = 1.5) +\n  scale_color_viridis(discrete=TRUE, option = \"mako\") + \n  labs(x = \"Reading strategy (z_FACTOR3)\", y = \"Predicted probability of a rating\") +\n  guides(colour = guide_legend(title = \"Rating\")) +\n  ylim(0, 1) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![A plot showing marginal or conditional predicted probabilities that a rating response will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable](Week20_files/figure-html/fig-rating-clmm-factor3-predictions-fancy-1.png){#fig-rating-clmm-factor3-predictions-fancy fig-alt='The figure presents a plot showing marginal or conditional predicted probabilities that a rating rating will have one value (among the 1-9 rating values possible), indicating how these predicted probabilities vary given variation in values of the standardized reading strategy (FACTOR3) variable. We can see flat lines for plots corresponding to predictions concerning response options 1-4, suggesting little probability that a rating response will take one of these values. We see norml curves for plots corresponding to predictions concerning response options 5-9, suggesting how the probability that a response will take one of these values may rise and then fall, depending on the FACTOR3 score a person has. The plots suggest that for higher FACTOR3 scores the probability increases that a rating response will have a higher value.' width=576}\n:::\n:::\n\n\nYou can read @fig-rating-clmm-factor3-predictions-fancy by observing that:\n\n- For low value ratings e.g. for `rating` responses from 1-4, there is not much predicted probability that a response with such a value will be made (flat lines) but if they are going to be made they are likely to be made by people with low scores on the `z_FACTOR3`.\n\nYou can see this because you can see how the curves peak around low values of `z_FACTOR3`.\nThis should make sense: people with low scores on reading strategy are maybe not doing reading effectively, are maybe as a result not doing well in understanding the texts they are given to read, and thus are not confident about their understanding.\n(This is a speculative causal theory but it will suffice for now.)\n\nRecall, also, that as @fig-rating-dotplots indicated, in the **Clearly understood** health comprehension dataset, we saw that few `rating` responses were recorded for low value ratings of understanding.\nFew people in our sample made rating responses by choosing ratings of `1` or `2` to indicate low levels of understanding.\n\n@fig-rating-clmm-factor3-predictions-fancy also suggests that:\n\n- For higher value `rating` responses -- responses representing ratings from `5` to `9` -- there is variation in the probability that responses with such values will be made.\n- That variation in probability is shown by the probability distribution curves.\n- For these data, and this model, we can see that the probability shifts suggesting that participants in our sample were more likely to choose a higher value rating if they were also presenting high scores on the `z_FACTOR3` measure of reading strategy.\n\n## Reporting model results {#sec-ordinal-reporting-results}\n\nAs the review reported by @liddell2018 suggests, we may have many many studies in which ordinal outcome data are analysed but very few published research reports that present analyses of *ordinal data* using *ordinal models*.\n\nYou can see two examples in the papers published by @ricketts2021 and by @rodríguez-ferreiro2020a.\nThese papers are both published open accessible, so that they are freely available, and they are both associated with accessible data repositories.\n\n- You can find the repository for @ricketts2021 [here](https://osf.io/76ev2/).\n- You can find the repository for @rodríguez-ferreiro2020a [here](https://osf.io/e5gzk/?view_only=038118528c7c426c9729983f54138c88).\n\nThe @rodríguez-ferreiro2020a shares a data .csv only.\n\nThe @ricketts2021 repository shares data and analysis code as well as a fairly detailed guide to the analysis methods.\nNote that the core analysis approach taken in @ricketts2021 is based on Bayesian methods but that we also conduct `clmm()` models using the `{ordinal}` library functions discussed here; these models are labelled *frequentist* models and can be found under *sensitivity analyses*.\n\nFor what it's worth, the @ricketts2021 is much more representative of the analysis approach I would recommend now.\n\nWhatever the specifics of your research question, dataset, analysis approach or model choices, I would recommend the following for your results report.\n\n1. Explain the model -- the advice extended by @meteyard2020a still apply: the reader will need to know:\n\n- The identity of the outcome and predictor variables;\n- The reason why you are using an ordinal approach, explaining the *ordinal* (ordered, categorical) nature of the outcome;\n- The structure of the fixed effects part of the model, i.e. the effects, in what form (main effects, interactions) you are seeking to estimate;\n- And the structure of the random effects part of the model, i.e. what grouping variable (participants? items?), whether they encompass random intercepts or random slopes or covariances.\n\nYou can report or indicate some of this information by presenting a table summary of the effects estimated in your model [e.g., see Table 5, @rodríguez-ferreiro2020a; see tables 2 and 3, @ricketts2021].\nJournal formatting restrictions or other conventions may limit what information you can present.\n\nNotice that I do not present information on threshold estimates.\n\n2. Explain the results -- I prefer to *show and tell*. \n\n- Present conditional or marginal effects plots [see figures 2 and 3, @ricketts2021] to indicate the predictions you can make given your model estimates.\n- And explain what the estimates or what the prediction plots appear to show.\n\n## Extensions {#sec-ordinal-extensions}\n\n### Different kinds of ordinal data {#sec-ordinal-kinds}\n\nAs I hint, when we discuss the concept that ordinal responses may map somehow to a latent unobserved underlying continuum (see @fig-latent-normal-splits), there are other ways to think about ordinal data.\nRather, there are other ways to think about the psychological mechanisms or the *data generating mechanisms* that give rise to the ordinal responses we analyse.\n\nIn @ricketts2021, we explain:\n\n> In the semantic post-test, participants worked their way through three steps, only progressing from one step to the next step if they provided an incorrect response or no response. Given the sequential nature of this task, we analysed data using sequential ratio ordinal models (burkner & Vuorre, 2019). In sequential models, we account for variation in the probability that a response falls into one response category (out of k ordered categories), equal to the probability that it did not fall into one of the foregoing categories, given the linear sum of predictors. We estimate the k-1 thresholds and the coefficients of the predictors. \n\nWhat this explanation refers to is the fact that, in our study:\n\n> The semantic post-test assessed knowledge for the meanings of newly trained words. We took a dynamic assessment or cuing hierarchy approach (Hasson & Joffe, 2007), providing children with increasing support to capture partial knowledge and the incremental nature of acquiring such knowledge (Dale, 1965). Each word was taken one at a time and children were given the op- portunity to demonstrate knowledge in three steps: definition, cued definition, recognition.\n\nWe follow advice set out by @burkner2019 in modeling the ordered categorical (i.e. ordinal) responses using a sequential ratio approach.\n\n## Richly parameterized mixed-effects models\n\nYou will have noticed that the mixed-effects model coded in @sec-ordinal-working-models-clmm incorporates a relatively simple random effect: a term specified to estimate the variance associated with the random effect of differences between participants in intercepts.\n\nAs we we have seen, more complex random effects structures may be warranted [@Baayen2008, @bates2015parsimonious, @Barr2013a and @matuschek2017].\nWhen we attempt to fit models with more complex structures, as we have discussed, for example, in [Week 18](Week18.qmd#sec-dev-mixed-convergence-problems) and [Week 19](Week19.qmd#sec-glmm-bad-signs), we may run into *convergence* problems.\n(Such convergence problems are one reason why I tend to favour Bayesian methods; see, for example, the discussions in @burkner2019 and @liddell2018.)\nThere are ways to resolve these problems by changing the control parameters of the `{ordinal}` functions (see e.g. [this discussion](https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q2/016165.html) or see the information [here](https://cran.r-project.org/web/packages/ordinal/ordinal.pdf)) or by simplfying the model.\n\n## Summary {#sec-ordinal-summary}\n\nWe discussed ordinal data and the reasons why we are motivated to analyze ordinal data using ordinal models.\n\nWe examine the coding required to fit ordinal models.\n\nWe look at the results outputs from ordinal models, and visualizations representing the predictions that can be generated given ordinal model estimates.\n\nWe consider the kinds of information that results reports should include.\n\nWe examine possible extensions to ordinal models.\n\n### Glossary: useful functions {#sec-ordinal-glossary-useful-functions}\n\nWe used two functions from the `{ordinal}` library to fit and evaluate ordinal models.\n\n-   We used `clm()` to fit an ordinal model without random effects.\n-   We used `clmm()` to fit an ordinal mixed-effects model with fixed effects and random effects.\n\n## Recommended reading {#sec-ordinal-recommended-reading}\n\nThe published example studies referred to in this chapter are published in [@ricketts2021; @rodríguez-ferreiro2020a].\n\n@liddell2018 present a clear account of the problems associated with treating ordinal data as metric, and explain how we can better account for ordinal data.\n\n@burkner2019 present a clear tutorial on cumulative and sequential ratio models.\n\nBoth @liddell2018 and @burkner2019 work from a Bayesian perspective but the insights are generally applicable.\n\nGuides to the `{ordinal}` model functions `clm()` and `clmm()` are presented in [@christensen2022; @christensen2015].\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}